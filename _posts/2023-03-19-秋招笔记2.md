---
title: 秋招笔记2
author: 周兵
date: 2023-03-19
category: Jekyll
layout: post
---

Questions

### 算法题：

* 二叉堆实现 priority_queue
* cs106L assignment 2 : hashmap 实现
* Top K 及其扩展
* [链表排序](#LeetCode 23 合并 k 个升序链表)
* [逆序对](#逆序对)
* 手写设计模式 （单例模式之类的）
* [实现 memcopy 函数](#实现 memcopy 函数)
* [实现 strcpy 函数](#实现 strcpy 函数)
* [实现 string 类](#实现 string 类)
* 链表类题目
* 滑动窗口类题目
* [堆排序](#堆排序)
* [接雨水](#接雨水)
* bitmap
* 布隆过滤器
* 手写 printf
* [实现写时复制 fork](#实现写时复制 fork)
* [生产者消费者](#生产者消费者)
* [滑动窗口最大值](#滑动窗口最大值)
* [回文链表](#回文链表)
* [多项式乘法 leetcode :43 i 位 * j 位 最终存放在数组 [i + j, i + j + 1]中](#多项式乘法 leetcode :43 i 位 * j 位 最终存放在数组 [i + j, i + j + 1]中)
* 纸牌游戏
* [快慢指针-链表及其数学证明: 存在环，环的长度，环的指针](#快慢指针-链表及其数学证明: 存在环，环的长度，环的指针)
* 生成0，1概率相同，在生成 0 ：p ,生成 1 ：(1 - p)
* [判断图是否存在环 - DFS & Topo](#判断图是否存在环 - DFS & Topo)
* 最长字符子串
* 状态压缩DP
* [素数筛](#素数筛)
* [LRU](#LRU)
* LFU
* leetcode 486 预测赢家
* LeetCode 48 旋转图像
* LeetCode 34
* [excel 题目 LeetCode 168 171](#excel 题目 LeetCode 168 171)
* 划分 K 个和相同的子数组
* 二叉搜索树有两个节点值有误，找到并交换
* 找到二叉树最小最大节点
* 判断一个字符串是否由多个重复子串组成(Google) leetcode459
* LeetCode 2050 ： 并行课程III
* LeetCode 1087 花括号展开
* leetcode 210 课程表II
* [判断环形链表](#快慢指针-链表及其数学证明: 存在环，环的长度，环的指针) ：leetcode 287: 寻找重复数，快慢指针：O(n)
* [leetcode 39 & 329 组合回溯相关题目](#leetcode 39 & 329 组合回溯相关题目)
* leetcode 273:整数转换成英文表示
* 剑指 offer 46 把数字翻译成字符串
* [leetcode 46 全排列](#leetcode 46 全排列)
* leetcode 208
* leetcode 222 完全二叉树的节点个数
* leetcode 428 序列化和反序列化N叉树
* [LeetCode 23 合并 k 个升序链表](#LeetCode 23 合并 k 个升序链表)
* leetcode 438
* leetcode 324 摆动排序 II
* leetcode 803
* leetcode 1306
* leetcode 641 设计双端队列
* 设计推特
* LeetCode 102 打印锯齿型树，DFS 解法空间复杂度 ： O(h) 树高,双栈解法，deque 解法
* 剑指 offer 13 ： 二维矩阵的和
* 蓄水池抽样 + 证明
* 岛屿数量
* [计算器 : 表达式计算问题](#计算器 : 表达式计算问题)
* [删除链表中的某个节点:三种解法](#删除链表中的某个节点:三种解法)
* [寻找链表的中间节点](#寻找链表的中间节点)
* 设计一个读写锁
* [最长递增子序列(O(n^2) && O(nlogn) 解法) LIS 个数](#最长递增子序列(O(n^2) && O(nlogn) 解法) 
* [将数组中的奇数放前面，偶数放后面，0 放中间](#将数组中的奇数放前面，偶数放后面，0 放中间)
* [移除字符串空格或指定元素：O(n)](#移除字符串空格或指定元素：O(n))
* [克隆图：深拷贝](#克隆图：深拷贝)
* [逆波兰式](#逆波兰式)
* 分割：回溯
* 子集
* 棋盘问题
* [汉诺塔问题](#汉诺塔问题)
* 有序数组/ 有序链表转成二叉搜索树
* 带环链表交叉节点
* 交替删除链表，返回奇数位和偶数位下标的链表
* 单调栈：坡度问题，求可能存在的区间最大值
* leetcode 406 & 399
* GCD
* 















### C++

* 实现 vector
* 实现 array
* set map 底层
* 红黑树设计
* unordered_map 如何处理冲突 （hash table 处理冲突的方法）
* c++ 函数编译的四个阶段
* [c 语言程序运行的过程](#程序执行的基本过程)
* [智能指针底层实现，智能指针是否线程安全](#智能指针)
* [浅拷贝和深拷贝及其实现](#浅拷贝和深拷贝及其实现)
* [this 指针](#this 指针)
* [引用作为返回值](#引用作为返回值)
* [引用和指针的区别](#引用和指针的区别)
* [c++ static 关键字](#c++ static 关键字)
* [为什么 vector 扩容1.5倍或者2倍](#为什么 vector 扩容1.5倍或者2倍)
* 

























## Solutions

### 算法题



#### 实现写时复制 fork

```cpp
/*
    #include <sys/types.h>
    #include <unistd.h>

    pid_t fork(void);
        函数的作用：用于创建子进程。
        返回值：
            fork()的返回值会返回两次。一次是在父进程中，一次是在子进程中。 注意 fork 是会存在两个返回值，返回两个值
            在父进程中返回创建的子进程的ID,
            在子进程中返回0
            如何区分父进程和子进程：通过fork的返回值。
            在父进程中返回-1，表示创建子进程失败，并且设置errno

        父子进程之间的关系：
        区别：
            1.fork()函数的返回值不同
                父进程中: >0 返回的子进程的ID
                子进程中: =0
            2.pcb中的一些数据
                当前的进程的id pid
                当前的进程的父进程的id ppid
                信号集

        共同点：
            某些状态下：子进程刚被创建出来，还没有执行任何的写数据的操作
                - 用户区的数据
                - 文件描述符表

        父子进程对变量是不是共享的？
            - 刚开始的时候，是一样的，共享的。如果修改了数据，不共享了。
            - 读时共享（子进程被创建，两个进程没有做任何的写的操作），写时拷贝。

*/


// 读时共享，写时复制, 当读的时候，不会复制物理地址空间，共享父进程的物理地址空间，父进程和子进程拥有两个虚拟地址空间，指向同一块真实的物理地址
// 当出现写的时候，去赋值父进程的虚拟地址空间，同时写的那一部分进行重新的指向
// 指向最新的开辟的物理地址空间中，但是虚拟地址空间还是没有改变，我们的页表发生更改从而 MMU 可以映射到不同的物理地址中
// copy-on-write 拷贝的是物理内存中的地址空间
// copy-on-write 技术


// 每一个进程结束之后，都会释放自己地址空间的用户区数据，内核去的 PCB 没有办法自己释放掉，需要父进程进行释放操作。
// 孤儿进程： 就是父进程已经结束了，子进程还没有结束，子进程由 init 领养， init pid = 1作为它的父进程
// 僵尸进程： 进程终止的时候，父进程尚未回收，子进程残留资源 （PCB） 存放在内核中，变成僵尸（Zombie）进程

// wait()  waitpid() 进程回收的函数，就是回收上面的僵尸进程的函数,彻底清楚这个函数

// 实际的时间 = 内核时间 + 用户时间 + 消耗的时间 (消耗的时间是指：用户态和内核态进行切换的时间)



int main() {
  	printf("ok\n");  // 只执行一次, fork() 之后全部执行两次
    // 创建一个进程
    pid_t pid = fork();  // 这个数据是一个局部变量，是存储在栈空间的,父进程和子进程返回的 pid 是不同的
    int num = 10;
    printf("The num is %d, and the num addr is %p\n", num, &num);  // 父进程和子进程会执行两次
    // 10 , 0x7fffc2e3334c
    if (pid > 0) {
        // 父进程, 当前父进程 pid 通过getpid() 获得，但是子进程 pid 还是 fork() 获得的 pid
        printf("we get the pid is : %d\n", pid);
        printf("This is parents pid : %d, and this process parent is : %d\n", getpid(), getppid());  // getpid() 获取的是内核空间的PCB 中的 pid
      	// This is parents pid : 2410605, and this process parent is : 2397708
        // PCB 中存放的 pid 值
        num += 10;
        printf("The num is %d, and the num addr is %p\n", num, &num); // 20 , 0x7fffc2e3334c
    } else if (pid == 0) {
        // 子进程
        printf("we get the pid is : %d\n", pid);
        printf("This is child pid : %d, and this child parent pid is %d\n", getpid(), getppid());
      	// This is child pid : 2410606, and this child parent pid is 2410605 / 1 因为有可能父进程 结束了，当前子进程变成孤儿进程，所以子进程的父进程被设置成 init 其 PID 是 1.由 pid = 1 的进程领养，同时释放内核态的进程资源,本来回收内核态资源就是父进程做的 
      	// 获取的是内核空间 PCB 中的 pid
        num += 20;
        printf("The num is %d, and the num addr is %p\n", num, &num); // 30 , 0x7fffc2e3334c
    }
    printf("Hi The num is %d, and the num addr is %p\n", num, &num);  // 如果子进程还是没有更改 num 值的话，那么 num 打印出来是 10
    // 就是说 num 没有发生改变
    // 10 , 0x7fffc2e3334c
    for (int i = 0; i < 3; i++) {
        printf("i is : %d\n", i);
    }

    return 0;
}
```











#### 约瑟夫环

```cpp
int lastRemaining(int n, int m) {
    int res = 0;
    for (int i = 2; i <= n; i++) {
        res = (res + m) % i;
    }
    return res;
}
```



#### GCD

```cpp
// 辗转相除法
// if (b == 0) 因为 b 是之前传经来的 a % b 所以 b == 0 那么 b 就是最终的答案,这个 b 是上一个答案
/*
a % b得余数r
若r == 0，则b即为GCD
若r != 0，则a = b, b = r，返回步骤1
*/

int gcd(int a, int b) {
  if (b == 0) return a;
  return gcd(b, a % b);
}

// 辗转相减法
/*
若a > b，则a = a - b
若b > a，则b = b - a
若a == b，则a(或b)即为最大公约数
若a != b，则回到1
*/
int gcd(int a, int b) {
  if (a == b) return a;
  return a < b ? gcd(a, b - a) : gcd(a - b, b);
}
```







#### 实现 priority_queue

```cpp
#include<bits/stdc++.h>

using namespace std;

template<class T>
class MyPriorityQueue {
public:
    MyPriorityQueue();
    ~MyPriorityQueue();
    T top();
    void push(const T obj);
    void pop();
    bool isEmpty();
    int size();
  
private:
    vector<T>nums;
    void top2down(int idx);
    void down2top(int idx);
};

template<class T>
MyPriorityQueue<T>::MyPriorityQueue() {
    nums.clear();
}

template<class T>
MyPriorityQueue<T>::~MyPriorityQueue() {
    cout << "deconstructor this priority_queue" << endl;
}

template<class T>
T MyPriorityQueue<T>::top() {
    if (!nums.empty()) {
        return nums[0];
    }
    return INT_MIN;
}

template<class T>
void MyPriorityQueue<T>::push(const T obj) {
    nums.push_back(obj);
    down2top(nums.size() - 1);
}

template<class T>
void MyPriorityQueue<T>::pop() {
    if (nums.empty()) return;
    swap(nums[0], nums[nums.size() - 1]);
    nums.pop_back();
    top2down(0);
}

template<class T>
bool MyPriorityQueue<T>::isEmpty() {
    return nums.empty();
}

template<class T>
int MyPriorityQueue<T>::size() {
    return nums.size();
}

template<class T>
void MyPriorityQueue<T>::top2down(int idx) {
    int n = nums.size();
    int max_idx = idx;
    int left = 2 * idx + 1;
    int right = 2 * idx + 2;
    if (left < n && nums[left] > nums[max_idx]) max_idx = left;
    if (right < n && nums[right] > nums[max_idx]) max_idx = right;
    if (max_idx != idx) {
        swap(nums[idx], nums[max_idx]);
        top2down(max_idx);
    }
}

template<class T>
void MyPriorityQueue<T>::down2top(int idx) {
    int n = nums.size();
    int max_idx = idx;
    int parent_idx = (idx - 1) / 2;
    if (idx >= 0 && nums[parent_idx] < nums[max_idx]) max_idx = parent_idx;
    if (max_idx != idx) {
        swap(nums[max_idx], nums[idx]);
        down2top(max_idx);
    }
}



void TestMyPriorityQueue() {
    MyPriorityQueue<int>pq;
    priority_queue<int>pq1;
    srand(time(NULL));
    for (int i = 0; i < 10; i++) {
        int num = rand() % 20 - 10;
        pq1.push(num);
        pq.push(num);
    }
    cout << pq1.size() << " " << pq.size() << endl;
    cout << pq1.empty() << " " << pq.isEmpty() << endl;

    for (int i = 0; i < 10; i++) {
        cout << pq1.top() << " " << pq.top() << endl;
        pq1.pop();
        pq.pop();
    }
    cout << pq1.size() << " " << pq.size() << endl;
    cout << pq1.empty() << " " << pq.isEmpty() << endl;
}


int main() {
    TestMyPriorityQueue();
    return 0;
}
```





#### 实现 hashmap





#### Top K 

```cpp
// 手写堆排序
void adjust(vector<int>& nums, int n, int idx) {
  int max_idx = idx;
  int left = idx * 2 + 1;
  int right = idx * 2 + 2;
  if (left < n && nums[left] > nums[max_idx]) max_idx = left;
  if (right < n && nums[right] > nums[max_idx]) max_idx = right;
  if (max_idx != idx) {
    swap(nums[max_idx], nums[idx]);
    adjust(nums, n, max_idx);
  }
}
void build(vector<int>& nums) {
  int n = nums.size();
  // 建一个堆，但是建队之后数组不一定是有序的
  for (int i = n / 2; i >= 0; i--) {
    adjust(nums, n, i);
  }
  // 下面是对这个数组进行排序的操作，这里只要是堆就可以了，所以可以没有
  // for (int i = n - 1; i >= 0; i--) {
  //     swap(nums[i], nums[0]);
  //     adjust(nums, i, 0);
  // }
}
void Insert(vector<int>nums) {

}
vector<int> getLeastNumbers(vector<int>& arr, int k) {
  if (k == 0) return {};
  vector<int>nums;
  for (int i = 0; i < k; i++) {
    nums.push_back(arr[i]);
  }
  build(nums);
  for (int i = k; i < arr.size(); i++) {
    if (arr[i] < nums[0]) {
      nums[0] = arr[i];
      adjust(nums, nums.size(), 0);
    }
  }
  return nums;
}
```







#### 逆序对

```c++
class Solution {
public:
    int res;
    void Merge(vector<int>&nums, int l, int r, int mid) {
        vector<int>temp(r - l + 1);
        int left = l, right = mid + 1;
        int k = 0;
        while (left <= mid && right <= r) {
            if (nums[left] <= nums[right]) {
              	// res += right - mid - 1; // 这个注释的解法也是可以的，表示的是后面比其小的个数，表示的是 right 之前的元素都是比其 left 小的。
               	// 所以上面对逆序对比当前元素小的元素个数很方便
                temp[k] = nums[left];
                left++;
            } else {
                res += mid - left + 1; // 表示的是比其大的个数，就是 left 往后都是比 nums[right] 大的。
              	// 上面对于求前面有多少个比当前的元素大的个数
                temp[k] = nums[right];
                right++;
            }
            k++;
        }
        while (left <= mid) {
          	// res += r - mid; // 这个注释的解法也是可以的,注意这个是不能省略的.
            temp[k] = nums[left];
            k++;
            left++;
        }
        while (right <= r) {
            temp[k] = nums[right];
            k++;
            right++;
        }
        // cout << "ok" << endl;
        for (int i = 0; i < k; i++) {
            nums[i + l] = temp[i];
        }
    }
    void MergeSort(vector<int>&nums, int l, int r) {
        // 左右均包括
        if (l == r) return ;
        int mid = (r - l) / 2 + l;
        MergeSort(nums, l, mid);
        MergeSort(nums, mid + 1, r);
        Merge(nums, l, r, mid);
    }
    int reversePairs(vector<int>& nums) {
        if (nums.size() == 0) return 0;
        res = 0;
        MergeSort(nums, 0, nums.size() - 1);
        return res;
    }
};
```









#### 接雨水

```c++
int trap(vector<int>& height) {
    // 如何将单调栈的性质应用到这一题上面
    // 注意每一个位置，只和左右最大高度的最小值相关

    // 解法一： 暴力解法，其实我也没啥想法，向左向右找最大值，选两个最大值的最小值为我们求得的答案
    // 时间复杂度 ： O(n^2)， 空间复杂度 ： O(1)
    int res = 0;
    int n = height.size();
    int num = height[0];
    bool flag = 0;

    for (int i = 1; i < n - 1; i++) {
        int left_max = 0, right_max = 0;
        for (int j = 0 ; j <= i; j++) {
            left_max = max(left_max, height[j]);
        }
        for (int j = i; j < n; j++) {  //
            right_max = max(right_max, height[j]);
        }
        // cout << left_max << " " << right_max << endl;
        if (left_max == 0 || right_max == 0) continue;
        res += (min(left_max, right_max) - height[i]);
    }
    return res;

    // 解法二： 使用数组将中间的 left_max 和 right_max 保存下来，这样时间复杂度降到 O(n)，但是空间复杂度增加到 ： O(n)
    int res = 0;
    int n = height.size();
    vector<int>left_max(n), right_max(n);
    left_max[0] = height[0];
    right_max[n - 1] = height[n - 1];
    for (int i = 1; i < n; i++) {
        left_max[i] = max(height[i], left_max[i - 1]);
    }
    for (int i = n - 2; i >= 0; i--) {
        right_max[i] = max(height[i], right_max[i + 1]);
    }

    for (int i = 1; i < n - 1; i++) {
        if (left_max[i] == 0 || right_max[i] == 0) continue;
        res += (min(left_max[i], right_max[i]) - height[i]);
    }
    return res;

    // 解法三： 使用双指针求解最优的算法， 时间复杂度 ：O(n) 空间复杂度 ： O(1)
    int res = 0;
    int n = height.size();
    int left_max = height[0], right_max = height[n - 1];
    int left = 0, right = n - 1;
    while (left <= right) {
        left_max = max(left_max, height[left]);  // 因为右边一直不移动的话，左边一定有一个时刻达到右边
        right_max = max(right_max, height[right]);

        if (left_max < right_max) { // 那么 left_max 一定是左边最小的，就是满足我们需求的
            res += (left_max - height[left]);
            left++;
        } else {
            res += (right_max - height[right]);
            right--;
        }
    }
    return res;
}
```

#### 滑动窗口最大值

```c++
 * [239] 滑动窗口最大值


// @lc code=start
class Queue {
public:
    void push(int n) {
        while (!dq.empty() && dq.back() < n) {
            dq.pop_back();
        }
        dq.push_back(n); // 这样这个就是最大的了,比他小的都被弹出去了
    }
    int max() {
        return dq.front();
    }
    void pop(int n) {
        if (!dq.empty() && dq.front() == n){  // 弹出去的元素只有在队头的可能性，因为队头是最先进入
        // 或者是之后进入进行覆盖了，那就不存在这个 n 了 啥也不需要做了,队头弹出有先后顺序的
            dq.pop_front();
        }
    }
private:
    deque<int>dq;  // dq 的一系列操作都是 O(1) 的
};

class Solution {
public:
    vector<int> maxSlidingWindow(vector<int>& nums, int k) {
        // 关键点s是如何在 O(1) 的时间复杂度中找到最大的元素,这个时候使用单调队列求解非常大额方便
        int n = nums.size();
        Queue que;
        vector<int>res;
        for (int i = 0; i < n; i++) {
            if (i < k - 1) {
                que.push(nums[i]);
                continue;
            }
            que.push(nums[i]);
            res.push_back(que.max());
            que.pop(nums[i - k + 1]);
        }
        return res;
    }
};
```

#### 判断图是否存在环 - DFS & Topo

```c++
// Topo 排序
vector<int>g[maxn]; // 存放的是临接矩阵
vector<int>indegree; // 存放的是节点的入度
int solve(vector<vector<int>>nums, int size) {
    for (int i = 0; i < nums.size(); i++) {  // 初始化操作
        g[nums[i][0]].push_back(nums[i][1]);
        indegree[nums[i][1]]++;
    }
    queue<int>que;
    for (int i = 0; i < size; i++) {
        if (indegree[i] == 0) {
            que.push(i);
        }
    }
    while (!que.empty()) {
        int front = que.front();
        que.pop();
        for (int i = 0; i < g[front].size(); i++) {
            indegree[g[front][i]]--;
            if (indegree[g[front][i]] == 0) {
                que.push(g[front][i]);
            } 
        }
    }
    for (int i = 0; i < size; i++) {
        if (indegree[i] != 0) return false;
    }
    return true;
}


// DFS, 分成三种状态，没搜索，搜索中，搜索完成
// 搜索中，不能再次出现这个在搜索中的节点，如果出现表示的是存在环

vector<int>vis;
vector<int>res;
int valid;
void dfs(int i) {
    vis[i] = 1;
    if (valid) return;
    for (int j = 0; j < g[i].size(); j++) {
        if (vis[g[i][j]] == 0) {
            dfs(g[i][j]);
        } else if (vis[g[i][j]] == 1) {
            valid = 1;
            return;
        }
    }
    // 当前 i 点回溯结束
    vis[i] = 2; // 为什么不使用 vis[i] = 0 因为这样可以减少之后的遍历，算是剪枝操作
  	// 因为 i点 dfs 过了，就表示是符合条件的，之后再遍历 i 遍历的结果也一定是一样的
    res.push_back(i);
}

int solve(vector<vector<int>>nums, int size) { // 
    for (int i = 0; i < nums.size(); i++) {
        g[nums[i][0]].push_back(nums[i][1]);
    }
    valid = 0;
    for (int i = 0; i < size; i++) vis.push_back(0);
    for (int i = 0; i < size; i++) { // 为什么需要 for 遍历一下，因为可能存在不是连通图，就是存在子图的可能
        if (vis[i] == 0) {
            dfs(i);
        }
        if (valid) return false;
    }
    return true;
}
```

#### 实现 string 类

```c++
// String.h
#ifndef STRING_H
#define STRING_H

#include <iostream>

using namespace std;

class String {
public:
    // 定义构造函数，析构函数
    String();  // 构造函数，默认的构造函数
    // 自定义构造函数
    String(const char* str);  // 浅拷贝，可能出现拷贝指针之后，两次执行析构函数，从而使某一个指针变成野指针
    String(int n, char c);  // 传入大小的构造函数
    ~String();  // 析构函数

public:
    // 定义函数, 需要实现的函数
    // 拷贝构造函数，实现的是深拷贝，避免浅拷贝可能出现的问题
    String(String& str);

    // 拷贝构造赋值函数
    const String& operator = (const String& str);  // 返回的是一个 String

    // 下标访问的运算符重载
    char operator [] (int pos) const;  // 返回的是一个 char
    char at(int pos) const;

    // String 类提供的方法
    int size() const;
    int length() const;
    bool empty() const;
    char* getstr() const;

    // 重载流运算符
    // 定义成 frirnd 表示的是别的函数希望访问当前类的私有成员变量，我们觉得这个是合法的，可以给他访问
    friend ostream& operator << (ostream& output, String& str); // 输出流的重载
    friend istream& operator >> (istream& input, String& str); // 输入流的重载

    // 连接两个字符串操作以及字符串是否相等的判断
    const String& operator += (const String& str);
    bool operator == (const String& str);
    bool compare(const String& str);


private:
    char* ptr_;  // 保存字符串的首地址
    int strlen_;  // 保存的是字符串的长度
    int str_buffer_len_;  // 保存的是字符串 buffer size
};

#endif
```

```c++
// String.cpp
#include "String.h"
#include <string.h>

#define EXT_LEN 50  // 定义一个宏大小,实际分配的大小，比原来的大小多 EXT_LEN 长度，目的是方便字符串拼接

// 构造和析构函数

String::String(){}

String::String(const char* str) {
    strlen_ = strlen(str);
    ptr_ = new char[strlen_ + EXT_LEN];
    // 下面是拷贝函数，可以使用 c 语言的拷贝函数，也可以使用 C++ 的拷贝函数
    // c 语言的拷贝函数
    // strcpy(ptr_, str);  // 意思是不需要指定拷贝的长度，当遇见 '\0' 就结束拷贝
    // C++ 语言的拷贝函数
    memcpy(ptr_, str, strlen_);  // 内存拷贝，不是遇见 '\0' 结束，直接提供了拷贝的长度
    str_buffer_len_ = strlen_ + EXT_LEN;  // 计算字符串的空间大小
}

String::String(int n, char c) {
    strlen_ = n;
    str_buffer_len_ = n + EXT_LEN;
    ptr_ = new char[str_buffer_len_];  // new 一段内存空间,调用构造函数
    for (int i = 0; i < n; i++) {  // 拷贝操作
        ptr_[i] = c;
    }
}

String::~String() {
    // delete ptr_;
    // 需要先判断一下，避免出现未定义的情况
    if (ptr_) {
        delete[] ptr_;
        ptr_ = nullptr;
    }
    strlen_ = 0;
    str_buffer_len_ = 0;
}

// 拷贝构造和拷贝复制函数

String::String(String& str) {
    size_t len = str.size();
    ptr_ = new char[len + EXT_LEN];
    strlen_ = len;
    str_buffer_len_ = len + EXT_LEN;
    memcpy(ptr_, str.ptr_, len);
}

const String& String::operator = (const String& str) {  // 返回的是一个引用
    if (this == &str) {  // 避免自我赋值
        return *this;
    }
    if (ptr_) {  // 如果 ptr_ 这个指针中还存在字符，那么将这个字符先删除，然后设置成空. 方便后面的赋值操作
        delete[] ptr_;
        ptr_ = nullptr;
    }
    size_t len = str.size();
    ptr_ = new char[len + EXT_LEN];
    strlen_ = len;
    str_buffer_len_ = len + EXT_LEN;
    memcpy(ptr_, str.ptr_, len);

    return *this;  // 返回的是左操作数的引用, 以引用的形式返回当前的对象
}

// 下面是：访问的运算符重载

char String::operator [] (int pos) const {
    /*
    if (pos < 0 || pos >= strlen_) {
        // throw exception("The idx is out of range!");
        cout << "Out of range!" << endl;
        return '\0';
    }*/
    assert(pos >= 0 && pos < strlen_);
    return ptr_[pos];
}


char String::at(int pos) const {
    /*
    if (pos < 0 || pos >= strlen_) {
        cout << "Out of range!" << endl;
        return '\0';
    }
    */
    assert(pos >= 0 && pos < strlen_);
    return ptr_[pos];
}

// 下面是 String 类的常见函数

int String::size() const {
    return strlen_;
}

int String::length() const {
    return str_buffer_len_;
}

bool String::empty() const {
    return strlen_ == 0;  // 根据字符串的长度输出判断是否是空
}

char* String::getstr() const {
    return ptr_;
}

// 下面是重载流运算符
// 首先需要注意的是，这个输入输出流不是 String 中的成员函数，是一个友元函数，所以是一个外来的函数
// 所以这个函数不是在 String 类的作用域下的。

ostream& operator << (ostream& output, String& str) {
    for (size_t i = 0; i < str.size(); i++) {
        std::cout.put(str.ptr_[i]);  // 将某一个字符打印到屏幕上
    }
    return output;
}

istream& operator >> (istream& input, String& str) {
    std::cin.get(str.ptr_, str.length(), '\n');  // '\n' 是结束字符，
    // 即使没有输入指定的 str.length() 长度的字符串，也需要结束，当遇见这个字符的时候
    return input;
}

// 下面是 : 连接两个字符串，以及判断是否相等

const String& String::operator += (const String& str) {

    // 首先需要判断一下空间是否够用

    if (str_buffer_len_ - strlen_ >= str.size()) {
        // 空间够用
        memcpy(ptr_ + strlen_, str.ptr_, str.size());
        strlen_ = strlen_ + str.size();
        // 原本的 buffer size 还是没有改变
    } else {
        // 需要分配空间
        size_t len = str.size();
        char* new_ptr = new char[str_buffer_len_ + len];
        memcpy(new_ptr, ptr_, strlen_);
        if (ptr_) {
            delete[] ptr_;
        }
        ptr_ = new_ptr;  // ptr_ 现在需要重新指向
        str_buffer_len_ = str_buffer_len_ + len;
        memcpy(ptr_ + strlen_, str.ptr_, str.size());
        strlen_ = strlen_ + len;
    }
    return *this;  // 最终返回的是 左操作数引用
}


bool String::operator == (const String& str) {
    if (str.size() != strlen_) return false;
    for (size_t i = 0; i < strlen_; i++) {
        if (str.ptr_[i] != ptr_[i]) return false;
    }
    return true;
}

bool String::compare (const String& str) {
    if (str.size() != strlen_) return false;
    for (size_t i = 0; i < strlen_; i++) {
        if (str.ptr_[i] != ptr_[i]) return false;
    }
    return true;
}
```

```makefile
cmake_minimum_required(VERSION 3.9)
project(string_class) # 当前工程的名称
# 设置 c++ 编译的版本
set(CMAKE_CXX_STANDARD 17)

aux_source_directory(. source_list) # 表示的是寻找所有的 cpp 文件
# source_List 表示的是所有当前文件夹的所有子文件夹中 cpp 文件
add_executable(string_class &{source_list})
```

#### 实现 memcopy 函数

```c++
// memcpy() 是一段内存拷贝到另外一段内存，拷贝指定大小内存段
void* memorycopy(void* dst, const void* src, size_t n) {  // 按照 一个字节拷贝
    // 因为这个函数是一段函数的拷贝，所以需要考虑是否存在内存重叠的情况。如果 dst 存在 src - src[0] + n 这段内存中，
    // 那么存在内存重叠，从前向后赋值，会导致覆盖 src 后面的内容，出现错误
    // 一个字符就是一个字节 8 位，内存是按照字节寻址的。内存的基本单位是字节
    if (dst == nullptr || src == nullptr) return nullptr;
    char* pDes = (char*)(dst);  // 按照字节拷贝
    char* pSrc = (char*)(src);
    if (pDes > pSrc && pDes < pSrc + n) { // 防止内存交叉，内存覆盖
        for (size_t i = n - 1; i >= 0; i--) {
            pDes[i] = pSrc[i];
        }
    } else {
        for (size_t i = 0; i < n; i++) {
            pDes[i] = pSrc[i];
        }
    }
    return dst;
}

void* memorycopy4bytes(void* dst, const void* src, size_t n) {
    // 按照四个字节拷贝,将 void* 转换成 int*
    if (dst == nullptr || src == nullptr) return nullptr;
    int* pDes = (int*)(dst);
    int* pSrc = (int*)(src);
    int num = n / 4;
    int other_num = n % 4;
    // 四个字节就没法进行判重操作了
    while (num--) {
        *pDes = *pSrc;
        pDes++;
        pSrc++;
    }
    // 需要对每一个字节进行赋值，我们必须将其设置成 char *
    char* pcDes = (char*)(pDes);
    char* pcSrc = (char*)(pSrc);
    while (other_num--) {
        *pcDes = *pcSrc;
        pcDes++;
        pcSrc++;
    }
    return dst;
}
```

#### 实现 strcpy 函数

```c++
char* strcpy(char* dst, const char* src) {  // 以 '\0' 为结束
    assert(dst != nullptr && src != nullptr);
    // 写法一：
    // char* address = dst;
    // for (size_t i = 0; src[i] != '\0'; i++) {
    //     dst[i] = src[i];
    // }
    // 写法二：
    char* address = dst;
    while ((*src) != '\0') {
        *dst = *src;
        dst++;
        src++;
    }
    return address;
}
```

#### 素数筛

##### 串行

```go
// 单线程版素数筛
func solve_single(n int) []int {
	nums := make([]int, n+1)
	nums[0], nums[1], nums[2] = 1, 1, 0
	for i := 2; i <= n; i++ {
		if nums[i] == 0 {
			// 表示当前这个数是素数,那么将和这个数相关的数均删除
			for j := 2 * i; j <= n; j += i { // 和 i 相关的数均设置成非素数
				nums[j] = 1 // 表示的是这个数是非素数
			}
		}
	}
	res := make([]int, 0)
	for i := 2; i <= n; i++ {
		if nums[i] == 0 {
			res = append(res, i)
		}
	}
	return res
}
```

##### 并行

```go
// 更清晰一点的写法，并发素数筛
func Generate(ch chan<- int) { // 输出
	for i := 2; ; i++ {
		ch <- i
	}
}

func solve(in <-chan int, out chan<- int, prime int) {
	for {
		i := <-in
		if i%prime != 0 {
			out <- i
		}
	}
}

func main() {
	ch := make(chan int)
	go Generate(ch)
	for i := 0; i < 100; i++ {
		prime := <-ch
		fmt.Printf("The %v prime is : %v\n", i+1, prime)
		ch1 := make(chan int)
		go solve(ch, ch1, prime)
		ch = ch1 // 因为每一个通道都是串联的
	}
}

```

#### 堆排序

```c++
#include<bits/stdc++.h>

using namespace std;
// 手写一个大顶堆，adjust 是自下向上自右向左进行调整的。
// 树的根节点下标是 0， 然后从左向右从上向下一步一步增加。
// 递归写法
void adjust_recursion(vector<int>&nums, int len, int idx) {  
    // len 表示的是总共有多少个节点, 根节点的下标是 0 开始的, 这个 adjust 的时间复杂度是 ： O(logn)
    int left = idx * 2 + 1;  // idx 左节点
    int right = idx * 2 + 2;  // idx 右节点
    int max_idx = idx;
    if (left < len && nums[left] > nums[max_idx]) max_idx = left;
    if (right < len && nums[right] > nums[max_idx]) max_idx = right;
    if (max_idx != idx) {
        swap(nums[max_idx], nums[idx]);
        adjust_recursion(nums, len, max_idx);
    }
}

// 迭代写法
void adjust_iterator(vector<int>&nums, int len, int idx) {
    queue<int>que;
    que.push(idx);
    int max_idx;
    while (!que.empty()) {
        int front = que.front();
        que.pop();
        max_idx = front;
        int left = front * 2 + 1;
        int right = front * 2 + 2;
        if (left < len && nums[left] > nums[max_idx]) max_idx = left;
        if (right < len && nums[right] > nums[max_idx]) max_idx = right;
        if (front != max_idx) {
            swap(nums[front], nums[max_idx]);
            que.push(max_idx);
        } else break;
    }
}

void heap_sort(vector<int>&nums) {
    int len = nums.size();
    for (int i = (len - 1) / 2; i >= 0; i--) {
        adjust_iterator(nums, len, i);  // 首先建堆，时间复杂度： O(n)
    }
    // 将根元素和最后一个元素互换，这样大的就放在后面了，较小的就放在根元素上了
    for (int i = len - 1; i >= 1; i--) { // 总的时间复杂度是 ： O(nlogn)
        swap(nums[i], nums[0]);
        adjust_iterator(nums, i, 0);  // len 大小变成 i，size 变小了
    }
}

// 测试程序
int main() {
    srand((unsigned)time(NULL));
    vector<int>nums, nums_temp;
    for (int i = 0; i < 1000; i++) {
        int num = rand() % 10000;
        nums.push_back(num);
        nums_temp.push_back(num);
    }
    heap_sort(nums);
    sort(nums_temp.begin(), nums_temp.end());
    for (int i = 0; i < nums.size(); i++) {
        if (nums[i] != nums_temp[i]) {
            cout << "Error !" << nums[i] << " " << nums_temp[i] << endl;
            return 0;
        }
    }
    cout << "Correct!!!" << endl;
    return 0;
}
```



#### LRU

```c++
// 解法一： 使用标准库的 双向链表 + hashmap 求解
// 解法二： 手写双向链表 + hashmap 求解

// 解法二：
struct DoubleLinkList {
    int key, val;
    DoubleLinkList* prev;
    DoubleLinkList* next;
    DoubleLinkList(int key, int val) : key(key), val(val) {
        prev = nullptr;
        next = nullptr;
    }
};
class LRUCache {
public:
    LRUCache(int capacity) {
        capacity_ = capacity;
        head = new DoubleLinkList(-1, -1);
        tail = new DoubleLinkList(-1, -1);
        head -> next = tail;
        head -> prev = tail;
        tail -> next = head;
        tail -> prev = head;
    }
    void puttofront(DoubleLinkList* node) {
        if (node -> next != nullptr && node -> prev != nullptr) {
            // 表示的是不是新加入的节点, 或者说是新建立的节点
            // 先删除再插入
            node -> next -> prev = node -> prev;
            node -> prev -> next = node -> next;
        }
        node -> next = head -> next;
        head -> next -> prev = node;
        node -> prev = head;
        head -> next = node;
    }
    int get(int key) {
        if (cachemap.find(key) != cachemap.end()) {
            puttofront(cachemap[key]);
            return cachemap[key] -> val;
        }
        return -1;
    }

    void put(int key, int value) {
        if (cachemap.find(key) != cachemap.end()) {
            cachemap[key] -> val = value;
            puttofront(cachemap[key]);
        } else {
            auto insert_node = new DoubleLinkList(key, value);
            if (cachemap.size() == capacity_) {
                // 删除操作
                auto delete_node = tail -> prev;
                tail -> prev = delete_node -> prev;
                delete_node -> prev -> next = tail;
                cachemap.erase(delete_node -> key);  // 注意需要从 map 中将最后一个节点删除，否则 map 的size 会一直增长
                delete_node -> next = nullptr;
                delete_node -> prev = nullptr;
                delete delete_node;
            }
            puttofront(insert_node);
            cachemap[key] = insert_node;
        }
    }
private:
    int capacity_;
    DoubleLinkList* head;
    DoubleLinkList* tail;
    unordered_map<int, DoubleLinkList*>cachemap;
    // list<Node>cachelist;
    // unordered_map<int, list<Node>::iterator>cachemap;
};


// 解法一： 使用标准库的 双向链表 + hashmap 求解
// 解法二： 手写双向链表 + hashmap 求解

// 解法一：
struct Node {
    int key, val;
    Node() : key(0), val(0) {}
    Node(int key, int val) : key(key), val(val) {}
};
class LRUCache {
public:
    LRUCache(int capacity) {
        capacity_ = capacity;
    }

    int get(int key) {
        // auto node = cachemap.find(key);
        if (cachemap.find(key) != cachemap.end()) {
            auto val = *cachemap[key];
            cachelist.erase(cachemap[key]);
            cachelist.push_front(Node(key, val.val));
            cachemap[key] = cachelist.begin();
            return val.val;
        }
        return -1;
    }
	/*
	// Creating iterator to point to first 
    // element in the list 
    list<int>::iterator itr = demoList.begin(); 
  
    // deleting the first element 
    demoList.erase(itr); 
    list.erase(传入的参数是一个迭代器)
	*/
    void put(int key, int value) {
        if (cachemap.find(key) != cachemap.end()) {
            cachelist.erase(cachemap[key]);
            cachelist.push_front(Node(key, value));
            cachemap[key] = cachelist.begin();
        } else {
            if (cachemap.size() == capacity_) {
                // 逐出缓存
                Node ky = cachelist.back();
                cachelist.pop_back();
                cachemap.erase(ky.key);
            }
            cachelist.push_front(Node(key, value));
            cachemap[key] = cachelist.begin();
        }
    }
private:
    int capacity_;
    list<Node>cachelist;
    unordered_map<int, list<Node>::iterator>cachemap;
};
```



#### LFU

* 使用 set 和 unordered_map 存储数据
* 因为 set 底层采用的是红黑树，所以是一个排序的状态，有序的状态。（我们可以指定按照我们的需求排序：频率从小到达排序，相同则使用时间排序）

* 定义一个全局的 time 变量，记录使用的时间

```cpp
struct Node{
    int key, val, time, freq;
    Node() : key(0), val(0), time(0), freq(0) {}
    Node(int key, int val, int time, int freq) : key(key), val(val), time(time), freq(freq) {}
    bool operator < (const Node& a) const {
        return freq == a.freq ? time < a.time : freq < a.freq;  // 从小到大排序
    }
};
class LFUCache {
public:
    LFUCache(int capacity) : m_capacity(capacity), m_time(0) {
        
    }
    
    int get(int key) {
        if (m_capacity == 0) return -1;
        m_time++;
        if (mp.find(key) == mp.end()) return -1;
        st.erase(mp[key]);
        mp[key].freq++;
        mp[key].time = m_time;
        st.insert(mp[key]);
        return mp[key].val;
    }
    
    void put(int key, int value) {
        if (m_capacity == 0) return ;
        m_time++;
        if (mp.find(key) == mp.end()) {
            if (mp.size() == m_capacity) {
                auto it = st.begin();
                mp.erase(it -> key);
                st.erase(it);
            }
            Node node = Node(key, value, m_time, 0);
            mp[key] = node;
            st.insert(node);
        } else {
            st.erase(st.find(mp[key]));
            mp[key].val = value;
            mp[key].freq++;
            mp[key].time = m_time;
            st.insert(mp[key]);
        }
    }
private:
    int m_capacity;
    int m_time;
    std::set<Node>st;
    std::unordered_map<int, Node>mp;
};

/**
 * Your LFUCache object will be instantiated and called as such:
 * LFUCache* obj = new LFUCache(capacity);
 * int param_1 = obj->get(key);
 * obj->put(key,value);
 */
```





#### 生产者消费者

```go
// Go 实现
package main

import (
	"fmt"
	"time"
)

func Producer(ch chan int, num int) {
	for i := 0; ; i++ {
		ch <- i * num
	}
}

func Consumer(ch chan int) {
	for i := range ch {
		fmt.Println(i)
	}
}

func main() {
	ch := make(chan int, 64)
	go Producer(ch, 3)
	go Producer(ch, 5)
	go Consumer(ch)
	time.Sleep(5 * time.Second)
}
```



```cpp
// C++ 实现生产者消费者模型
#include <unistd.h>
#include <cstdlib>
#include <iostream>
#include <mutex>
#include <thread>
#include <condition_variable>
#include <chrono>

/*
1. join() 函数？
join() 函数有两个作用， 1. 等待子线程执行完毕，主线程才结束执行。 2. 清理子线程相关的存储器， std::thread 不再和子线程相关联，释放子线程的资源

2. detach 函数
detach() 函数，让线程在后台运行，意味着线程之间不能直接产生交互，后台线程归属和控制都是由 C++ 运行库处理
detach() 函数需要考虑的是：确保子线程的参数必须是对象的复制，因为可能主线退出导致临时对象的实现，子线程对象相继实现.
主线程退出，子线程还在执行，子线程拥有使用主线程资源，会出错.

*/


// 版本一：单生产者，单消费者
// 对于条件变量我们只申请 2 个，分别用来控制 consumer 和 producer
// 当 std::condition_variable 对象的某个 wait 函数被调用的时候，
// 它使用 std::unique_lock(通过 std::mutex) 来锁住当前线程。当前线程会一直被阻塞，
// 直到另外一个线程在相同的 std::condition_variable 对象上调用了 notification 函数来唤醒当前线程。


static const int bufSize = 10; // 缓冲的大小
static const int proNum = 20; // 可以生产的物体个数

typedef std::chrono::milliseconds _ms;

struct resource_ {
    int buf[bufSize];  // 产品缓冲区， 配合 read_pos 和 write_pos 模型形成环形队列
    int read_pos; // 消费者读取产品位置
    int write_pos; // 生产者写入产品位置
    std::mutex mtx; // 互斥量，保护产品缓冲区
    std::condition_variable not_full; // 条件变量，声明当前的缓冲区是不满的，用来唤醒生产者线程
    std::condition_variable not_empty; // 条件变量，声明当前的缓冲区是不空的，用来唤醒消费者线程
} instance;

typedef struct resource_ resource;


// 这里使用的是数组模仿队列，固定数组的大小，模仿队列
void Producer(resource* src, int item) {
    std::unique_lock<std::mutex> lock(src -> mtx);
    while ((src -> write_pos + 1) % bufSize == src -> read_pos) {// 需要预留一个空间来和空队列作为区别，就是可以判断区间是空还是满
        // 写满了，就是生产满了，占完整个 buffer size 了
        _ms(100);
        std::cout << "Producer is waiting for an empty slot" << std::endl;
        (src -> not_full).wait(lock); // 当前线程调用 wait 函数，将处于阻塞状态，直到另外一个线程 notify 才能唤醒接着执行
        // 注意这个时候处于阻塞状态，当前线程 wait 会自动调用 lock.unlock() 对互斥锁解锁，使得其他被阻塞在需要当前互斥锁的线程继续执行
        // 当本函数的线程被 notify 唤醒之后，wait 函数恢复执行，并自动调用 lock.lock() 对互斥锁加锁
        // 生产者等待另外消费者线程来唤醒当前线程
    }

    (src -> buf)[src -> write_pos] = item; // 写入产品
    src -> write_pos++;
    if (src -> write_pos == bufSize) {
        src -> write_pos = 0;
    }
    (src -> not_empty).notify_all(); // 通知消费者产品库不是空
}

int Consumer(resource* src) {
    int data; // 返回的是产品
    std::unique_lock<std::mutex> lock(src -> mtx);
    while (src -> write_pos == src -> read_pos) {
        // _ms(100);
        std::cout << "Consumer is waiting for items ..." << std::endl;
        (src -> not_empty).wait(lock);
    }
    data = (src -> buf)[src -> read_pos];
    src -> read_pos++;
    if (src -> read_pos == bufSize) {
        src -> read_pos = 0;
    }
    (src -> not_full).notify_all();
    return data;
}

void ProducerTask() { // 生产者任务
    for (int i = 1; i <= proNum; i++) {
        _ms(100);
        std::cout << "Producer the " << i << "th item..." << std::endl;
        Producer(&instance, i);
    }
}

void ConsumerTask() { // 消费者任务
    static int cnt = 0;
    while (1) {
        sleep(1); // unistd.h 里面的函数，表示是休眠 1 s
        int item = Consumer(&instance);
        std::cout << "Consumer the " << item << "th item" << std::endl;
        _ms(100);
        if (++cnt == proNum) break; // 表示的是生产完成了,我们最多生产的个数是 20 个
    }
}

void Initresource(resource* src) {
    src -> read_pos = 0;  // 初始化产品读取的位置,初始化都先设置成 0
    src -> write_pos = 0;  // 初始化产品写入的位置
}

int main() {
    Initresource(&instance);
    std::thread producer(ProducerTask);
    std::thread consumer(ConsumerTask);
    producer.join();
    consumer.join(); // join 需要等到当前线程执行结束再结束整个进程
    // getchar();
    return 0;
}



// 版本二： 单生产者，多消费者

typedef std::chrono::milliseconds _ms;
static const int bufSize = 8;
static const int proSize = 30;
struct resource_ {
    int buf[bufSize];
    int read_pos;
    int write_pos;
    std::mutex mtx;
    std::mutex item_cnt_mutex; // 注意需要一个对 item_cnt 的互斥变量，互斥访问 item_cnt 这个变量
    std::condition_variable not_full;
    std::condition_variable not_empty;
    int item_cnt;
} instance;


typedef struct resource_ resource;

void Producer(resource* src, int item) {
    std::unique_lock<std::mutex> lock(src -> mtx);
    while ((src -> write_pos + 1) % bufSize == src -> read_pos) {
        std::cout << "Producer is waiting for a empty slot!" << std::endl;
        _ms(100);
        (src -> not_full).wait(lock);
    }
    (src -> buf)[src -> write_pos] = item;
    src -> write_pos++;
    if (src -> write_pos == bufSize) {
        src -> write_pos = 0;
    }
    (src -> not_empty).notify_all();
}

int Consumer(resource* src) {
    int data;
    std::unique_lock<std::mutex> lock(src -> mtx);
    while (src -> write_pos == src -> read_pos) {
        std::cout << "Consumer is waiting for items!" << std::endl;
        _ms(100);
        (src -> not_empty).wait(lock);
    }
    data = (src -> buf)[src -> read_pos];
    src -> read_pos++;
    if (src -> read_pos == bufSize) {
        src -> read_pos = 0;
    }
    (src -> not_full).notify_all();
    return data;
}

void ProducerTask() {
    for (int i = 1; i <= proSize; i++) {
        _ms(100);
        std::cout << "Producer the " << i << "th item." << std::endl;
        Producer(&instance, i);
    }
}

void ConsumerTask() {
    while (1) {
        sleep(1);
        // 因为有很多个消费者，所以对消费的数量进行访问的时候需要定义一个互斥锁
        std::unique_lock<std::mutex>lock_item_cnt(instance.item_cnt_mutex);
        if (instance.item_cnt < proSize) {
            int item = Consumer(&instance);
            ++(instance.item_cnt);
            std::cout << "Consumer thread: " << std::this_thread::get_id() << " is consuming the " << item << "th item" << std::endl;
        } else {
            break;
        }
    }
    std::cout << "Consumer thread " << std::this_thread::get_id() << "is existing..." << std::endl;
}

void Initresource(resource* src) {
    src -> read_pos = 0;
    src -> write_pos = 0;
    src -> item_cnt = 0;
}

int main() {
    Initresource(&instance);
    std::thread producer(ProducerTask);
    std::thread consumer1(ConsumerTask);
    std::thread consumer2(ConsumerTask);
    std::thread consumer3(ConsumerTask);
    std::thread consumer4(ConsumerTask);
    std::thread consumer5(ConsumerTask);
    producer.join();
    consumer1.join();
    consumer2.join();
    consumer3.join();
    consumer4.join();
    consumer5.join();
    return 0;
}




// 版本三： 多生产者，单消费者

typedef std::chrono::milliseconds _ms;
static const int bufSize = 10;
static const int proSize = 30;
struct resource_ {
    int buf[bufSize];
    int write_pos;
    int read_pos;
    int item_cnt;
    std::mutex mtx;
    std::mutex item_cnt_mutex;
    std::condition_variable not_full;
    std::condition_variable not_empty;
} instance;

typedef struct resource_ resource;

void Producer(resource* src, int item) {
    std::unique_lock<std::mutex> lock(src -> mtx);
    while ((src -> write_pos + 1) % bufSize == src -> read_pos) {
        std::cout << "Producer is waiting a empty slot" << std::endl;
        (src -> not_full).wait(lock);
    }
    (src -> buf)[src -> write_pos] = item;
    src -> write_pos++;
    if (src -> write_pos == bufSize) {
        src -> write_pos = 0;
    }
    (src -> not_empty).notify_all();
}

int Consumer(resource* src) {
    int data;
    std::unique_lock<std::mutex> lock(src -> mtx);
    while (src -> write_pos == src -> read_pos) {
        std::cout << "Consumer is waiting a item!" << std::endl;
        (src -> not_empty).wait(lock);
    }
    data = (src -> buf)[src -> read_pos];
    src -> read_pos++;
    if (src -> read_pos == bufSize) {
        src -> read_pos = 0;
    }
    (src -> not_full).notify_all();
    return data;
}

void ProducerTask() {
    while (1) {
        sleep(1);
        std::unique_lock<std::mutex> lock_item_cnt(instance.item_cnt_mutex);
        if (instance.item_cnt < proSize) {
            instance.item_cnt++;
            Producer(&instance, instance.item_cnt);
            std::cout << "Producer thread " << std::this_thread::get_id() << " is producing the " << instance.item_cnt << "th item!" << std::endl;
        } else {
            break;
        }
    }
    std::cout << "Producer thread " << std::this_thread::get_id() << " is existing..." << std::endl;
}

void ConsumerTask() {
    static int cnt = 0;
    while (1) {
        sleep(2);
        int item = Consumer(&instance);
        std::cout << "Consumer id : " << std::this_thread::get_id() << " is consuming the " << item << "th item!" << std::endl;
        cnt++;
        if (cnt == proSize) break;
    }
    std::cout << "Consumer thread " << std::this_thread::get_id() << " is existing..." << std::endl;
}

void Initresource(resource* src) {
    src -> write_pos = 0;
    src -> read_pos = 0;
    src -> item_cnt = 0;
}

int main() {
    Initresource(&instance);
    std::thread producer1(ProducerTask);
    std::thread producer2(ProducerTask);
    std::thread producer3(ProducerTask);
    std::thread producer4(ProducerTask);
    std::thread consumer(ConsumerTask);
    producer1.join();
    producer2.join();
    producer3.join();
    producer4.join();
    consumer.join();
    return 0;
}

// 版本四： 多生产者，多消费者

```



#### 读者写者

```cpp
// 写优先



// 普通的读者写者



```





#### 哲学家进餐

```cpp


```





#### 快慢指针-链表及其数学证明: 存在环，环的长度，环的指针

```c++
// 链表找环使用这种写法，找链表的中间节点也是使用这种解法，但是特殊情况可能需要特殊的处理：
bool hasCycle(ListNode *head) {
    if (head == nullptr) return false;
    ListNode* slow = head;
    ListNode* fast = head;  // 注意模板，就应该这样写，方便，同时容易判断，其他题目可能需要视情况而定
    while (fast != nullptr && fast -> next != nullptr) {
        slow = slow -> next;
        fast = fast -> next -> next;
        if (slow == fast) return true;
    }
    return false;
}
// 检测环的入口节点的值
ListNode *detectCycle(ListNode *head) {
    if (head == nullptr) return nullptr;
    ListNode* slow = head;
    ListNode* fast = head;
    bool flag = 0;
    while (fast != nullptr && fast -> next != nullptr) {

        slow = slow -> next;
        fast = fast -> next -> next;
        if (slow == fast) {
            flag = 1;
            break;
        }

    }
    if (!flag) return nullptr; 
    slow = head;
    while (fast != slow) {
        fast = fast -> next;
        slow = slow -> next;
    }
    return slow;
}
```

* 数学理论：见 IPad

#### 寻找链表的中间节点



```c++
ListNode* solve(ListNode* head) {
    // 我们更倾向使用下面的方式,注意和上面快慢指针区别
    if (head == nullptr) return nullptr;
    ListNode* slow = head;
    ListNode* fast = head -> next;  // 注意这个和找链表环的区别
    while (fast != nullptr && fast -> next != nullptr) {
        slow = slow -> next;
        fast = fast -> next -> next;
    }
    ListNode* second = slow -> next;
    slow -> next = nullptr;// 注意需要将两个链表给分开
    return second; // 返回的是后半部分的链表头结点,注意无论是奇数还是偶数个节点都是返回的后半部分
}
```



#### LeetCode 23 合并 k 个升序链表

```c++
// 解法一：使用优先队列：
// 时间复杂度 ： O(nklogk) 空间复杂度 ： O(k) k 个链表均要存放在优先队列中
ListNode* mergeKLists(vector<ListNode*>& lists) {
    // 也许，需要将数据存储到 优先队列中，这样可以简化我们的处理时间
    if (lists.size() == 0) return nullptr;
    if (lists.size() == 1) return lists[0];
    priority_queue<Node>pq;
    for (int i = 0; i < lists.size(); i++) {
        if (lists[i] == nullptr) continue;
        pq.push(Node(lists[i]));
    }
    ListNode* node = new ListNode(-1);
    ListNode* res = node;
    while (!pq.empty()) {
        Node top = pq.top();
        pq.pop();
        // if (pq == nullptr) continue;
        node -> next = top.node;
        node = node -> next;
        auto top1 = top.node -> next;
        if (top1 != nullptr) {
            pq.push(Node(top1));
        }
    }
    return res -> next;
}
// 解法二：归并排序，时间复杂度 ：O(nklogk),空间复杂度 ：O(logk) 栈空间
 ListNode* Merge(ListNode* l, ListNode* r) {
     ListNode* node = new ListNode(-1);
     ListNode* res = node;
     while (l && r) {
         if (l -> val < r -> val) {
             node -> next = l;
             l = l -> next;
         } else {
             node -> next = r;
             r = r -> next;
         }
         node = node -> next;
     }
     node -> next = l != nullptr ? l : r;
     return res -> next;
 }
ListNode* MergeAndSort(vector<ListNode*>& lists, int left, int right) {
    if (left > right) return nullptr;
    if (left == right) return lists[left];  // 返回去这样方便我们的合并处理
    int mid = (right - left) / 2 + left;
    ListNode* l = MergeAndSort(lists, left, mid);
    ListNode* r = MergeAndSort(lists, mid + 1, right);
    return Merge(l, r);
}
```



#### 回文链表

```cpp
		bool isPalindrome(ListNode* head) {
        // 边遍历，边反转
        // 先找到中点，但是边反转边找中间结点
        if (head == nullptr) return true;
        if (head -> next == nullptr) return true;
        ListNode* slow = head;
        ListNode* fast = head;
        ListNode* node = new ListNode(-1);
        ListNode* second;
        while (fast && fast -> next) {
            fast = fast -> next -> next;
            ListNode* next = slow -> next;
            slow -> next = node -> next;
            node -> next = slow;
            slow = next;
            second = next;
        }
        // 经过上面的步骤一定可以将一半的元素反转了，不需要考虑奇数或者偶数个
        
        if (fast != nullptr && fast -> next == nullptr) {
            // 奇数个,需要单独考虑，偶数个不需要
            second = second -> next;
        }
        ListNode* first = node -> next;
        while (first) {
            if (first -> val != second -> val) return false;
            first = first -> next;
            second = second -> next;
        }
        return true;
    }
```



```c++
bool isPalindrome(ListNode* head) {
    // 边遍历边翻转链表,然后再遍历一次进行比较，注意链表个数的奇数和偶数需要分别的讨论, 时间复杂度 ： O(n)，空间复杂度 ： O(1)
    if (head == nullptr) return true;
    if (head -> next == nullptr) return true;
    if (head -> next -> next == nullptr) return head -> val == head -> next -> val;
    ListNode* node = new ListNode(-1);
    ListNode* res = node;
    ListNode* slow = head;
    ListNode* fast = head -> next;
    while (fast != nullptr && fast -> next != nullptr) {
        ListNode* temp = slow -> next;
        slow -> next = node -> next;
        node -> next = slow;
        slow = temp;
        fast = fast -> next -> next;
    }
    cout << slow -> val << endl;
    ListNode* second = slow -> next;
    slow -> next = node -> next;
    node -> next = slow;
    // slow -> next = nullptr;
    if (fast == nullptr) {
        res = res -> next -> next;
        while (res) {
            if (res -> val != second -> val) return false;
            res = res -> next;
            second = second -> next;
        }
        return true;
    }
    // 注意不能先 if (fast -> next == nullptr) 因为可能存在 fast == nullptr 的情况
    // 表示偶数个节点
    res = res -> next;
    while (res) {
        if (res -> val != second -> val) return false;
        res = res -> next;
        second = second -> next;
    }
    return true;
}
```



#### 删除链表中的某个节点:三种解法

```c++
// 这种删除方式还是会保留一个节点，就是重复的节点
// leetcode 203
ListNode* removeElements(ListNode* head, int val) {
    // if (head == nullptr) return nullptr;
    // if (head -> next == nullptr) return head -> val == val ? nullptr : head;
    // 三种解法
    // 解法一： 使用临时的节点
    ListNode* node = new ListNode(-1);
    ListNode* res = node;
    node -> next = head;
    while (node -> next != nullptr) {
        if (node -> next -> val == val) {
            node -> next = node -> next -> next;
        } else {
            node = node -> next;
        }
    }
    return res -> next;

    // 解法二：不使用临时节点
    while (head != nullptr && head -> val == val) {
        head = head -> next;
    }
    ListNode* node = head;
    if (head != nullptr) {
        while (head -> next != nullptr) {
            if (head -> next -> val == val) {
                head -> next = head -> next -> next;
            } else {
                head = head -> next;
            }
        }
    }
    return node;

    // 解法三：使用递归求解,思考链表递归的解法
    if (head == nullptr) {
        return head;
    }
    head -> next = removeElements(head -> next, val); // 先执行这个，因为下面会用到 head -> next
    if (head -> val == val) return head -> next;
    else return head;
}
```



#### 删除排序链表重复元素

* #### [82. 删除排序链表中的重复元素 II](https://leetcode.cn/problems/remove-duplicates-from-sorted-list-ii/) [83. 删除排序链表中的重复元素](https://leetcode.cn/problems/remove-duplicates-from-sorted-list/)

```cpp
// 83 保留一个重复的元素， 递归解法
class Solution {
public:
    ListNode* deleteDuplicates(ListNode* head) {
        // 递归
        /*
        if (head == nullptr || head -> next == nullptr) return head;
        
        if (head -> val == head -> next -> val) { // 注意与下面一题的 while 做比较
            return deleteDuplicates(head -> next); // 这里就会保留一个重复的元素
        } else {
            head -> next = deleteDuplicates(head -> next);
            return head;
        }
        */
        if (!head || !head -> next) return head;
        head -> next = deleteDuplicates(head -> next); // 链表需要被串起来，所以这一步不能少
        if (head -> val == head -> next -> val) return head -> next;
        return head;  // 和上面的删除一个元素是一个道理
    }
};

// 迭代做法
class Solution {
public:
    ListNode* deleteDuplicates(ListNode* head) {
        // 迭代
        if (head == nullptr || head -> next == nullptr) return head;
        
        ListNode* res = head;
        
        while (head -> next) {
            if (head -> val == head -> next -> val) {
                head -> next = head -> next -> next;
            } else {
                head = head -> next;
            }
        }
        
        return res;
    }
};
```



```cpp
// 82 重复元素全部删除,不做任何保留
// 递归做法，可以仔细思考链表的递归解法
class Solution {
public:
    ListNode* deleteDuplicates(ListNode* head) {
        // 递归
        if (head == nullptr || head -> next == nullptr) return head; // 通常这样写,递归一定这样写
        
        ListNode* p = head -> next;
        
        if (p -> val == head -> val) {
            // 我们需要做到的是过滤掉所有的重复的节点
            while (p && p -> val == head -> val) { // 上面的 if 换成 while
                p = p -> next;
            }
            head -> next = deleteDuplicates(p);  // 链表是一定需要被串起来的
            return head -> next; // 这样就直接过滤掉所有和 head -> val 相同的节点
        } else {
            head -> next = deleteDuplicates(head -> next);
            return head;
        }
    }
};

// 迭代做法
class Solution {
public:
    ListNode* deleteDuplicates(ListNode* head) {
        // 迭代
        if (head == nullptr || head -> next == nullptr) return head;

        ListNode* node = new ListNode(-1);  // 注意必须需要这个节点，否则可能出现错误
        ListNode* res = node;  // 注意最终是 node 将这些节点串联起来的

        while (head && head -> next) {
            ListNode* p = head -> next;
            if (p -> val == head -> val) {
                while (p && p -> val == head -> val) {
                    p = p -> next;
                }
                head = p;
            } else {
                node -> next = head;
                node = node -> next;
                head = head -> next;
            }
        }
        
        // 注意下面代码，上面可能 head -> next == nullptr 但是 head 还是符合条件的
        // if (head) node -> next = head;
        // else node -> next = nullptr;
        node -> next = head;
        return res -> next;
    }
};
```





#### 最长递增子序列(O(n^2) && O(nlogn) 解法)

```c++
int lengthOfLIS(vector<int>& nums) {
    int n = nums.size();
    if (n <= 1) return n;
    vector<int>dp(n, 1);
    int res = 1;
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < i; j++) {
            if (nums[i] > nums[j]) {
                dp[i] = max(dp[j] + 1, dp[i]);
                res = max(res, dp[i]);
            }
        }
    }
    return res;

    // 二分法求解
    int n = nums.size();
    if (n <= 1) return n;
    // 二分方式，将其放入到一个数组中，数组存的数是牌的点数，小的可以 覆盖掉大的数字，太大就需要重行的开辟数组空间了
    vector<int>dp(n, 0);
    int piles = 0;
    for (int i = 0; i < n; i++) {
        int num = nums[i];
        int l = 0, r = piles;
        // 注意，一定是优先选择左边边界的元素
        while (l < r) {
            int mid = (r - l) / 2 + l;
            if (nums[mid] >= num) {
                r = mid;
            } else {
                l = mid + 1;
            }
        }
        if (l == piles) {
            piles++;  // 重行开拓一个位置
        }
        nums[l] = num;
    }
    return piles;
}

// LIS 的总共个数
int findNumberOfLIS(vector<int>& nums) {
    int n = nums.size();
    if (n <= 1) return n;
    vector<int>dp(n, 1), cnt(n, 1);
    int res = 1;
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < i; j++) {
            if (nums[i] > nums[j]) {
                if (dp[j] + 1 > dp[i]) {  // 新的 LIS 长度
                    cnt[i] = cnt[j];
                    dp[i] = dp[j] + 1;
                } else if (dp[j] + 1 == dp[i]) { // 说明之前已经存在过这种答案了
                    cnt[i] = cnt[i] + cnt[j];
                }
                res = max(res, dp[i]);
            }
        }
    }
    int ans = 0;
    for (int i = 0; i < n; i++) {
        if (dp[i] == res) ans += cnt[i];
    }
    return ans;
}
```



#### 将数组中的奇数放前面，偶数放后面，0 放中间

```c++
void solve(vector<int>& nums) {
    int l = 0, r = nums.size() - 1;
    while (l < r) {
        while (l < r && nums[l] % 2 == 1) l++;
        while (l < r && nums[r] % 2 == 0) r--;
        if (l < r) {
            swap(nums[l], nums[r]);
            l++;
            r--;
        }
    }
    int right = nums.size() - 1;
    int i = nums.size() - 1;
    for (; i >= 0 && nums[i] % 2 != 1; i--) {
        if (nums[i] != 0) {
            nums[right] = nums[i];
            right--;
        }
    }
    for (int k = right; k > i; k--) {
        nums[k] = 0;
    }
}

```



#### 移除字符串空格或指定元素：O(n)

```c++
void removeSpace(string& s) {
    // 删除前面的空格
    int left = 0;
    while (left < s.size() && s[left] == ' ') {
        left++;
    }
    // 之后将字符赋值到最前面,同时删除单词中间可能存在的空格
    // 注意，单词之间是可以存在一个空格的
    int low_index = 0;
    for (; left < s.size(); left++) {
        if (left + 1 < s.size() && s[left] == s[left + 1] && s[left] == ' ') {
            continue;
        } else {
            s[low_index] = s[left];
            low_index++;
        }
    }
    // 最后是删除最后的空格，直接可以使用 resize 解决，直接 resize 成 low_index 的格式
    // 但是需要注意的是我们可能最后一个 low_index 是 空格，这个时候需要单独处理
    if (low_index - 1 > 0 && s[low_index - 1] == ' ') {
        s.resize(low_index - 1);
    } else {
        s.resize(low_index);
    }
}
```

#### excel 题目 LeetCode 168 171

```c++
 string convertToTitle(int columnNumber) {
     string res;
     while (columnNumber) {
         columnNumber--;  // 注意将其变成 0-25 直接的26进制，然后就方便处理了
         res.push_back(columnNumber % 26 + 'A');
         columnNumber /= 26;
     }
     reverse(res.begin(), res.end());
     return res;
 }

int titleToNumber(string columnTitle) {
    int res = 0;
    int p = 0;
    for (int i = columnTitle.size() - 1; i >= 0; i--) {
        res = res + (columnTitle[i] - 'A' + 1) * pow(26, p);
        p++;
    }
    return res;
}

```

#### 克隆图：深拷贝

```c++
unordered_map<Node*, Node*>mp;  // 存放的是已经被访问的节点以及其对应的克隆节点
Node* cloneGraph(Node* node) {
    // DFS 如何求解
    if (node == nullptr) return node;
    if (mp.find(node) != mp.end()) {
        return mp[node];
    }
    Node* cloneNode = new Node(node -> val);
    mp[node] = cloneNode;
    for (auto& nei : node -> neighbors) {
        mp[node] -> neighbors.push_back(cloneGraph(nei));
    }
    return cloneNode;
    // BFS 求解
    if (node == nullptr) return node;
    queue<Node*>que;
    mp[node] = new Node(node -> val);
    que.push(node);
    while (!que.empty()) {
        auto front = que.front();
        que.pop();
        for (auto& nei : front -> neighbors) {
            if (mp.find(nei) == mp.end()) {
                mp[nei] = new Node(nei -> val); 
                que.push(nei); // 这个是原来的节点
            }
            mp[front] -> neighbors.push_back(mp[nei]); // 注意这个是对当前的 front 节点对应的新建的节点而言的
        }
    }
    return mp[node];
}
```



#### leetcode 46 全排列

```c++
// 包含重复的元素
// 注意这里需要使用一个 vis 数组记录哪些元素是遍历过的。 只有全排列需要，组合数不需要
// 全排列需要 idx 判断元素个数。
void solve(vector<int>&nums, vector<vector<int>>& res, vector<int>&temp, int idx, vector<int>&vis) {
    if (idx == nums.size()) {
        res.push_back(temp);
        return;
    }
    for (int i = 0; i < nums.size(); i++) {  // for 是从 0 开始，因为可能回过头访问之前的元素
        if (vis[i] == 0) { // 避免重复的访问，只有全排列需要使用 vis 数组确定某个元素是否被重复访问了
            vis[i] = 1;
            temp.push_back(nums[i]);
            solve(nums, res, temp, idx + 1, vis);
            temp.pop_back();
            vis[i] = 0;
        }
    }
}
vector<vector<int>> permute(vector<int>& nums) {
    vector<vector<int>>res;
    vector<int>temp;
    vector<int>vis(nums.size());
    solve(nums, res, temp, 0, vis);
    return res;
}

// 去重，注意一定需要提前排序，否则一定出错
void solve(vector<int>& nums, vector<vector<int>>& res, vector<int>&temp, vector<int>&vis, int idx) {
    if (idx == nums.size()) {
        res.push_back(temp);
        return;
    }
    for (int i = 0; i < nums.size(); i++) {
        if (i > 0 && nums[i - 1] == nums[i] && vis[i - 1] == 0) continue; // 注意vis[i - 1] == 0表示 这个元素还没有访问，这个也一定需要有
        // 否则 可能第一次 1,1,2 出现错误
        if (vis[i] == 0) {
            vis[i] = 1;
            temp.push_back(nums[i]);
            solve(nums, res, temp, vis, idx + 1);
            temp.pop_back();
            vis[i] = 0;
        }
    }
}
vector<vector<int>> permuteUnique(vector<int>& nums) {
    vector<vector<int>>res;
    vector<int>temp;
    vector<int>vis(nums.size());
    sort(nums.begin(), nums.end()); // 注意这个必须要排序否则可能出错
    solve(nums, res, temp, vis, 0);
    return res;
}
```

#### leetcode 39 & 329 组合回溯相关题目

```c++
// leetcode 77 组合
// 组合数题目不需要使用 vis 判断某个元素是否被访问过了，只有全排列需要使用，因为全排列是从 0 开始 for 循环的
// 组合数的 for 循环是从 idx 开始的
// 组合数需要 idx 用于寻找下一个元素的位置
void solve(vector<vector<int>>& res, vector<int>& temp, vector<int>& vis, int n, int k, int idx) {
    // if (idx > n) return;
    if (temp.size() == k) {
        res.push_back(temp);
        return;
    }
    for (int i = idx; i <= n; i++) {  // 注意需要从 idx 开始，因为一定是从某一位向后遍历
        //if (vis[i] == 0) {
            //vis[i] = 1;
        temp.push_back(i);
        solve(res, temp, vis, n, k, i + 1);
        //vis[i] = 0;
        temp.pop_back();
        //}
    }
}
vector<vector<int>> combine(int n, int k) {
    vector<vector<int>> res;
    vector<int>temp;
    vector<int>vis(n + 1);
    solve(res, temp, vis, n, k, 1);
    return res;
}

// leetcode 17 电话号码
string nums[] = {"", "", "abc", "def", "ghi", "jkl", "mno", "pqrs", "tuv", "wxyz"};
class Solution {
public:
    void solve(vector<string>&res, string& temp, const string& digits, vector<int>& vis, int idx, int len) {
        if (temp.size() == len) {
            res.push_back(temp);
            return;
        }
        
        string str = nums[digits[idx] - '0'];  // 每一个单词都是会访问遍历的。
        for (int j = 0; j < str.size(); j++) {
            temp.push_back(str[j]);
            solve(res, temp, digits, vis, idx + 1, len);
            temp.pop_back();
        }
    }
    vector<string> letterCombinations(string digits) {
        vector<string>res;
        string temp;
        vector<int>vis(10);
        int len = digits.size();
        if (len == 0) return res;
        solve(res, temp, digits, vis, 0, len);
        return res;
    }
};
// 组合总数 39 每个元素可以使用 无限次 并且不能包含重复的,原来数组中不包含重复的元素
void solve(vector<int>& candidates, vector<vector<int>>& res, vector<int>& temp, int idx, int target, int cur_sum) {
    if (cur_sum == target) {
        res.push_back(temp);
        return;
    }
    for (int i = idx; i < candidates.size(); i++) { // 不包含重复的
        if (cur_sum + candidates[i] > target) continue;
        temp.push_back(candidates[i]);
        solve(candidates, res, temp, i, target, cur_sum + candidates[i]);  // 注意这个传进去的 idx 是 i 而不是 i + 1 因为我们某一个数可以重复使用，所以不是 i + 1, 如果不能重复使用那么是 i + 1
        // 同时这里不能从 0 开始，因为这样会输出重复的数据，变成全排列的情况了
        temp.pop_back();
    }
}
vector<vector<int>> combinationSum(vector<int>& candidates, int target) {
    vector<vector<int>>res;
    vector<int>temp;
    solve(candidates, res, temp, 0, target, 0);
    return res;
}

// 组合数II 40 ： 每个元素只能使用 1 次 且 不能包含重复的
// 需要使用 vis 数组，因为需要进行去重的操作
// 同时也可以不使用 vis 数组进行去重, 注意只有 全排列 才必须需要使用 vis 数组去重
void solve(vector<int>& candidates, vector<vector<int>>& res, vector<int>& temp, int target, int idx, int cur_sum, vector<int>& vis) {
    if (cur_sum == target) {
        res.push_back(temp);
        return;
    }
    for (int i = idx; i < candidates.size(); i++) {
        //if (vis[i] == 1) continue;
        if (cur_sum + candidates[i] > target) continue;
        // if (i > 0 && candidates[i] == candidates[i - 1] && vis[i - 1] == 0) continue;
        //vis[i] = 1;
        if (i > idx && candidates[i] == candidates[i - 1]) continue; // 非常常见的去重方式，需要掌握.注意 这个 for 循环是 i = idx， 不是 从 0 开始的.
        // i > idx 而不是 i > 0 的原因是防止其删除了不应该删除的重复数字
        temp.push_back(candidates[i]);
        solve(candidates, res, temp, target, i + 1, cur_sum + candidates[i], vis);
        temp.pop_back();
        //vis[i] = 0;
    }
}
vector<vector<int>> combinationSum2(vector<int>& candidates, int target) {
    sort(candidates.begin(), candidates.end());
    vector<vector<int>>res;
    vector<int>temp;
    vector<int>vis(candidates.size());
    solve(candidates, res, temp, target, 0, 0, vis);
    return res;
}


// 组合总数 |||

void solve(vector<vector<int>> &res, vector<int>& temp, int k, int n, int idx, int cur_sum) {
    if (temp.size() > k) return;
    if (n == 0 && temp.size() == k) {
        res.push_back(temp);
        return;
    }
    for (int i = idx; i <= 9; i++) {
        // if (cur_sum + i > n) continue; // 使用这个和下面的判断语句均可以求解
        if (n - i < 0) continue;
        temp.push_back(i);
        solve(res, temp, k, n - i, i + 1, cur_sum + i);
        temp.pop_back();
    }
}
vector<vector<int>> combinationSum3(int k, int n) {
    vector<vector<int>> res;
    vector<int>temp;
    solve(res, temp, k, n, 1, 0);
    return res;
}


```







#### 逆波兰式

```c++
int solve(string& s1, int num1, int num2) {
    if (s1 == "+") {
        return num1 + num2;
    } else if (s1 == "-") {
        return num1 - num2;
    } else if (s1 == "*") {
        return num1 * num2;
    }
    return num1 / num2;
}
int string2int(string& s) {
    int res = 0;
    int flag = 1;
    for (int i = 0; i < s.size(); i++) {
        if (s[i] == '-') {
            flag = -1;
            continue;
        }
        if (s[i] == '+') continue;
        res = res * 10 + s[i] -'0';
    }
    return res * flag;
}
bool isNum (const string& s) {
    if (s.size() == 1 && (s == "+" || s == "-" || s == "*" || s == "/")) return false;
    return true;
}
int evalRPN(vector<string>& tokens) {
    // 使用一个栈进行相应的操作，当存在数字的时候入栈，当存的的是字符的时候出两个数字
    stack<int>st;
    for (int i = 0; i < tokens.size(); i++) {
        if (isNum(tokens[i])) {
            st.push(string2int(tokens[i]));
        } else {
            int num2 = st.top();
            st.pop();
            int num1 = st.top();
            st.pop();
            int res = solve(tokens[i], num1, num2);
            st.push(res);
        }

    }
    return st.top();
}
```

#### 汉诺塔问题

```c++
int res;
void solve(vector<int>& A, vector<int>& B, vector<int>& C, int n) {
    if (n == 1) {
        // 表示最后一个盘子了
        res++;
        C.push_back(A.back());
        A.pop_back();
        return;
    }
    solve(A, C, B, n - 1);
    res++;
    C.push_back(A.back()); // 最终将A 的最后一个移动到 C 中
    A.pop_back();
    solve(B, A, C, n - 1);
}
void hanota(vector<int>& A, vector<int>& B, vector<int>& C) {
    // A -> C 可以使用 借助 B 当做中间盘，然后放到 C
    // A -> B A 通过 C 移动 n -1 个盘子， A -> C A将最后一个盘子移动到 C 中
    // 之后 B 将 n - 1 个盘子通过A 移动到 C， 这么 又变成 A -> C 只不过这个时候盘子个数减一 
    int n = A.size();
    res = 0;
    solve(A, B, C, n);
    cout << res << endl; // 总共执行的次数
}
```



#### 计算器 : 表达式计算问题

```c++
void removeSpace(string& s) {
    int left = 0;
    for (int i = 0; i < s.size(); i++) {
        if (s[i] != ' ') {
            s[left] = s[i];
            left++;
        }
    }
    s.resize(left);
}

void eval(stack<int>& num, stack<char>& ops) {
    int num1 = num.top();
    num.pop();
    int num2 = num.top();
    num.pop();
    char c = ops.top();
    ops.pop();
    int ans;
    if (c == '+') {
        ans = num1 + num2;
    } else if (c == '-') {
        ans = num2 - num1;
    } else if (c == '*') {
        ans = num1 * num2;
    } else {
        ans = num2 / num1;
    }
    num.push(ans);
}
int calculate(string s) {
    // 解法一： 使用两个栈一个来存储数字，一个用来存储运算符
    num.clear();
    ops.clear();
    stack<int>num;
    stack<char>ops;
    unordered_map<char, int>mp;
    mp['+'] = mp['-'] = 1;
    mp['*'] = mp['/'] = 2;
    removeSpace(s);
    // cout << s << endl;
    for (int i = 0; i < s.size();) {
        // 因为表达式中所有的整数都是非负的整数，所以这个不会出现错误
        if (isdigit(s[i])) {
            // int temp = s[i] - 'a';
            ll temp = 0;
            while (i < s.size() && isdigit(s[i])) {
                temp = temp * 10 + s[i] - '0';
                i++;
            }
            num.push(temp);
        } else {
            // 表示的是运算符, 当顶部的运算符优先级比当前的运算符优先级大的时候我们可以直接运算
            while (ops.size() >= 1 && mp[ops.top()] >= mp[s[i]]) eval(num, ops);
            ops.push(s[i]);
            i++;
        }
    }
    while (!ops.empty()) {
        eval(num, ops);
    }
    return num.top();


    // 解法二： 表达式中没有括号，大大降低运算的复杂度,只使用一个栈，因为我们将减号变成加号直接取反求解,之后，只需要简单的额求解加法就行了
    // 思路就是将所有的运算都变成加法进行求解
    removeSpace(s);
    stack<int>nums;
    char sign = '+';
    int d = 0;
    for (int i = 0; i < s.size();) {
        if (isdigit(s[i])) {
            int temp = 0;
            while (i < s.size() && isdigit(s[i])) {
                temp = temp * 10 - '0' + s[i];
                i++;
            }
            d = temp;
        } else {
            if (sign == '+') {
                nums.push(d);
            } else if (sign == '-') {
                // 注意这个是保存的是上一个元素的情况
                nums.push(-d);
            } else if (sign == '*') {
                int num = nums.top();
                nums.pop();
                num = num * d;
                nums.push(num);
            } else {
                int num = nums.top();
                num /= d;
                nums.pop();
                nums.push(num);
            }
            sign = s[i];
            d = 0;
            i++;
        }
    }
    // 注意每一次出来的都是上一个运算符，和上一个数字，所以最后一次还有一个运算符和数字没有出来，一定不要忘记了
    if (sign == '+') {
        nums.push(d);
    } else if (sign == '-') {
        nums.push(-d);
    } else if (sign == '*') {
        int num = nums.top();
        nums.pop();
        nums.push(num * d);
    } else {
        int num = nums.top();
        nums.pop();
        nums.push(num / d);
    }

    int res = 0;
    while (!nums.empty()) {
        res += nums.top();
        nums.pop();
    }
    return res;
    // 解法三： 使用逆波兰的方式求解，首先需要将其转成逆波兰式，然后计算逆波兰式，逆波兰式直接使用 栈求解

}
```



#### 多项式乘法 leetcode :43 i 位 * j 位 最终存放在数组 [i + j, i + j + 1]中

```c++
string multiply(string num1, string num2) {
    // i位 和 j 位相乘 最终的位置是 : [i +j, i + j + 1]， 高位，低位
    // 0, 1, 2, 3, 4,...., i + j, i + j + 1,...., m + n + -1 值越小表示的是越高位
    int n = num1.size();
    int m = num2.size();
    if (n == 1 && num1 == "0") return "0";
    if (m == 1 && num2 == "0") return "0";
    if (n < 1 || m < 1) return "";
    vector<int>nums(m + n);
    // 按照我们正常的思维从后向前乘
    for (int i = n - 1; i >= 0; i--) { // 注意我们需要从后向前遍历
        for (int j = m - 1; j >= 0; j--) {
            int num = (num1[i] - '0') * (num2[j] - '0');
            num += nums[i + j + 1];  // 存放的是低位
            nums[i + j] += num / 10;
            nums[i + j + 1] = num % 10;
        }
    }
    string res;
    int i = 0;
    while (i < nums.size() && nums[i] == 0) i++;
    for (; i < nums.size(); i++) {
        res.push_back(nums[i] + '0');
    }
    return res;
}
```

#### 交替删除链表，返回奇数位和偶数位下标的链表

```c++
// 只有一个头结点，返回奇数和偶数下标的链表
void solve(ListNode* head) {
    if (head == nullptr) return nullptr;
    ListNode* even = head;
    ListNode* odd = head -> next;
    while (head -> next) {
        ListNode* node = head -> next;
        head -> next = head -> next -> next;
        head = node;  // 交替删除
    }
}
```

#### 奇偶下标链表分拆

```cpp
class Solution {
public:
    ListNode* oddEvenList(ListNode* head) {
        if (!head || !head -> next) return head;
        ListNode* node1 = head;
        ListNode* node2 = head -> next;
        ListNode* res1 = node1;
        ListNode* res2 = node2;
        while (node2 && node2 -> next) {  // 注意凡是 node2 -> next 需要满足条件的时候，一定要先让node2 满足条件
            node1 -> next = node2 -> next;
            node2 -> next = node1 -> next -> next;
            node1 = node1 -> next;
            node2 = node2 -> next;
        }
        node1 -> next = res2;
        return res1;
    }
};
```



#### 单调栈：坡度问题，求可能存在的区间最大值

```c++
int solve(vector<int>& nums) {
    int n = nums.size();
    if (n <= 1) return 0;
    stack<int>st;
    for (int i = 0; i < nums.size(); i++) { // 注意这个是从小到大的，不能出错
        if (st.size() == 0 || nums[st.top()] > nums[i]) {
            st.push(i); // 这样就构建了一个 以第一个数字为基准的单调递减的栈
            // 为什么是以第一个数字为基准，因为我们希望间隔尽可能的大
        }
    }
    // 注意之后是从后向前遍历，因为我们希望找到最大的间距
    int res = 0;
    for (int i = nums.size() - 1; i >= 0; i--) {
        while (!st.empty() && nums[st.top()] <= nums[i]) {
            res = max(res, i - st.top());
            st.pop();
        }
    }
    return res;
}
```




#### Bitmap

* 相比使用 bool 存储，节约了 8 倍的空间，因为一个 bool 只能判断一个元素是否在 hashmap 中，这个8个位可以判断 8 个数据

```cpp
#include<bits/stdc++.h>

using namespace std;

typedef long long ll;

class BitMap{
public:
    BitMap() {
        flags = NULL;
        size = 0;
    }
    BitMap(unsigned int size) {
        // 声明 bitmaps 数组
        flags = NULL;
        flags = new char[size];  // 申请一段空间，下面需要做的是初始化
        memset(flags, 0x0, size * sizeof(char));
        this -> size = size;
    }
    // 根据 index 设置元素出现
    int bitmapSet(unsigned int index) {
        int addr = index / 8;
        int offset = index % 8;
        unsigned char b = 0x1 << offset;
        if (addr > (size + 1)) {
            return 0;
        } else {
            flags[addr] |= b;
            return 1;
        }
    }
    // 根据 index 判断元素是否出现过
    int bitmapGet(unsigned int index) {
        int addr = index / 8;
        int offset = index % 8;
        unsigned char b = 0x1 << offset;
        if (addr > (size + 1)) {
            return 0;
        } else {
            return ((flags[addr] & b) > 0) ? 1 : 0;
        }
    }
private:
    unsigned int size;
    char* flags;
};


int main() {
    // cout << INT_MAX << endl; // 9 位数
    BitMap* b = new BitMap(4000000000);
    vector<unsigned int>qq;
    for (int i = 0; i < 10; i++) {
        unsigned int num = rand() % 10;
        if (b -> bitmapGet(num)) {
            cout << "The set has the qq : " << num << endl;
        }
        b -> bitmapSet(num);
        qq.push_back(num);
    }
    return 0;
}
```







#### 外部排序

归并是可以逐段进行的哦 只要**一段段读入两个文件的一部分内容并比较合并**(注意只要一段中一部分就好），直至读完整个文件即可利用有限的内存对理论上无限的外存进行排序

* 部分排序阶段

我们根据内存大小，将待排序的文件拆成多个部分，使得每个部分都是足以存入内存中的。然后选择合适的内排序算法，将多个文件部分排序，并输出到容量可以更大的外存临时文件中，每个临时文件都是有序排列的，我们将其称之为一个“顺段”。

* 归并阶段

我们对前面的多个“顺段”进行合并，思想和归并排序其实是一样的。以 2 路归并为例，每次都将两个连续的顺段合并成一个更大的顺段。

因为内存限制，每次可能只能读入两个顺段的部分内容，所以我们**需要一部分一部分读入**，在内存里将**可以确定顺序的部分排列**，并输出到外存里的文件中，不断重复这个过程，直至两个顺段被完整遍历。

![img](https://static001.geekbang.org/resource/image/ed/ce/ed5c8cfa170455d8a8ff4c3c736a0fce.jpg?wh=1920x1145)

* 假设现在有含有 1000 个记录的文件，而内存最多只能读取 100 个记录。那么我们要做的第一步就是先把 1000 个记录拆成十个文件，每个文件有 100 个记录，读入后在内存中排序得到 10 个有序的临时文件，并输出到外存也就是硬盘中。

##### 时间复杂度

* 第一个阶段部分排序： 快速排序。
* 归并阶段： 内存不足以装下所有需要排序的元素，所以 O(nlogn) 的堆排和快排都已经没办法被应用在外排的场景中了，但基于分治思想的归并排序却依然可以很好地发挥作用。
* 外部排序场景下，我们还有个**非常大的时间消耗就是 IO**，也就是输入输出。
* 所以考虑外排效率时，非常重要的一点就是我们要**尽量减少从磁盘中读取数据的耗时**，而这主要关系要访问多少次外存。

###### 如何降低归并层数

* 为了增加归并路数，也就是尽量增加 k。
* 但是增加 k 的大小，其实也会导致每次归并的时候合并的成本变大，**一个显著的问题就是在 k 路归并中，我们需要从 k 个元素中选择出最小的元素，代价比 2 路归并的更高**。
* **败者树**，则是解决从 k 个元素中选取最小元素并可以动态更新的另一种方法，也是更广泛运用于多路归并中的算法

###### 败者树

![img](https://static001.geekbang.org/resource/image/97/e8/97c53873407010835d7ee108225085e8.jpg?wh=1920x1145)

* 对叶子结点两两比赛，在它们共同的父节点中存储失败者；然后对获胜者节点再两两比较，得到更上一层的败者，取出胜者继续往上比较（类似欧冠比赛）



1TB 文件排序的思路：

* 先用内排序算法，尽可能多的加载源文件，将其变成 n 个有序顺段。
* 在内存有限的前提下每 k 个文件为一组，**每次流式地从各个文件中读取一个单词**，借助败者树选出字典序最低的一个，输出到文件中，这样就可以将 k 个顺段合并到一个顺段中了；反复执行这样的操作，直至所有顺段被归并到同一个顺段。
* ![img](https://static001.geekbang.org/resource/image/36/42/3660c083db7a5e190822yy66ff37cb42.jpg?wh=1920x1145)
* 上图 n = 10, k = 5 (目的还是希望尽量减少归并的层数： **增加更多的归并路数**或者降低初始的顺段。)
* 看起来我们每次从文件中只读取了一个单词，但操作系统在读文件的时候是会**按页为单位读取并缓存下来的**，所以某一次磁盘访问之后的若干次访问，其实都会直接**命中 cache**，也就是说，并不是每次从败者树中取出元素时都会真的产生磁盘 IO。

* 如果内存32G，其实 1TB的数据，是可以全部加载到内存的。**malloc并没有实际分配内存，分配的都是虚拟内存**，64位的虚拟内存远大于 1T，内存还可以和磁盘swap。(直接排估计缺页中断次数会很多吧)
* 败者树、胜者树的主要区别在于**内存的访问次数**；胜者树是和兄弟节点做比较，所以取出父节点还要再取一次子节点；访存次数更多。败者树则解决了这个问题。 而堆则必然每次都需要比较两次。



#### Bitmap

* 如果使用 hash 表需要一个字节的 bool 变量来表示是否存在某个 QQ 号，提高了 8 倍的存储效率，用更少的空间来表示状态

```cpp
#include<bits/stdc++.h>

using namespace std;

typedef long long ll;

class BitMap{
public:
    BitMap() {
        flags = NULL;
        size = 0;
    }
    BitMap(unsigned int size) {
        // 声明 bitmaps 数组
        flags = NULL;
        flags = new char[size];  // 申请一段空间，下面需要做的是初始化
        memset(flags, 0x0, size * sizeof(char));
        this -> size = size;
    }
    // 根据 index 设置元素出现
    int bitmapSet(unsigned int index) {
        int addr = index / 8;
        int offset = index % 8;
        unsigned char b = 0x1 << offset;
        if (addr > (size + 1)) {
            return 0;
        } else {
            flags[addr] |= b;
            return 1;
        }
    }
    // 根据 index 判断元素是否出现过
    int bitmapGet(unsigned int index) {
        int addr = index / 8;
        int offset = index % 8;
        unsigned char b = 0x1 << offset;
        if (addr > (size + 1)) {
            return 0;
        } else {
            return ((flags[addr] & b) > 0) ? 1 : 0;
        }
    }
private:
    unsigned int size;
    char* flags;
};


int main() {
    // cout << INT_MAX << endl; // 9 位数
    BitMap* b = new BitMap(4000000000);
    vector<unsigned int>qq;
    for (int i = 0; i < 10; i++) {
        unsigned int num = rand() % 10;
        if (b -> bitmapGet(num)) {
            cout << "The set has the qq : " << num << endl;
        }
        b -> bitmapSet(num);
        qq.push_back(num);
    }
    return 0;
}
```




#### 布隆过滤器

* Redis 缓存击穿，可以使用布隆过滤器记录哪些元素不再 mysql 中在，这样就不会有大量的请求访问数据库了
* *将数据库中所有用户id压入布隆过滤器，存于JVM内存*
* **写入数据库数据时，使用布隆过滤器做个标记**，然后在用户请求到来时，业务线程确认缓存失效后，**可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在**。
* 即使发生了缓存穿透，**大量请求只会查询 Redis 和布隆过滤器，而不会查询数据库**，保证了数据库能正常运行，Redis 自身也是支持布隆过滤器的。
* 定义： 布隆过滤器不是一个简单的映射，更像是一个散射，它包含 K 个不同的散列函数，每个散列函数都会把某个 key 映射到一个数字 h，然后我们会把一个 M 位二进制对应的第 h 位二进制，置为 1。这样，每个 key，我们就映射到 m 位二进制中 k 位为 1 的数字上了，记录的方式其实和 bitmap 的定义如出一辙。
* 之后再查询不同的 key，比如 x、y、z，我们首先会进行同样的 Hash 计算，判断出每个对应的位置是否为 1，**如果有一位不为 1，说明这个 key 肯定在系统中不存在，我们也就不需要进行后续的查询了**。这样我们能减少很大一部分缓存穿透的情况，而且付出的存储空间也非常有限。

缺陷：

* 判断并不是完全准确的。布隆过滤器可以保证自己判断出不在系统中的 key 一定不在，但是剩下的部分不一定都在系统中存在 key，有一定的误判风险；
* 布隆过滤器的记录删除比较困难。



![img](https://static001.geekbang.org/resource/image/af/15/afb525640ed926a184c90887950ef015.jpg?wh=1920x956)

* 如果每一位都为 1，其实仍然有一定的概率这个 key 没有出现在布隆过滤器中。比如例子中的 w：
* 缓存系统是空间敏感的应用，这也是我们没有**直接用 Redis 存储**不存在记录的原因，为了使用更小的空间，**付出小概率的“误差”成本完全是可接受的。**
* 只要确定布隆过滤器中不存在的 key，则该 key 在系统中一定是不存在的，因为但凡有一个 Hash 值对应的 Bitmap 位不为 1，说明这个 key **一定没有被添加到过 bloom filter 中**。
* 第二个劣势记录删除困难，根本原因和上一点是一致的，因为我们重叠了 Hash 的位数，所以**无法判断每一位的 1 到底是由哪一次 Hash 计算贡献的**，**同一个 1 可能被多次 Hash 计算更新，所以我们不好在想要移除某个字段的时候，把通过 Hash 计算得到的位直接置 0**。不过这个问题在许多需要使用布隆过滤器的场景下影响也不大。
* 100w 的数据量，如果希望通过 5 次 Hash 得到 5% 以内的误判率，我们大概需要 700 万位 Bitmap，内存只需要不到 1M 即可完成，效果非常不错。而且 5% 的误判率，我们也是完全可以接受的，**以数据库 - 缓存构成的系统为例，这足以帮我们降低 95% 的穿透请求，效果显著。**
* Q： 布隆过滤器清除状态比较困难，但有些缓存数据是有生命周期的，比如一周或者一个月之后就大概率过期了，你有没有什么好办法可以帮助我们更好地清理过期数据呢？
* A： 每过一定的周期就建立一个新的布隆过滤器B，和原有的布隆过滤器A保持双写，再过一段时间后，A与B的指代交换，然后删除A。



#### 翻转全 0 字符串到指定字符串最小次数



* 将全是0字符串翻转成指定字符串需要的最小次数

  ```c++
  int solve(const string& s) {
      int n = s.size();
      int res = 0;
      for (int i = 0; i < n; i++) {
          if (res % 2 == 0) {
              // 偶数情况
              if (s[i] != '0') {
                  res++;
              }
          } else {
              // 奇数情况， 如果是字符是 '0' 那么我们需要翻转
              if (s[i] == '0') {
                  res++;
              }
          }
      }
      return res;
  }
  ```

  

#### 数组 & 链表转成二叉搜索树

* 思路都是找到中间节点然后二分成左右两颗子树接着求解
* 





#### 大数乘法

* #### [43. 字符串相乘](https://leetcode.cn/problems/multiply-strings/)

```cpp
class Solution {
public:
    string multiply(string num1, string num2) {
        int m = num1.size();
        int n = num2.size();
        if (m < 1 || n < 1) return "0";
        if (m == 1 && num1 == "0") return num1;
        if (n == 1 && num2 == "0") return num2;
        vector<int>nums(m + n);
        // 从后向前计算，因为需要求进位以及乘法一般都是从低位开始算
        for (int i = m - 1; i >= 0; i--) {
            for (int j = n - 1; j >= 0; j--) {
                int num = (num1[i] - '0') * (num2[j] - '0');
                num += nums[i + j + 1];
                nums[i + j] += num / 10; // 注意这个是 += 不是 =,因为从大到小，之后也是需要取模
                nums[i + j + 1] = num % 10;
            }
        }
        int i = 0;
        string res;
        while (i < nums.size() && nums[i] == 0) i++;
        for (; i < nums.size(); i++) {
            res.push_back((nums[i] + '0'));
        }
        return res;
    }
};
```



#### 素数求解

* 传统解法，暴力，时间复杂度 ： `O(n^(3/2))`,空间复杂度 ： `O(1)`

* 素数晒，时间复杂度 ： `O(nloglogn)` ,空间复杂度 ： `O(n)`

* #### [204. 计数质数](https://leetcode.cn/problems/count-primes/)

* ```cpp
  // 传统做法
  class Solution {
  public:
      int countPrimes(int n) {
          int res = 0;
          if (n <= 2) return 0;
          if (n == 3) return 1;
          for (int i = 2; i < n; i++) {
              bool flag = 0;
              for (int j = 2; j * j <= i; j++) {
                  if (i % j == 0) {
                      flag = 1;
                      break;
                  }
              }
              if (!flag) res++;
          }
          return res;
      }
  };
  
  // 素数晒
  class Solution {
  public:
      int countPrimes(int n) {
          // 素数晒的方式，将时间复杂度降到 O(nloglogn) 但是会带来额外的空间开销 ： O(n)
          if (n < 2) return 0;
          vector<int>isPrime(n + 1, 1);
          isPrime[0] = 0;
          isPrime[1] = 0;
          for (int i = 2; i * i < n; i++) {
              if (!isPrime[i]) continue;
              for (int j = i * i; j < n; j += i) {  // 每一次将 + i 表示的是所有和 i 相关的数都设置成 0 
                  isPrime[j] = 0;
              }
          }
          int res = 0;
          for (int i = 2; i < n; i++) {
              if (isPrime[i]) res++;
          }
          return res;
      }
  };
  ```





#### 计算器问题（通解）

* #### [224. 基本计算器](https://leetcode.cn/problems/basic-calculator/)

* #### [227. 基本计算器 II](https://leetcode.cn/problems/basic-calculator-ii/)

* 通解：都是采用**双栈**的形式求解，根据运算优先级进行求解。

* leetcode 772 也是如此,只不过多了结果判断条件

* ```cpp
  // leetcode 224 () + - 这四个符号
  class Solution {
  public:
      void eval(stack<int>& st, stack<char>& ops) {
          char c = ops.top();
          ops.pop();
          int a = st.top();
          st.pop();
          int b = st.top();
          st.pop();
          if (c == '+') st.push(a + b);
          else st.push(b - a);
      }
      void removeSpace(string& s) {
          int n = s.size();
          int left = 0;
          string s1;
          for (int i = 0; i < n; i++) {
              if (s[i] != ' ') {
                  if (i > 0 && (s[i] == '+' || s[i] == '-') && s[i - 1] == '(') s1.push_back('0');
                  // 主要目的是： (-5) -> (0 - 5) , (+5) -> (0 + 5) 方便计算避免出错
                  s1.push_back(s[i]);
              }
          }
          s = s1;
      }
      int calculate(string s) {
          // 双栈通解计算器问题
          removeSpace(s);
          unordered_map<char, int>mp;
          mp['('] = 1;  // 优先级最小，避免出现这个页执行运算了
          mp['+'] = 2;
          mp['-'] = 2;
          stack<int>st;
          st.push(0); // 防止字符串一开始是： -5 + ... 或者 + 5 +...
          stack<char>ops;
          for (int i = 0; i < s.size() ; i++) {
              if (isdigit(s[i])) {
                  int temp = 0;
                  while (i < s.size() && isdigit(s[i])) {
                      temp = temp * 10 - '0' + s[i];
                      i++;
                  }
                  st.push(temp);
                  i--;
              } else if (s[i] == ')') {
                  while (!ops.empty() && ops.top() != '(') {
                      eval(st, ops);
                  }
                  ops.pop(); // 将 '(' 出栈
              } else if (s[i] == '('){  // 直接入栈就可以了
                  ops.push(s[i]);
              } else {
                  while (!ops.empty() && mp[s[i]] <= mp[ops.top()]) {  // 优先级小于等于之前的都要出栈运算
                      // 避免：  10 / 2 / 5 这种错误
                      eval(st, ops);
                  }
                  ops.push(s[i]); // 将当前的运算符入栈
              }
          }
          while (!ops.empty()) {
              eval(st, ops);
          }
          return st.top();
      }
  };
  ```

* ```cpp
  // leetcode 227 + - * / 这四个符号
  class Solution {
  public:
      void resmoveSpace(string& s) {
          int left = 0;
          for (int i = 0; i < s.size(); i++) {
              if (s[i] != ' ') {
                  s[left] = s[i];
                  left++;
              }
          }
          s.resize(left);
      }
      void eval(stack<int>& st, stack<char>& ops) {
          char c = ops.top();
          ops.pop();
          int a = st.top();
          st.pop();
          int b = st.top();
          st.pop();
          if (c == '+') st.push(a + b);
          else if (c == '-') st.push(b - a);
          else if (c == '*') st.push(a * b);
          else if (c == '/') st.push(b / a);
      }
      int calculate(string s) {
          resmoveSpace(s);
          unordered_map<char, int>mp;
          mp['+'] = 2;
          mp['-'] = 2;
          mp['*'] = 3;
          mp['/'] = 3;
          stack<int>st;
          st.push(0);
          stack<char>ops;
          for (int i = 0; i < s.size(); i++) {
              if (isdigit(s[i])) {
                  int temp = 0;
                  while (i < s.size() && isdigit(s[i])) {
                      temp = temp * 10 + - '0' + s[i];
                      i++;
                  }
                  st.push(temp);
                  i--;
              } else {
                  while (!ops.empty() && mp[s[i]] <= mp[ops.top()]) {
                      eval(st, ops);
                  }
                  // 计算的都是之前已经入栈的值和运算符
                  ops.push(s[i]);
              }
          }
          while (!ops.empty()) {
              eval(st, ops);
          }
          return st.top();
      }
  };
  ```

* ```cpp
  // leetcode 772 + - * / ( ) 这六个符号
  // 由 LeetCode 224 改编的代码
  
  class Solution {
  public:
      void eval(stack<int>& st, stack<char>& ops) {
          char c = ops.top();
          ops.pop();
          int a = st.top();
          st.pop();
          int b = st.top();
          st.pop();
          if (c == '+') st.push(a + b);
          else if (c == '-') st.push(b - a);
          else if (c == '*') st.push(a * b);
          else st.push(b / a);
      }
      void removeSpace(string& s) {
          int n = s.size();
          int left = 0;
          string s1;
          for (int i = 0; i < n; i++) {
              if (s[i] != ' ') {
                  if (i > 0 && (s[i] == '+' || s[i] == '-') && s[i - 1] == '(') s1.push_back('0');
                  // 主要目的是： (-5) -> (0 - 5) , (+5) -> (0 + 5) 方便计算避免出错
                  s1.push_back(s[i]);
              }
          }
          s = s1;
      }
      int calculate(string s) {
          // 双栈通解计算器问题
          removeSpace(s);
          unordered_map<char, int>mp;
          mp['('] = 1;  // 优先级最小，避免出现这个也执行运算了
          mp['+'] = 2;
          mp['-'] = 2;
          mp['*'] = 3;
          mp['/'] = 3;
          stack<int>st;
          st.push(0); // 防止字符串一开始是： -5 + ... 或者 + 5 +...
          stack<char>ops;
          for (int i = 0; i < s.size() ; i++) {
              if (isdigit(s[i])) {
                  int temp = 0;
                  while (i < s.size() && isdigit(s[i])) {
                      temp = temp * 10 - '0' + s[i];
                      i++;
                  }
                  st.push(temp);
                  i--;
              } else if (s[i] == ')') {
                  while (!ops.empty() && ops.top() != '(') {
                      eval(st, ops);
                  }
                  ops.pop(); // 将 '(' 出栈
              } else if (s[i] == '('){  // 直接入栈就可以了
                  ops.push(s[i]);
              } else {
                  while (!ops.empty() && mp[s[i]] <= mp[ops.top()]) {  // 优先级小于等于之前的都要出栈运算
                      // 避免：  10 / 2 / 5 这种错误
                      eval(st, ops);
                  }
                  ops.push(s[i]); // 将当前的运算符入栈
              }
          }
          while (!ops.empty()) {
              eval(st, ops);
          }
          return st.top();
      }
  };
  ```

* 



#### 卡特兰数

卡特兰数的应用：

- 出栈次序
- n个节点的二叉树构成
- 凸多边形的三角形划分
- 括号匹配，网格两点之间抵达方案等

套公式：
`C(2n,n)/(n+1)` 即可





#### 桶排序

* #### [220. 存在重复元素 III](https://leetcode.cn/problems/contains-duplicate-iii/)

```cpp
using LL = long long;
class Solution {
public:
    LL getId(LL num, LL w) {
        if (num >= 0) {
            return (num / w);
        } 
        return (num + 1) / w - 1; // 整体右移，然后除 w 之后减一，举例子试试
    }
    bool containsNearbyAlmostDuplicate(vector<int>& nums, int k, int t) {
        // 解法一：咱们可以将数据放到一个有序集合中，然后进行二分查找
        // 注意这个有序集合只包含我们需要的数据，就是 abs(i - j) <= k 内的所有元素
        // 整个有序集合只能包含 k 个元素，多了不行
        // 思考一下 getID（） 是如何求解的，写一个小的总结
        set<LL>st;
        for (int i = 0; i < nums.size(); i++) {
            auto it = st.lower_bound(1LL * nums[i] - 1LL * t);
            if (it != st.end() && *it <= 1LL * (1LL * t + 1LL * nums[i])) return true;
            st.insert(nums[i]);
            if (i >= k) {  // 注意这个是 i >= k 因为下一次循环就会出现 里面元素个数是 k + 1 了，所以需要在这个时候删除一个元素
                st.erase(nums[i - k]);
            }
        }
        return false;

        // 解法二： 桶排序, 每个桶的大小是？ t + 1 这样我们就可以分清楚这个桶中的所有元素了
        LL w = 1LL * t + 1;  // 每一个桶中的元素个数
        unordered_map<LL, LL> mp;
        for (int i = 0; i < nums.size(); i++) {
            LL id = getId(nums[i], w);
            if (mp.find(id) != mp.end()) return true;
            if (mp.find(id - 1) != mp.end() && nums[i] - mp[id - 1] <= t) return true;
            if (mp.find(id + 1) != mp.end() && mp[id + 1] - nums[i] <= t) return true;
            mp[id] = nums[i];
            if (i >= k) mp.erase(getId(nums[i - k], w));
        }
        return false;
    }
};
```



#### 摩尔投票法

* 这个也是需要记忆模板的！
* 摩尔投票不一定能找到那个数字一定是超过半数 eg : 1, 2, 3 所以 最终需要遍历一遍整个数组，进行验证（当然如果题目告诉你保证存在一个数字超过半数，那么摩尔投票一定可以找到超过半数的数字）
* 





#### 字节经典算法题

* 给定 0-9 数字，每个数字可以重复使用。给定一个 n, 求用之前的数字能组成的小于 n 的最大数字？

* eg : A = {2, 4, 9}, n = 231121, ans = 22999

* ```cpp
  #include<bits/stdc++.h>
  
  using namespace std;
  
  int getTarget(vector<int>& nums, int target) {
      // 寻找 target 如果没找到返回 -1
      int l = 0, r = nums.size() - 1;
      while (l <= r) {
          int mid = (r - l) / 2 + l;
          if (nums[mid] == target) return target;
          else if (nums[mid] < target) l = mid + 1;
          else r = mid - 1;
      }
      if (l == 0) return -1;
      if (nums[l - 1] <= target) return nums[l - 1];
      return -1;
  }
  
  // int getTarget(vector<int>& nums, int target) {
  //     int n = nums.size();
  //     for (int i = n - 1; i >= 0; i--) {
  //         if (nums[i] <= target) return nums[i];
  //     }
  //     return -1;
  // }
  
  string solve(vector<int>& nums, string& digit) {
      sort(nums.begin(), nums.end());
      int n = nums.size();
      string res;
      bool flag = 0;
      for (int i = 0; i < digit.size(); i++) {
          if (flag) {
              res.push_back(nums[n - 1] + '0');
              continue;
          }
          int target = digit[i] - '0';
          if (i == digit.size() - 1) {
              target--; // 因为题目要求需要小于这个数字的最大值
          }
          // cout << target << endl;
          int temp = getTarget(nums, target);
          // cout << temp << endl;
          if (temp == -1) {
              // 没找到，可能需要回溯去找符合条件的数字
              if (i == 0) {
                  flag = 1;
              } else {
                  int idx = i - 1; // 从上一个位置进行回溯操作
                  int tempNum = -1;
                  while (idx >= 0 && tempNum == -1) {
                      tempNum = getTarget(nums, digit[idx] - '0' - 1);
                      res.pop_back();
                      idx--;
                  }
                  if (tempNum == -1) {
                      i = 0;
                      flag = 1;
                      continue;  // 从第一位后一位一直向前操作
                  }
                  res.push_back(tempNum + '0');
                  for (int j = idx + 2; j <= i; j++) {
                      res.push_back(nums[n - 1] + '0');
                  }
                  flag = 1; // 后面都是填最大的值
              }
          } else if (temp == target) {
              res.push_back(temp + '0');
          } else {
              res.push_back(temp + '0');
              flag = 1;
          }
      }
      
      return res;
  }
  
  int main() {
  
      vector<int>nums{7, 6, 8, 5};
      string s1 = "6879", s2 = "5555", s3 = "5698", s4 = "5678";
      cout << "6879" << " " << solve(nums, s1) << endl;
      cout << "5555" << " " << solve(nums, s2) << endl;
      cout << "5698" << " " << solve(nums, s3) << endl;
      cout << "5678" << " " << solve(nums, s4) << endl;
  
      return 0;
  }
  
  // print:
  /*
  6879 6878
  5555 888
  5698 5688
  5678 5677
  
  */
  ```

* 

























### Go语言

* go语言中使用了哪些锁？

> 1. 互斥锁：同一资源的锁定对各个协程来说是相互排斥的
> 2. 读写锁： 基于互斥锁实现的，当多个协程对某个资源是只读操作，那么这多个协程是都可以获得该资源的读锁，并且互不影响，当多个协程希望对某一资源进行写操作，如果某一个协程获取了写锁，那么再有其它协程希望获取读写或者写锁的时候会出现错误，  总结： 读写互斥，读读共享， 写写互斥



### 简历项目问题

#### redis

##### 如何持久化

[Redis持久化](#Redis 持久化)





#### rabbitmq

##### 消息队列的作用

* 支持高并发，高吞吐，高可用

###### 异步处理

* 引入消息队列后，把发送邮件,短信不是必须的业务逻辑异步处理.
* 引入消息队列后，用户的响应时间就等于写入数据库的时间+写入消息队列的时间(可以忽略不计),引入消息队列后处理后,响应时间是串行的3倍,是并行的2倍。
* ![这里写图片描述](https://img-blog.csdn.net/20170209150824008?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hvYW1peWFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

###### 应用解耦

* 当库存系统出现故障时,订单就会失败。
* ![这里写图片描述](https://img-blog.csdn.net/20170209151602258?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hvYW1peWFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* 订单系统:用户下单后,订单系统完成持久化处理,将消息写入消息队列,返回用户订单下单成功。(直接先返回下单成功)
* 库存系统:订阅下单的消息,获取下单消息,进行库操作。就算库存系统出现故障,消息队列也能保证消息的可靠投递,不会导致消息丢失.
* ![这里写图片描述](https://img-blog.csdn.net/20170209152116530?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hvYW1peWFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



###### 流量削峰

* 秒杀活动，一般会因为流量过大，导致应用挂掉,为了解决这个问题，一般在应用前端加入消息队列。

* 作用：

* > 1. 可以控制活动人数，超过此一定阈值的订单直接丢弃(我为什么秒杀一次都没有成功过呢^^)
  > 2. 可以缓解短时间的高流量压垮应用(应用程序按自己的最大处理能力获取订单)



###### 整体架构

![这里写图片描述](https://img-blog.csdn.net/20170209162609150?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hvYW1peWFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

* Broke： 提供一种传输服务，维护一条从生产者到消费者的路线，保证数据能够按照指定的方式进行传输
* Exchange： 消息交换机，他指定消息按照什么规则，路由到哪个队列
* Queue： 消息的载体，每个消息都会被投放到一个或者多个队列
* Producer： 消息生产者，就是投递消息的程序
* Consumer： 消息的消费者，就是接受消息的程序
* Channel： 消息通道，在客户端的每个连接里，可建立多个 Channel









##### rabbitmq 消费重复数据如何解决

[如何保证只消费一次](#如何保证只消费一次)





##### 如何解决秒杀的超卖问题

* 使用锁的方式，比如分布式锁，也可以利用redis本身操作原子性的特点
* 写入消息队列，在消息队列中做减库存的操作，做异步校验
* 可以在网关层用“令牌桶”控制，令牌数量是固定的，超过的请求可以被网关打回







#### JWT 如何鉴权

* 打印出来的是字符串，会经过 base64 编码

##### 基本流程：

* > * 服务器当验证用户账号和密码正确的时候，给用户颁发一个令牌，这个令牌作为后续用户访问一些接口的凭证
  > * 后续访问会根据这个令牌判断用户时候有权限进行访问
  > * ![image-20220714202229611](/home/zhoubing/.config/Typora/typora-user-images/image-20220714202229611.png)

* **Header:**

* 每个JWT都会带有头部信息，这里**主要声明使用的算法**。声明算法的字段名为`alg`，同时还有一个`typ`的字段，默认`JWT`即可。以下示例中算法为HS256

* ```json
  {  "alg": "HS256",  "typ": "JWT" } 
  ```

* **Payload**

* 载荷即消息体，这里会存放实际的内容，也就是`Token`的数据声明，**例如用户的`id`和`name`**，默认情况下也会携带令牌的签发时间`iat`，通常还可以设置过期时间，如下：

* ```go
  claims := &MyClaims{
      ID:       userId,
      UserName: name,
      StandardClaims: jwt.StandardClaims{
          ExpiresAt: expirationTime.Unix(),
          IssuedAt:  time.Now().Unix(),
          Issuer:    "zhoubing",
          Subject:   "user token",
      },
  }
  ```

* **Signature**

* 签名是对头部和载荷内容进行签名，一般情况，设置一个`secretKey`，对前两个的结果进行`HMACSHA25`算法

* ```go
  token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
  // 此处是 增加 秘钥 生成完整的 jwt 部分，个人理解是第三部分
  signToken, err := token.SignedString([]byte(config.GetConfig().JWT.Secret))  // 这个 secret 保存在服务端的
  ```

* **一旦前面两部分数据被篡改，只要服务器加密用的密钥没有泄露，得到的签名肯定和之前的签名不一致**



##### Token 校验

* secret 必须和 sign 时候的秘钥保持一致

* 登陆/注册都可以不用校验,如果是全局中间件，可以设置鉴权白名单，设置哪些是不需要鉴权的.

* 在分布式系统中，每个子系统都要获取到秘钥，那么这个子系统根据该秘钥可以发布和验证令牌，但有些服务器只需要验证令牌

  这时候可以采用非对称加密，利用私钥发布令牌，公钥验证令牌，加密算法可以选择`RS256`



##### 优缺点

优点：

- json具有通用性，所以可以跨语言
- 组成简单，字节占用小，便于传输
- 服务端无需保存会话信息，很容易进行水平扩展
- 一处生成，多处使用，可以在**分布式系统中，解决单点登录问题**
- 可防护CSRF攻击

缺点：

- payload部分仅仅是进行简单编码，所以只能用于存储逻辑必需的非敏感信息
- 需要保护好加密密钥，一旦泄露后果不堪设想
- 为避免token被劫持，最好使用https协议
- 安全性， jwt 的 payload 部分只是 base64 编码，所以不能存储敏感的信息。
- 性能 ： jwt 太长： 由于是无状态使用JWT，所有的数据都被放到JWT里，如果还要进行一些数据交换，那载荷会更大，经过编码之后导致jwt非常长。
- 并且用户在系统中的**每一次http请求都会把jwt携带在Header里面**，http请求的Header可能比Body还要大。**而sessionId只是很短的一个字符串**，因此使用jwt的http请求比使用session的开销大得多。
- 一次性： 无状态是jwt的特点，但也导致了这个问题，jwt是一次性的。**想修改里面的内容，就必须签发一个新的jwt**。但是老的 jwt 在过期结束时间前还是可以使用的。所以一旦签发了新的jwt，**那么旧的就加入黑名单**（比如存到redis里面），避免被再次使用。



#### 加密算法

##### bcrypt

* **bcrypt是单向Hash加密算法，不可反向破解生成明文。**
* bcrypt是一种加盐的[加密](https://so.csdn.net/so/search?q=加密&spm=1001.2101.3001.7020)方法，**MD5加密**时候，同一个密码经过hash的时候生成的是同一个hash值，在大数据的情况下，有些经过md5加密的方法将会被破解。
* 使用bcrypt进行加密，同一个密码每次生成的[hash](https://so.csdn.net/so/search?q=hash&spm=1001.2101.3001.7020)值都是不相同的。每次加密的时候首先会生成一个随机数就是盐，**之后将这个随机数与密码进行hash。**





#### SQL 注入

* 什么是 SQL 注入？

  > * 恶意的字符串拼接.
  >
  > * ```sql
  >   SELECT * FROM users WHERE user_id = $user_id
  >   user_id = 1234; DELETE FROM users
  >   -> 
  >   SELECT * FROM users WHERE user_id = 1234; DELETE FROM users
  >   那么会删除整个 user 表
  >   ```
  >
  > * 利用注释执行非法命令 (sql 可以插入注释)
  >
  > * ```sql
  >   SELECT COUNT(*) AS 'num' FROM game_score WHERE game_id=24411 AND version=$version
  >   version = '-1' OR 3 AND SLEEP(500)--
  >   -> 
  >   SELECT COUNT(*) AS 'num' FROM game_score WHERE game_id=24411 AND version='-1' OR 3 AND SLEEP(500)--
  >   ```
  >
  > * 传入非法的参数
  >
  > * SQL 语句中传入的字符串参数是用单引号引起来的，如果字符串本身包含单引号而没有被处理，那么可能会篡改原本 SQL 语句的作用
  >
  > * ```sql
  >   SELECT * FROM user_name WHERE user_name = $user_name
  >   user_name = G'chen
  >   ->
  >   SELECT * FROM user_name WHERE user_name ='G'chen' (执行错误)
  >   ```
  >
  > * 添加额外的条件
  >
  > * 在 SQL 语句中添加一些额外条件，以此来改变执行行为。条件一般为真值表达式。
  >
  > * ```sql
  >   UPDATE users SET userpass='$userpass' WHERE user_id=$user_id;
  >   user_id = 1234 OR TRUE
  >   -> 
  >   UPDATE users SET userpass= '123456' WHERE user_id=1234 OR TRUE;
  >   # 这将更改所有用户的密码。
  >   ```

* 如何避免 SQL 注入

  > * 1. gorm  sql 预编译技术：
  > * **将 SQL 语句和用户输入的查询数据分别进行处理，而不是一视同仁的作为 SQL 语句的不同部分进行拼接处理**
  > * 我们可以将其中需要用户输入的值用占位符替代，可以视为将 SQL 语句模板化或者说参数化，再将这样的 SQL 语句进行预编译的处理，在实际运行的时候，再传入用户输入的数据。
  > * **除了可以防止 SQL 注入外，还可以对预编译的 SQL 语句进行缓存，之后的运行就省去了解析优化 SQL 语句的过程，可以加速 SQL 的查询**。
  > * 在使用参数化查询的情况下，数据库服务器不会将参数的内容视为 SQL 语句的一部分来进行处理，而是在数据库完成 SQL 语句的编译之后，才套用参数运行。因此就算参数中含有破坏性的指令，也不会被数据库所运行
  > * 2. 过滤掉输入不合法的内容，校验字符串
  > * 使用正则表达式等技术匹配字符串是否安全，对其输入的 SQL 进行安全验证
  > * 3. 避免使用动态 SQL
  > * 避免将用户的输入数据直接放入 SQL 语句中，最好使用准备好的语句和参数化查询，这样更安全。
  > * 4. 不要将敏感信息保存在纯文本中 （密码加密）
  > * 加密存储在数据库中的私有/机密数据，这样可以提供了另一级保护，以防攻击者成功地排出敏感数据。
  > * 5. 限制数据库使用权限
  > * 将数据库用户的功能设置为最低要求；这将限制攻击者在设法获取访问权限时可以执行的操作。
  > * 6. 避免直接向用户显示数据库错误
  > * 攻击者可以使用这些错误消息来获取有关数据库的信息。



#### raft

##### raft 基本知识点

* raft协议在leader选举阶段，由于老leader可能也还存活，也会存在不只一个leader的情形，**只是不存在term一样的两个leader**.
* 因为选举算法要求leader得到同一个term的多数派的同意，同时赞同的成员会承诺不接受term更小的任何消息。
* 在《etcd技术内幕》中说，Leader提供读服务前，必须确认自己还是Leader, 做法是想其他Follower交换心跳信息，如果半数返回确认才可以提供读服务，在这种机制下，脑裂读脏数据的问题就没了.(印证了 raft 是强 Leader 算法)
* Raft对只读操作的处理办法是
  1. 只读请求最终也必须依靠Leader来执行，如果是Follower接收请求的，那么必须转发
  2. 记录下当前日志的commitIndex <= readIndex.
  3. 执行读操作前要向集群广播一次心跳，并得到majority的反馈
  4. 等待状态机的applyIndex移动过readIndex
  5. 通过查询状态机来执行读操作并返回客户端最终结果。
*  由于在Raft算法中，**写操作成功仅仅意味着日志达成了一致（已经落盘）**，而并不能确保当前状态机也已经apply了日志。状态机apply日志的行为在大多数Raft算法的实现中都是异步的，**所以此时读取状态机并不能准确反应数据的状态，很可能会读到过期数据**。
* 每次读操作的时候记录此时集群的`commited index`，当**状态机的`apply index`大于或等于`commited index`时才读取数据并返回**。由于此时状态机已经把读请求发起时的已提交日志进行了apply动作，所以此时状态机的状态就可以反应读请求发起时的状态，符合线性一致性读的要求。
* Leader执行ReadIndex大致的流程如下：
* 1. 记录当前的commit index，称为ReadIndex；(所有的请求都会交给leader，如果follower收到读请求，会将请求forward给leader)
  2. 向 Follower 发起一次心跳，如果大多数节点回复了，那就能确定现在仍然是Leader；(确认当前leader的状态,避免当前节点状态切换，数据不能及时被同步更新)
  3. 等待状态机的apply index大于或等于commited index时才读取数据；(`apply index`大于或等于`commited index`就能表示当前状态机已经把读请求发起时的已提交日志进行了apply动作，所以此时状态机的状态就可以反应读请求发起时的状态，满足一致性读)
  4. 执行读请求，将结果返回给Client。
* 



#### Raft 基本流程

##### 简介

* 日志被理解为来自客户端的请求序列，在一个集群中，有**唯一的一个节点用于接收客户端请求**，称为"Leader Node"。
* 为了保证数据的安全性，"Leader Node"的日志应该复制给若干个节点用于备份，称为"Follower Node"。
* "Follower Node"的日志需要和"Leader Node"保持一致，**Raft就是一种为了管理日志复制而产生的一致性算法。**



##### Leader 选举

* Raft集群通常有奇数个节点，设为N，集群允许N/2个节点失效，**在正常情况下，集群有1个Leader和N-1个Follower组成**，
* 当Leader失效时，会产生除了Leader和Follower外的第三种身份：Candidate。
* Follower在election timeout后，身份转换为Candidate，在获取(N-1)/2个其他节点的选票后（过半数节点认可），身份转换为Leader。

###### 超时选举

* 如果当前节点不是Leader，且超过election timeout未收到其他节点的消息，则发起选举。此处设置election timeout在150ms - 300ms之间。（是心跳时间的三倍,同时还需要加上一个随机数，避免大量节点同时超时，同时发起选举，从而都无法选举成功）
* 发起选举后，身份转换为Candidate，并通过RequestVote RPC获取其余节点的选票。在获取(N-1)/2个其他节点的选票后，身份转换为Leader。成为Leader后，需要立即向其余节点发送心跳，宣告自己的存在。

###### 心跳发送

* 不携带日志的日志复制即心跳，Leader通过心跳刷新其余节点的election timeout。Hint 4限制了心跳频率在每秒10次，因此这里让心跳一次后休眠100ms。







##### 日志复制

* 客户端将指令`SET A 0`发送到Leader，Leader会将指令追加到自己的日志序列中，Leader中未复制到其他节点的日志，会在随着下一次心跳复制到其他节点，被复制到半数以上节点的日志将被标记为**Commit**状态，并由另一个goroutine负责提交到状态机中，被提交到状态机的日志被标记为**Applied**状态。（会先判断是否是过半数都同意 applied 了），同时在下一次 发送心跳的时候， follower 节点也会提交它们的日志。

* **commitIndex**表示最后一个状态为Commit的日志索引，使用**lastApplied**表示最后一个状态为Applied的日志索引。

  并且由于只有Applied只能由Commit转换而来，所以`lastApplied <= commitIndex`一定成立。

* **日志匹配原则**：如果两个日志在相同索引的位置的Term相同，那么从开始到该索引位置的日志一定相同。

* Raft 论文 图 8 出现的问题： Raft 永远不会通过计算副本数量的方式去提交一个之前任期的日志条目。只有领导人**当前任期里 的日志条目通过计算副本数目可以被提交**。

```go
// 注意下面是 leader 的日志 applied 过程
if (rf.matchIndex[j] >= N && rf.log[N-rf.log[0].Index].Term == rf.currentTerm) || (j == rf.me) {
    nCommit += 1
}
if nCommit > len(rf.peers)/2 {
    rf.commitIndex = N
    go rf.Apply() // 需要将 Leader 中可以提交的日志给提交了
    return
}
```

* 一个旧的日志和Leader 日志相差太大如何解决？

* > * Part B中对于失败的AppendEntries请求，**让nextIndex自减，这样效率是比较慢的**。
  >
  > * 如果 log 不一致，那么 leader 会将对应的 `nextIndex` 减一，然后继续尝试
  >
  > * > * 如果`follower.log`不存在`prevLog`，让Leader下一次从`follower.log`的末尾开始同步日志。
  >   > * 如果当前的 log 不一致，follower 直接将当前冲突的 term 值的第一个索引返回给 leader，leader 可以直接跳过这个 term 中的所有 log entry，从上一个 term 的 log 进行尝试。（**跳过整个任期，因为任期不同里面的日志一定也是不同的**）
  >   > * `nextIndex`的正确位置**可能依旧需要多次RPC才能找到**，改进的流程只是**加快了找到正确`nextIndex`的速度**。
  >   > * Raft 在进行日志比较的时候，只用比较当前那条日志的 index 和 term，不需要比较 log 里面具体的内容
  >
  > * 慢速版本：  leader 日志一个一个和落后的日志进行向前比较，直到 Leader 日志找到和 落后日志相同的日志为止 （相同的index 以及相同位置的任期一样）之后将 Leader 从当前位置往后的所有日志都复制给 follower，实现日志同步。 
  >
  > * 快速版本： 直接设置一个 conflictIndex 一次日志发送找到冲突的位置（使用 for 循环，遍历 Leader 日志和 follower 日志中）
  >
  > * **冲突的日志，raft 只听 leader 的指挥，完全不考虑 follower 的感受**，leader 发现 follower 和自己的日志不一样，那么Leader就需要将 fololower 中的日志覆盖掉。





##### snapshot 快照技术

###### 日志压缩

* 日志序列不断扩张，是无法全部存储在内存中的，对于**已经应用到状态机的部分日志**，就不再需要维护在Raft中（可以保存使用快照技术压缩处理）。
* 状态机可以吐出来一个 snapshot 替换掉 raft 中的部分日志。
* 但由于仍可能存在部分**Follower的日志序列远远落后于Leader**，因此这部分日志不能被Leader丢弃，在同步日志时，**若Leader中原应被同步的日志在快照中，则将快照发送给Follower。**
* `snapshot`由状态机生成，需要Raft保存，用于发送Follower时需要。
* 首先是`heartbeat()`应该新增如下逻辑，**当Leader中应被同步到Follower的日志在快照中时**，将**快照发送给Follower**。
* 我们基于原有的raft构架选择使用**rf.log[0].Index来标志最后一个snapshot的Index**使用rf.log[0].Term 来标志最后一个snapshot的Term







##### Raft 如何避免死锁

* **互斥锁保护区域不要使用操作其他互斥锁的代码。**
* 尽量避免同时只对一个互斥锁上锁。







##### 脑裂如何解决

* 脑裂：可能存在多个 term 的任期。（有两个 Leader 同时这两个 leader 均可以读写才会出现脑裂）

* [!脑裂](#脑裂)

*  当 raft group 发生脑裂的情况下，老的 raft leader 可能在一段时间内并不知道新的 leader 已经被选举出来（因为这个时候老的 raft leader 还在任期阶段内），这时候客户端在老的 leader 上可能会读取出陈旧的数据（stale read）。

*  解决方法：**多数投票！！！**

* **每个节点在一个任期内，只能投票一次。**

* **获得半数以上的选票，才能成为Leader。**

* 当raft在集群中遇见网络分区的时候,集群就会因此而相隔开,在不同的网络分区里会因为无法接收到原来的leader发出的心跳而超时选主,这样就会造成多leader现象,见下图：在网络分区1和网络分区2中，出现了两个leaderA和D,假设此时要更新分区2的值,因为分区2无法得到集群中的大多数节点的ACK,会复制失败。而网络分区1会成功,因为分区1中的节点更多,leaderA能得到大多数回应。

* 当网络恢复的时候,集群不再是双分区,raft会有如下操作：

* > * leaderD发现自己的Term小于LeaderA,会自动下台(step down)成为follower,leaderA保持不变依旧是集群中的主leader角色
  > * 分区中的所有节点会回滚roll back自己的数据日志,并匹配新leader的log日志,然后实现同步提交更新自身的值。通知旧leaderA也会主动匹配主leader节点的最新值,并加入到follower中
  > * 最终集群达到整体一致，集群存在唯一leader（节点A）

* ![img](https://img2020.cnblogs.com/blog/1066538/202010/1066538-20201031210601798-546846443.png)

* ###### 为 Raft 引入 leader lease 机制解决集群脑裂时的 stale read 问题

* > * 引入一个新的概念, region leader。region leader 有一个租期，表示字节可以成为 leader 的时间
  >
  > * region leader 是一个逻辑上的概念, **任意时刻对于某一个 region 来说, 一定只拥有一个 region leader**, 每个 region leader 在任期之内尝试每隔 t 时间间隔, 在 raft group 内部更新一下 region leader 的 lease. 所有的**读写请求都必须通过 region leader 完成**
  >
  > * region leader 和 真正的领导人之间可能不重合，重合就啥事也没有了。不重合的时候 region leader 会把客户端的请求发送给 raft leader.
  >
  > * 网络可能出现以下情况的分区：
  >
  >   > region leader 落在多数派，老 raft leader 在多数派这边
  >   >
  >   > * region leader 的 lease 不会过期，因为 region leader 的心跳仍然能更新到多数派的节点上，老的 raft leader 仍然能同步到大多数节点上，少数派这边也不会选举出新的 leader， 这种情况下不会出现 stale read.
  >   > * ![img](https://upload-images.jianshu.io/upload_images/6243-3518a6999d85782a.png?imageMogr2/auto-orient/strip|imageView2/2/w/514/format/webp)
  >   >
  >   > region leader 落在多数派，老 raft leader 在少数派这边
  >   >
  >   > * 因为所有的读写请求都会找到 region leader 进行，即使在原来没有出现网络分区的情况下，客户端的请求也都是要走 node 1 ，经由 node 1 转发给 node 5，客户端不会直接访问 node 5，所以此时即使网络出现分区，新 leader 也正好在多数派这边，读写直接就打到 node 1 上，皆大欢喜，没有 stale read。
  >   > * ![img](https://upload-images.jianshu.io/upload_images/6243-c0093b232b14eab7.jpeg?imageMogr2/auto-orient/strip|imageView2/2/w/837/format/webp)
  >   >
  >   > region leader 落在少数派，老 raft leader 在多数派这边
  >   >
  >   > * region leader 落在少数派这边，老 raft leader 在多数派这边，这种情况客户端的请求找到 region leader，他发现的无法联系到 leader（因为在少数派这边没有办法选举出新的 leader），请求会失败，直到本次 region leader 的 lease 过期，同时新的 region leader 会在多数派那边产生（因为新的 region leader 需要尝试走一遍 raft 流程）。因为老的 region leader 没办法成功的写入，所以也不会出现 stale read。但是付出的代价是在 region leader lease 期间的系统的可用性。
  >   >
  >   > region leader 落在少数派，老 raft leader 在少数派这边
  >   >
  >   > * 多数派这边会产生新的 raft leader 和 region leader。
  >   >
  >   > * region leader和raft leader都落入少数派，这时多数派重新选举出region leader和raft leader，但是由于少数派中的region leader由于心跳无法到达多数节点上，然后少数派中的region leader lease结束，然后请求就会发到多数派中新选举出来的region leader上，而在这之前的请求都会失败。Raft对只读操作的处理办法是
  >   >
  >   >   只读请求最终也必须依靠Leader来执行，如果是Follower接收请求的，那么必须转发
  >   >   记录下当前日志的commitIndex => readIndex
  >   >   执行读操作前要向集群广播一次心跳，并得到majority的反馈
  >   >   等待状态机的applyIndex移动过readIndex
  >   >   通过查询状态机来执行读操作并返回客户端最终结果。
  >
  > * region leader 第三种第四种情况会牺牲可用性（在脑裂时部分客户端的可用性）换取了一致性的保证。
  >
  > * 多数派的网络分区挂了，网络直接不可以读写了
  >
  > * 如何解决多数派的网络分区挂了，服务就不可用的问题？
  >
  >   - 部署不止两个网络分区，至少三个网络分区；这样任意一个分区挂了，服务都可用，除非三个分区的大多数分区挂了(挂了2个)；





##### raft 和 paxos 以及 ZAB 比较

> * Raft协议比paxos的优点是 容易理解，容易实现。它强化了leader的地位
>
>   > * Leader在时。由Leader向Follower同步日志
>   > * Leader挂掉了，选一个新Leader，Leader选举算法。
>
> * 而VR,ZAB,Raft这些强调**合法leader的唯一性**协议(以 leader 为中心)，它们直接从leader的角度描述协议的流程，也从leader的角度出发论证正确性.
>
> * **一个日志一旦被提交(或者决定），就不会丢失，也不可能更改，这一点这4个协议都是一致的**。
>
> * **Multi-paxos和Raft都用一个数字来标识leader的合法性**，multi-paxos中叫proposer-id，Raft叫term，意义是一样的，**multi-paxos proposer-id最大的Leader提出的决议才是有效的，raft协议中term最大的leader才是合法的**。
>
> * **raft 强 Leader** ，paxous 不是一个强 leader 的协议（**原始的是两阶段提交，这样会比较的慢**）
>
> * **paxos 选主会有约定，就是先选第一个再选第二个， raft 是一个比较随机的过程，不强调规则（谁多可以当 leader）**
>
> * 一个日志被多数派拥有，那么它就可以被提交，但是Leader需要通过某种方式得知这一点，同时为了已经被提交的日志不被新leader覆写，新leader需要拥有所有已经被提交的日志
>
> * raft协议，paxos，zab，viewstamp都是利用了**同一个性质：两个多数派集合之间存在一个公共成员**
>
> * **4个协议都使用一个唯一的整数作为标识符来标明leader的合法性**，paxos叫做proposer-id，ZAB叫epoch，VR叫view，raft叫term。
>
> * 同时Raft协议中**日志的commit（提交）也是连续的**，一条日志被提交，代表这条日志之前所有的日志都已被提交，一条日志可以被提交，代表之前所有的日志都可以被提交.
>
> * 而对于multi-paxos来说，日志是有空洞的，每个日志需要单独被确认是否可以commit，也可以单独commit。
>
> * **两者的区别在于Leader确认提交和获取所有可以被提交日志的方式上，而方式上的区别又是由于是日志是否连续造成的，Raft协议利用日志连续性，简化了这个过程**。
>
> * 因为multi-paxos并不假设日志间有连续确认的关系，**每条日志之间相互独立，没有关系**
>
> * Raft 中的所有日志的副本都有相同的索引、任期和命令，而 Paxos 中这些任期可能有所不同。
>
> * Paxos 论文中给出的选举算法是通过比较服务器 id 的大小，几个节点同时竞选时，服务器 id 较大的节点胜出。问题是，这样选出的领导者如果缺少一些日志，它不能立即执行写操作，必须首先从别的节点那里复制一些日志。
>
> * 对于 Paxos 算法，如果一个服务器非常落后，甚至落后了几天的日志，但却在某个时候选为了领导者，这将导致一定时间的阻塞。可在 Raft 算法中，永远不会选出日志落后的节点。**Raft 的领导者选举算法是一个非常高效的算法。**
>
> * 说raft在某些场景并发处理同步日志有性能影响,所以 oceanbase 选择了 paxos 作为一致性算法
>
> * Basic paxos 存在**多个 proposer** (或者理解成 leader)(可能出现活锁，就是多个 proposer 都提出议案），multi-paxos 只有一个 propose（不会出现活锁）
>
> * ZAB 算法，通过使用 follower -> leader 发送心跳，看leader 是否还是活着 （和 raft 相反）
>
> * Paxos 和 Raft 是一样的，**也是基于多数派投票原则的算法**
>
> * 
>
> 





#### IO 多路复用技术

[IO 多路复用](#IO 多路复用)







### 数据库

* 数据库中有哪些锁

> 1. 独占锁 ： 该锁同一时间只能被某一个线程持有-----写锁
> 2. 共享锁 ： 该锁同一时间可以被多个线程持有-----读锁
> 3. 乐观锁 ： 认为别人不会读数据进行修改（乐观派），乐观锁不会上锁，在更新的时候判断一下在此期间别人是否修改了数据，如果修改放弃此次操作
> 4. 悲观锁 ： 认为别人会对数据进行修改（悲观派），操作数据的时候需要把数据给锁上，知道操作完成之后才释放锁，上锁期间其他的人不能修改数据.



**其他的锁：**

> 1. 阻塞锁
> 2. 偏向锁
> 3. 可重入锁
> 4. 自旋锁
> 5. 分段锁
> 6. 公平锁
> 7. 非公平锁









#### SQL 语句

* join 操作

![20200628073508386.jpg (1703×1278)](https://img-blog.csdnimg.cn/20200628073508386.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Ryb25rZXI=,size_16,color_FFFFFF,t_70#pic_center)



* 记住一点：GROUP BY 后面跟的列的个数和 SELECT 后面跟的列，除了聚集函数以外，其他的必须一样,就是 select 后面字段使用了函数了，可以忽略，如果没有那么也需要放到 group by 后面。(select语句后面可以跟聚集函数，对于非聚集函数，则必须是group by后的)

* 聚合函数结果作为筛选条件时，不能用where，而是用having语法 ,聚合函数之后生成的是一个新表并非原来的表，一旦生成新表一律用having

* 创建了新的字段不能用where，要用having，跟聚不聚合啥没关系.

* order by后面可以加聚合函数。但要注意，group by后面不可

	有三类后面可以加聚合函数

	1、select

	2、order by

	3、having

* left join 仍然会保留原始表的数据，但inner join 只会保留两表交集部分

* 只要满足一个条件就被筛选出来，但总会存在一个人满足了两个条件只筛选一次。这里的坑时使用or，因为or自带去重，而union等价于or，但union all 可以不去重，UNION默认去重

* MySQL对查询做了增强没有严格遵循SQL的执行顺序，where后面不能用select中的别名，但是group by ，order by都是可以的，Oracle数据库严格遵循了SQL执行顺序在Oracle里group by是不能引用select里的别名的.

* Mysql执行顺序应该是 from,where,select,group by,having,order by,limit

* ```sql
	case when age >= 25 then '25岁及以上' else '25岁以下' end  # 条件语句
	IF(age >= 25, '25岁及以上', '25岁以下')  # 条件语句,内部是没有 else 的
	```

* 1、LOCATE(substr , str )：返回子串 substr 在字符串 str 中第一次出现的位置，如果字符substr在字符串str中不存在，则返回0；

  2、POSITION(substr IN str )：返回子串 substr 在字符串 str 中第一次出现的位置，如果字符substr在字符串str中不存在，与LOCATE函数作用相同；

  3、LEFT(str, length)：从左边开始截取str，length是截取的长度；

  4、RIGHT(str, length)：从右边开始截取str，length是截取的长度；

  5、SUBSTRING_INDEX(str ,substr ,n)：返回字符substr在str中第n次出现位置之前的字符串;

  6、SUBSTRING(str ,n ,m)：返回字符串str从第n个字符开始，截取m个字符；

  7、REPLACE(str, n, m)：将字符串str中的n字符替换成m字符；

  8、LENGTH(str)：计算字符串str的长度。

* 函数执行优先级：

* > * 温 (where) 哥 (group by) 华 (having) OI (order by limit)
  > * 哪里 (where) 组队 (group by) 有 (having) 顺序 (order by) 限制 (limit)

##### 窗口函数

* 基本语法：

* ```sql
  <窗口函数> over (partition by <用于分组的列名>
                  order by <用于排序的列名>)
  ```

* > 1） 专用窗口函数，包括后面要讲到的rank, dense_rank, row_number等专用窗口函数。
  >
  > 2） 聚合函数，如sum. avg, count, max, min等

* 因为窗口函数是对where或者group by子句处理后的结果进行操作，所以**窗口函数原则上只能写在select子句中**.

* 这是因为，**group by分组汇总后改变了表的行数，一行只有一个类别。而partiition by和rank函数不会减少原表中的行数**。

* ![img](https://pic2.zhimg.com/80/v2-a9342df7f64ec7d9a52b42adcdb48341_720w.jpg)

* **窗口函数有以下功能：**

* > 1）同时具有分组和排序的功能
	>
	> 2）不减少原表的行数

* Q: 专用窗口函数rank, dense_rank, row_number有什么区别呢？

* ```sql
  select *,
     rank() over (order by 成绩 desc) as ranking,
     dense_rank() over (order by 成绩 desc) as dese_rank,
     row_number() over (order by 成绩 desc) as row_num
  from 班级表
  ```

* ![img](https://pic2.zhimg.com/80/v2-ad1d86f5a5b9f0ef684907b20b341099_720w.jpg)

###### 聚合函数作为窗口函数

```sql
select *,
   sum(成绩) over (order by 学号) as current_sum,
   avg(成绩) over (order by 学号) as current_avg,
   count(成绩) over (order by 学号) as current_count,
   max(成绩) over (order by 学号) as current_max,
   min(成绩) over (order by 学号) as current_min
from 班级表
```

* ![img](https://pic2.zhimg.com/80/v2-c48f0218306f65049fcf9f98c184226d_720w.jpg)



###### lead & lag

* **选择第3行数据，LAG()提取先前行（3行之前）内容，LEAD()提取后续行（3行之后）的数据。两者语法没有区别，就是方向不同。**
* lag（参数1，参数2，参数3）over（order by 列）
* 



* Union： 合并表的结果，上下合并两张表，最终形成一张表的结果



#### Mysql 如何优化

* 合理的表设计，冗余字段用空间换时间。

* > * 当我们遇到数据量非常大的表时候,这时去做表的关联查询(Join)是非常虽然消耗系统性能的,甚至可能会出现数据库连接超时,crash等问题,这时就需要进行冗余设计,**将部分字段冗余到关联表中,避免大表之间的关联查询,提高更快的响应速度**.

* 索引优化，避免索引失效

* 大表进行表分区

* 架构上调整，包括但是不限于读写分离，多主多从

* 高并发下的 IO 瓶颈做分库分表

##### 避免使用 select *

* 反例：

* ```sql
  SELECT  *  FROM   user
  ```

* 正例：

* ```sql
  SELECT   id ,username,tel  FROM   user
  ```

* 原因：

* **节省资源、减少网络开销。**

* **可能用到覆盖索引，减少回表，提高查询效率。**



##### 避免 where 子句中使用 or 

* 反例： **SELECT** ***** **FROM**  **user**  **WHERE**  **id** **=** **1**  **OR** **salary=** **5000**

* 正例： 

* 使用 Union ALL ：

*  **SELECT** ***** **FROM**  **user**  **WHERE**  **id** **=** **1** **UNION**  **ALL**
  **SELECT** ***** **FROM**  **user**  **WHERE** **salary=** **5000**

* 分开两条 sql 写：

* **SELECT** ***** **FROM**  **user**  **WHERE**  **id** **=** **1**

  **SELECT** ***** **FROM**  **user**  **WHERE** **salary=** **5000**

* 理由： 

* 使用or可能会使索引失效，从而全表扫描；

* 对于or没有索引的salary这种情况，假设它走了id的索引，但是走到salary查询条件时，它还得全表扫描；

* 也就是说**整个过程需要三步：全表扫描+索引扫描+合并。如果它一开始就走全表扫描，直接一遍扫描就搞定**；

* 虽然mysql是有优化器的，出于效率与成本考虑，遇到or条件，索引还是可能失效的；





##### 使用数值替代字符串

* 正例：
* 主键（id）：primary key优先使用**数值类型int，tinyint**
* 性别（sex）：0代表女，1代表男；数据库没有布尔类型，**mysql推荐使用tinyint**
* 理由：
* 因为**引擎在处理查询和连接时会逐个比较字符串中每一个字符**；
* 而对于**数字型而言只需要比较一次就够了**；
* 字符会降低查询和连接的性能，并会增加存储开销；





##### 使用 varchar 替换 char

* 反例：
* **`address` char(100) DEFAULT NULL** **COMMENT**  **'地址'**
* 正例：
* **`address` varchar(100) DEFAULT NULL** **COMMENT**  **'地址'**
* 原因： 
* **varchar变长字段按数据内容实际长度存储，存储空间小，可以节省存储空间**；
* char按声明大小存储，**不足补空格**；
* 其次**对于查询来说，在一个相对较小的字段内搜索，效率更高**；





##### where 使用默认值代替 null

* 反例： 

* **SELECT** ***** **FROM**  **user**  **WHERE** **age** **IS**  **NOT**  **NULL**
* 正例：
* **SELECT** ***** **FROM**  **user**  **WHERE** **age>** **0**
* 理由：
* 并不是说使用了is null或者 is not null就会不走索引了，这个跟mysql版本以及查询成本都有关；
* 如果mysql优化器发现，**走索引比不走索引成本还要高**，就会放弃索引，**这些条件 !=，<>，is null，is not null经常被认为让索引失效；**
* 其实是因为一般情况下，查询的成本高，优化器自动放弃索引的；
* **如果把null值，换成默认值，很多时候让走索引成为可能，同时，表达意思也相对清晰一点**；













##### 避免 where 语句中使用 != 或者 <> 

* 反例： 
* **SELECT** ***** **FROM**  **user**  **WHERE** **salary!=** **5000**
* **SELECT** ***** **FROM**  **user**  **WHERE** **salary<>** **5000**
* 理由：
* **使用!=和<>很可能会让索引失效**
* 应尽量避免在where子句中使用!=或<>操作符，否则**引擎将放弃使用索引而进行全表扫描**
* 实现业务优先，实在没办法，就只能使用，并不是不能使用





##### inner join, left join , right join 优先使用 inner join

三种连接如果结果相同，优先使用inner join，如果使用left join左边表尽量小。

-  **inner join 内连接，只保留两张表中完全匹配的结果集；** 
-  **left join会返回左表所有的行，即使在右表中没有匹配的记录；** 
-  **right join会返回右表所有的行，即使在左表中没有匹配的记录；**

* 原因：
* **如果inner join是等值连接，返回的行数比较少，所以性能相对会好一点**；
* 使用了左连接，左边表数据结果尽量小，条件尽量放到左边处理，意味着返回的行数可能比较少；
* 这是mysql优化原则，就是**小表驱动大表**，**小的数据集驱动大的数据集，从而让性能更优**；





##### 提高 group by 语句效率

* 先过滤后分组

* 反例：

* ```go 
  select  job,  avg （salary）  from  employee  
  group by  job 
  having  job = 'develop' or  job =  'test' ;

* 正例：

* ```go
  select  job， avg （salary）  from  employee 
  where  job = 'develop'   or  job =  'test'  
  group   by  job;
  ```

* 理由： **可以在执行到该语句前，把不需要的记录过滤掉**





##### 清空表优先 truncate

* truncate table在功能上与不带 where子句的 delete语句相同：二者均删除表中的全部行。但 **truncate table比 delete速度快，且使用的系统和事务日志资源少。**

* delete语句每次删除一行，并在事务日志中为所删除的每行记录一项。truncate table通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。

* **truncate table删除表中的所有行，但表结构及其列、约束、索引等保持不变**。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。**如果要删除表定义及其数据，请使用 drop table语句**。

* 对于由 foreign key约束引用的表，不能使用 truncate table，而应使用不带  where子句的 DELETE 语句。由于 truncate table不记录在日志中，所以它不能激活触发器。

* **truncate table不能用于参与了索引视图的表**。









##### 操作 delete 和 update 语句，加个 limit 或者循环分批次删除

* 1. 降低写错 SQL 的代价
* 清空表数据可不是小事情，一个手抖全没了，删库跑路？如果加limit，**删错也只是丢失部分数据，可以通过binlog日志快速恢复的**。
* 2. SQL 效率可能更高
* SQL中加了limit 1，如果**第一条就命中目标return， 没有limit的话，还会继续执行扫描表**。
* 3. 避免长事务：
* delete执行时,如果age加了索引，MySQL会将所有相关的行加写锁和间隙锁，所有执行相关行会被锁住，如果删除数量大，会直接影响相关业务无法使用。
* 4. 数据量大的话，容易把CPU打满
* 如果你删除数据量很大时，不加 limit限制一下记录数，容易把cpu打满，导致越删越慢。
* 5. 锁表
* **一次性删除太多数据，可能造成锁表**，会有lock wait timeout exceed的错误，所以建议分批操作。





##### Union 操作符

* UNION在进行**表链接后会筛选掉重复的记录**，所以在表**链接后会对所产生的结果集进行排序运算，删除重复**的记录再返回结果。实际大部分应用中是不会产生重复的记录，**最常见的是过程表与历史表UNION**。如：

* ```sql
  select  username,tel  from   user
  union
  select  departmentname  from  department
  // 但是还是会排序操作，耗时非常的大
  ```

* 1. 这个SQL在运行时先取出两个表的结果，2. 再用排序空间进行排序删除重复的记录，3. 最后返回结果集，**如果表数据量大的话可能会导致用磁盘进行排序**。推荐方案：**采用UNION ALL**操作符替代UNION，因为UNION ALL操作只是简单的将两个结果合并后就返回。





##### 批量插入性能提升



* 多条提交

INSERT   INTO   user  ( id ,username)  VALUES ( 1 , '哪吒编程' );

INSERT   INTO   user  ( id ,username)  VALUES ( 2 , '妲己' );

* 批量提交

INSERT   INTO   user  ( id ,username)  VALUES ( 1 , '哪吒编程' ),( 2 , '妲己' );

* 理由

默认新增SQL有事务控制，**导致每条都需要事务开启和事务提交，而批量处理是一次事务开启和提交**，效率提升明显，达到一定量级，效果显著，平时看不出来。







##### 表连接不能太多，索引不能太多

* 表连接不宜太多，一般5个以内
  * 关联的表个数越多，编译的时间和开销也就越大
  * 每次关联内存中都生成一个临时表
  * 应该把连接表拆开成较小的几个执行，可读性更高
  * 如果一定需要连接很多表才能得到数据，那么意味着这是个糟糕的设计了
  * 阿里规范中，建议多表联查三张表以下
  * 

* 索引不宜太多，一般5个以内
  * 索引并不是越多越好，虽其提高了查询的效率，但却**会降低插入和更新的效率**；
  * **索引可以理解为一个就是一张表，其可以存储数据，其数据就要占空间**；
  * 索引表的数据是排序的，排序也是要花时间的；
  * **insert或update时有可能会重建索引，如果数据量巨大，重建将进行记录的重新排序**，所以建索引需要慎重考虑，视具体情况来定；
  * 一个表的**索引数最好不要超过5个**，若太多需要考虑一些索引是否有存在的必要；







##### 避免在索引列使用内置函数

* 可能索引失效





##### 组合索引

* 排序时**应按照组合索引中各列的顺序进行排序，即使索引中只有一个列是要排序的**，否则排序性能会比较差。

```sql 
create   index  IDX_USERNAME_TEL  on   user (deptid, position ,createtime);
select  username,tel  from   user   where  deptid=  1   and   position  =  'java开发'   order   by  deptid, position ,createtime  desc ; 
```

* 实际上只是查询出符合 deptid= 1 and position = 'java开发' 条件的记录并按createtime降序排序，**但写成order by createtime desc性能较差。**







##### 符合索引最左特性

* 创建复合索引

ALTER   TABLE  employee  ADD   INDEX  idx_name_salary ( name ,salary)

* 满足复合索引的最左特性，哪怕只是部分，复合索引生效

SELECT  *  FROM  employee  WHERE   NAME = '哪吒编程'

* 没有出现左边的字段，则不满足最左特性，**索引失效**

SELECT  *  FROM  employee  WHERE  salary= 5000

* 复合索引全使用，按左侧顺序出现 name,salary，**索引生效**

SELECT  *  FROM  employee  WHERE   NAME = '哪吒编程'   AND  salary= 5000

* 虽然违背了最左特性，但MySQL执行SQL时会进行优化，**底层进行颠倒优化**

SELECT  *  FROM  employee  WHERE  salary= 5000   AND   NAME = '哪吒编程'

* 理由

复合索引也称为联合索引，当我们创建一个联合索引的时候，如(k1,k2,k3)，**相当于创建了（k1）、(k1,k2)和(k1,k2,k3)三个索引，这就是最左匹配原则。**

**联合索引不满足最左原则，索引一般会失效。**









##### 优化 like 语句

* 索引可能失效





##### 使用 explain 分析你的 SQL 执行计划

1. type

* system：表仅有一行，基本用不到；
* const：表最多一行数据配合，主键查询时触发较多；
* eq_ref：对于每个来自于前面的表的行组合，从该表中读取一行。这可能是最好的联接类型，除了const类型；
* ref：对于每个来自于前面的表的行组合，所有有匹配索引值的行将从这张表中读取；
* range：只检索给定范围的行，使用一个索引来选择行。当使用=、<>、>、>=、<、<=、IS NULL、<=>、BETWEEN或者IN操作符，用常量比较关键字列时，可以使用range；
* index：该联接类型与ALL相同，除了只有**索引树被扫描。这通常比ALL快**，因为索引文件通常比数据文件小；
* all：全表扫描；
* 性能排名：system > const > eq_ref > ref > range > index > all。
  实际sql优化中，最后达到ref或range级别。
* 

2. Extra常用关键字

* Using index：**只从索引树中获取信息，而不需要回表查询**；
* Using where：WHERE子句用于限制哪一个行匹配下一个表或发送到客户。除非你专门从表中索取或检查所有行，如果Extra值不为Using where并且表联接类型为ALL或index，查询可能会有一些错误。需要回表查询。
* Using temporary：**mysql常建一个临时表来容纳结果，典型情况如查询包含可以按不同情况列出列的GROUP BY和ORDER BY子句时；**





##### 其他优化方式

1、设计表的时候，所有表和字段都添加相应的注释。

2、SQL书写格式，关键字大小保持一致，使用缩进。

3、修改或删除重要数据前，要先备份。

4、很多时候用 exists 代替 in 是一个好的选择

5、where后面的字段，留意其数据类型的隐式转换。

未使用索引

SELECT  *  FROM   user   WHERE   NAME = 110
（1） 因为不加单引号时，是字符串跟数字的比较，它们类型不匹配； 

（2）MySQL会做隐式的类型转换，把它们转换为数值类型再做比较；

6、尽量把所有列定义为NOT NULL

NOT NULL列更节省空间，NULL列需要一个额外字节作为判断是否为 NULL的标志位。NULL列需要注意空指针问题，NULL列在计算和比较的时候，需要注意空指针问题。

7、伪删除设计

8、数据库和表的字符集尽量统一使用UTF8

（1）可以避免乱码问题；

（2）可以避免，不同字符集比较转换，导致的索引失效问题；

9、select count(*) from table；

**这样不带任何条件的count会引起全表扫描，并且没有任何业务意义，是一定要杜绝的。**

10、避免在where中对字段进行表达式操作

（1）SQL解析时，如果字段相关的是表达式就进行全表扫描 ；

（2）字段干净无表达式，索引生效；

11、关于临时表

（1）避免频繁创建和删除临时表，以减少系统表资源的消耗；

（2）在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log；

（3）如果数据量不大，为了缓和系统表的资源，应先create table，然后insert；

（4）如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除。先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定；

12、索引不适合建在有大量重复数据的字段上，比如性别，排序字段应创建索引

13、去重distinct过滤字段要少

带distinct的语句占用cpu时间高于不带distinct的语句
当查询很多字段时，如果使用distinct，数据库引擎就会对数据进行比较，过滤掉重复数据
然而这个比较、过滤的过程会占用系统资源，如cpu时间
14、尽量避免大事务操作，提高系统并发能力

15、所有表必须使用Innodb存储引擎

Innodb「支持事务，支持行级锁，更好的恢复性」，高并发下性能更好，所以呢，没有特殊要求（即Innodb无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用Innodb存储引擎。

16、尽量避免使用游标

因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。



#### exists 和 in 的区别

* 有两个表需要关联查询，表的情况如下：

* ```sql
  # 2759174行数据
  SELECT COUNT(*) FROM tb_data t1;
  
  # 7262行数据
  SELECT COUNT(*) FROM tb_task t2;
  
  # 执行时间为44.88s
  SELECT SQL_NO_CACHE t1.id FROM tb_data t1 WHERE t1.task_id IN (SELECT t2.id FROM tb_task t2);
  
  # 执行时间为28.93s
  SELECT SQL_NO_CACHE t1.id FROM tb_data t1 WHERE EXISTS (SELECT * FROM tb_task t2 WHERE t1.task_id = t2.id);
  ```

*  **如果两个表中一个表大，另一个是表小，那么IN适合于外表大而子查询表小的情况；EXISTS适合于外表小而子查询表大的情况。**

* exists 更加高效，相对于 in，所以是索引优化中的一部分



#### limit 为什么低效

* 为了实现分页。很容易联想到下面这样的sql语句。

* ```sql
  select * from page order by id limit offset, size;
  ```

* 比如一页有10条数据。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiak10WxVpianzxZicJKTb4Kg74LvjrBh7xiadCuBXbe6aSdBKWQiaVC6iaO6A7qCaJx3GpqpLdhHR39WZrw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 第一页就是下面这样的sql语句。

* ```sql
  select * from page order by id limit 0, 10;
  ```

* 第一百页就是

* ```sql
  select * from page order by id limit 990, 10;
  ```

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiak10WxVpianzxZicJKTb4Kg74FUeAWfaCOvicElrZiasM9TodjcTCqJ8WV3PSTl4EEHSy1Nob6icNBao7g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* server层有很多模块，其中需要关注的是**执行器**是用于跟存储引擎打交道的组件。

* 执行器可以通过调用存储引擎提供的接口，将一行行数据取出，当这些数据完全符合要求（比如满足其他where条件），则会放到**结果集**中，最后返回给调用mysql的**客户端**

* 我们可以对下面的sql先执行下 `explain`。

* ```sql
  explain select * from page order by id limit 0, 10;
  ```

* 可以看到，explain中提示 key 那里，执行的是**PRIMARY**，也就是走的**主键索引**。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiak10WxVpianzxZicJKTb4Kg74icOcnp1qZWFtagXaI57jjvzuwicr4rGZNaia8jfRPGqq5cZnKH5aqPyHg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 但不管是主键还是非主键索引，他们的叶子结点数据都是**有序的**。比如在主键索引中，这些数据是**根据主键id的大小，从小到大，进行排序的。**

* ```sql
  select * from page order by id limit 6000000, 10;
  ```

* server层会调用innodb的接口，由于这次的offset=6000000，会在innodb里的主键索引中获取到第0到（6000000 + 10）条**完整行数据**，**返回给server层之后根据offset的值挨个抛弃，最后只留下最后面的size条**，也就是10条数据，放到server层的结果集中，返回给客户端。

* 当offset非0时，server层会从引擎层获取到**很多无用的数据**，而获取的这些无用数据都是要耗时的。

* **mysql查询中 limit 1000,10 会比 limit 10 更慢。原因是 limit 1000,10 会取出1000+10条数据，并抛弃前1000条，这部分耗时更大**

* 





#### count(*) 为什么低效

![图片](https://img-blog.csdnimg.cn/img_convert/d9b9817e92f805e9a16faf31a2c10d9a.png)

##### 哪种 count 性能最好

* ![图片](https://img-blog.csdnimg.cn/img_convert/af711033aa3423330d3a4bc6baeb9532.png)

* count() 是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是**统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个**。

* ```sql 
  select count(name) from t_order;
  ```

* 这条语句是统计「 t_order 表中，**name 字段不为 NULL 的记录**」有多少个。也就是说，如果某一条记录中的 name 字段的值为 NULL，则就不会被统计进去。

* ```sql
  select count(1) from t_order;
  ```

* 这条语句是统计「 t_order 表中，1 这个表达式不为 NULL 的记录」有多少个。

* 1 这个表达式就是单纯数字，它永远都不是 NULL，所以上面这条语句，其实是在**统计 t_order 表中有多少个记录**



##### count(主键字段) 执行过程

* 在通过 count 函数统计有多少个记录时，MySQL 的 **server 层会维护一个名叫 count 的变量**。

* **server 层会循环向 InnoDB 读取一条记录**，如果 count 函数指定的参数不为 NULL，那么就会将变量 count 加 1，直到符合查询的全部记录被读完，就退出循环。**最后将 count 变量的值发送给客户端。**

* ```sql
  //id 为主键值
  select count(id) from t_order;
  ```

* 如果表里只有主键索引，没有二级索引时，那么，InnoDB 循环遍历聚簇索引，将读取到的**记录返回给 server 层，然后读取记录中的 id 值，就会 id 值判断是否为 NULL，如果不为 NULL，就将 count 变量加 1**。

* ![图片](https://img-blog.csdnimg.cn/img_convert/9bb4f32ac843467684a2664d4db61ae3.png)

* 但是，如果表里有二级索引时，**InnoDB 循环遍历的对象就不是聚簇索引，而是二级索引**。

* ![图片](https://img-blog.csdnimg.cn/img_convert/aac550602ef1022e0b45020dbe0f716a.png)

* 这是因为**相同数量的二级索引记录可以比聚簇索引记录占用更少的存储空间**，所以**二级索引树比聚簇索引树小**，这样遍历二级索引的 I/O 成本比遍历聚簇索引的 I/O 成本小，因此「优化器」**优先选择的是二级索引**



##### count(1) 执行过程

```sql
select count(1) from t_order;
```

* ![图片](https://img-blog.csdnimg.cn/img_convert/e630fdc5897b5c5dbc332e8838afa1fc.png)
* 那么，InnoDB 循环遍历聚簇索引（主键索引），将读取到的记录返回给 server 层，**但是不会读取记录中的任何字段的值**，因为 count 函数的参数是 1，不是字段，所以不需要读取记录中的字段值。参数 1 很明显并不是 NULL，**因此 server 层每从 InnoDB 读取到一条记录，就将 count 变量加 1**。
* count(1) 相比 count(主键字段) 少一个步骤，就是不需要读取记录中的字段值，所以通常会说 **count(1) 执行效率会比 count(主键字段) 高一点**。
* 但是，**如果表里有二级索引时，InnoDB 循环遍历的对象就二级索引了。**
* ![图片](https://img-blog.csdnimg.cn/img_convert/01e83441a7721f0864deb1ac14ad8ea6.png)
* ![图片](https://img-blog.csdnimg.cn/img_convert/f24dfeb85e2cfce0e4dc3a17b893b3f5.png)



##### 













##### count(*) 执行过程

* 看到 `*` 这个字符的时候，是不是大家觉得是读取记录中的所有字段值？
* 对于 `selete *` 这条语句来说是这个意思，但是在 count(*) 中并不是这个意思。
* **count(`\*`) 其实等于 count(`0`)**，也就是说，当你使用 count(`*`) 时，MySQL 会将 `*` 参数转化为参数 0 来处理。



* ![图片](https://img-blog.csdnimg.cn/img_convert/27b229f049b27898f3a86c7da7e26114.png)
* 所以，**count(\*) 执行过程跟 count(1) 执行过程基本一样的**，性能没有什么差异。







##### count(字段) 执行过程

* count(字段) 的执行效率相比前面的 count(1)、 count(*)、 count(主键字段) 执行效率是最差的。
* 对于这个查询来说，**会采用全表扫描的方式来计数，所以它的执行效率是比较差的**。





##### 小结

* 如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引**效率会高一些**。
* 就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。**如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引**



##### 为什么 count() 采用的是遍历的方式

* 我前面将的案例都是基于 Innodb 存储引擎来说明的，但是在 MyISAM 存储引擎里，执行 count 函数的方式是不一样的，通常在没有任何查询条件下的 count(*)，MyISAM 的查询速度要明显快于 InnoDB。
* 使用 MyISAM 引擎时，执行 count 函数只需要 O(1 )复杂度，这是因为每张 MyISAM 的数据表都有一个 meta 信息有存储了row_count值，由表级锁保证一致性，所以直接读取 row_count 值就是 count 函数的执行结果。
* 而 InnoDB 存储引擎是支持事务的，同一个时刻的多个查询，**由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的**，所以无法像 MyISAM一样，只维护一个 row_count 变量。
* ![图片](https://img-blog.csdnimg.cn/img_convert/04d714293f5c687810562e984b67d2e7.png)
* 在会话 A 和会话 B的最后一个时刻，同时查表 t_order 的记录总个数，可以发现，**显示的结果是不一样的**。所以，**在使用 InnoDB 存储引擎时，就需要扫描表来统计具体的记录**。
* 





##### 如何优化 count(*)

* 比如下面我这个案例，表 t_order 共有 1200+ 万条记录，我也创建了二级索引，但是执行一次 `select count(*) from t_order` 要花费差不多 5 秒！
* ![图片](https://img-blog.csdnimg.cn/img_convert/74a4359b58dc6ed41a241e425f43764d.png)



###### 近似值

* 我们就可以使用 show table status 或者 explain 命令来表进行估算。
* 执行 explain 命令效率是很高的，因为它并不会真正的去查询，下图中的 rows 字段值就是 explain 命令对表 t_order 记录的估算值。
* ![图片](https://img-blog.csdnimg.cn/img_convert/7590623443e8f225e5652109e6d9e3d2.png)



###### 额外表保存计数值

* 如果是想精确的获取表的记录总数，我们**可以将这个计数值保存到单独的一张计数表中。**
* 当我们在数据表插入一条记录的同时，将计数表中的计数字段 + 1。也就是说，**在新增和删除操作时，我们需要额外维护这个计数表。**







#### 覆盖索引

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2019/10/16/16dd033b2a2c7c24~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.awebp)

* key 是联合索引值，可以直接获取数据的，但是 value 值依旧是 主键值，这算是二级索引的一种
* 覆盖索引（covering index ，或称为索引覆盖）即从**非主键索引中就能查到的记录，而不需要查询主键索引中的记录**，**避免了回表**的产生减少了树的搜索次数，显著提升性能。





#### 索引的基础知识

* 如果**叶子节点存储的是实际数据的就是聚簇索引**，一个表只能有一个聚簇索引(主键索引）；如果叶子节点存储的不是实际数据，而是**主键值则就是二级索引**，一个表中可以有多个二级索引。

* 覆盖索引：当一条查询语句符合覆盖索引的条件的时候，**sql 只需要通过索引就可以查找到所需要的数据**，  索引包含需要的所有数据，这样就可以避免查询到索引之后，再进行回表操作，减少 I/O 提高效率。(比如二级索引希望查询的是 主键 ID，可以直接在二级索引表中找到主键 id，不需要回表操作)

* Innodb 使用聚集索引组织数据，如果二级索引中包含查询所需要的数据，那么不需要回表进行数据的查询了。

* 以下情况不会选择覆盖查询：

  > * seletc 选择的字段含有不在索引中的字段，即索引没能够覆盖所有的字段
  > * where 条件中不能含有包含对索引进行 like 操作。

* 按「数据结构」分类：**B+tree索引、Hash索引、Full-text索引**。

* 按「物理存储」分类：**聚簇索引（主键索引）、二级索引（辅助索引）**。

* 按「字段特性」分类：**主键索引、唯一索引、普通索引、前缀索引**。

* 按「字段个数」分类：**单列索引、联合索引**。

* **聚集索引（clustered inex，就是主键索引）和辅助索引（secondary index）其实都是一种 B+ 树索引**
* **聚集索引与辅助索引不同之处就是，叶子节点存放的是否是一整行的信息**
* **聚集索引就是按照每张表的主键构造一棵 B+ 树，同时叶子节点中存放的即为表中一行一行的数据**，所以聚集索引的叶子节点也被称为数据节点
* 因为数据页只能在一个 B+ 树上进行查找查询，或者说没法同时把数据行存放在两个不同的位置，所以**每张表只能拥有一个聚集索引**。
* 在聚集索引中，**索引即数据，数据即索引**。
* 聚集索引是按照主键来构建的，一张表只能一个聚集索引表，同时只有一个唯一主键
* 主键是唯一的，是一种约束，同时主键非空。
* 聚集索引不是一种约束，他是一种索引，目的是为了提高数据的查询效率。它决定的是数据库的物理存储结构
* **聚集索引一般都是加在主键上的。聚集索引是根据主键建立的。**



##### 索引的缺点

1. **创建索引和维护索引要耗费时间**，这种时间随着数据量的增加而增加

2. **索引需要占物理空间**，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大

3. **当对表中的数据进行增加、删除和修改的时候**，索引也要动态的维护，降低了数据的维护速度











#### B+ 在索引中的作用

##### 数据页

* 因此，**InnoDB 的数据是按「数据页」为单位来读写的**，也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。
* 数据库的 I/O 操作的最小单位是页，**InnoDB 数据页的默认大小是 16KB**，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。
* ![图片](https://img-blog.csdnimg.cn/img_convert/243b1466779a9e107ae3ef0155604a17.png)
* 在 File Header 中有两个指针，分别**指向上一个数据页和下一个数据页**，连接起来的页相当于一个双向的链表，（物理上不连续，但是逻辑上连续）
* ![图片](https://img-blog.csdnimg.cn/img_convert/557d17e05ce90f18591c2305871af665.png)
* **数据页中的记录按照「主键」顺序组成单向链表**，(注意数据页之间是采用双向链表的) 单向链表的特点就是插入、删除非常方便，但是检索效率不高，最差的情况下需要遍历链表上的所有节点才能完成检索。
* 数据页中有一个**页目录**，起到记录的索引作用.
* <img src="https://img-blog.csdnimg.cn/img_convert/261011d237bec993821aa198b97ae8ce.png" alt="图片" style="zoom:80%;" />
* 只有**叶子节点（最底层的节点）才存放了数据**，**非叶子节点（其他上层节）仅用来存放目录项作为索引**。
* 页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slot），**每个槽相当于指针指向了不同组的最后一个记录**。
* **我们通过槽查找记录时，可以使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到对应的记录**，
* InnoDB 里的 B+ 树中的**每个节点都是一个数据页**
* ![图片](https://img-blog.csdnimg.cn/img_convert/7c635d682bd3cdc421bb9eea33a5a413.png)
* 我们再看看 B+ 树如何实现快速查找主键为 6 的记录，以上图为例子：
  - 从根节点开始，通过二分法快速定位到符合页内范围包含查询值的页，因为查询的主键值为 6，在[1, 7)范围之间，所以到页 30 中查找更详细的目录项；
  - 在非叶子节点（页30）中，继续通过**二分法快速定位到符合页内范围包含查询值的页**，主键值大于 5，所以就到叶子节点（页16）查找记录；
  - 接着，在叶子节点（页16）中，通过槽查找记录时，使用**二分法快速定位要查询的记录在哪个槽**（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到主键为 6 的记录。
* 在定位记录所在哪一个页时，也是通过二分法快速定位到包含该记录的页（在非页节点上）。定位到该页后，又会在该页内进行二分法快速定位记录所在的分组（槽号），最后在分组内进行遍历查找。

##### 聚簇索引和二级索引

* 聚簇索引(**只有一个而且必定存在一个**）的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚簇索引的叶子节点；

* 二级索引（**可以有多个或者没有**）的叶子节点存放的是主键值，而不是实际数据。

* InnoDB 在创建聚簇索引时，会根据不同的场景选择不同的列作为索引：

* > - 如果有主键，默认会使用主键作为聚簇索引的索引键；
  > - 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键；
  > - 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键；

* **如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，也就是说要查两个 B+ 树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据**

* 





#### B+ 树 和 SkipList 的区别

* B+树和跳表的**最下面一层，都包含了所有的数据**，且都是**顺序的，适合用于范围查询**。往上的层级都是构建出来用于提升搜索性能的。
* 他们两者在**新增和删除数据**时，还是有些区别的
* 只有在叶子和索引结点**都满了**的情况下，B+树才会考虑加入一层新的结点！！！！
* 跳表新增加层节点完全按照随机函数决定的， 当**数据量样本足够大**的时候，数据的分布就符合我们理想中的"二分"。
* 跟上面B+树不一样，**跳表是否新增层数，纯粹靠随机函数，根本不关心前后上下结点**。



##### MySQL 为什么使用 B+ 树

* **B+树**是多叉树结构，每个结点都是一个16k的数据页，能存放较多**索引信息**(非页节点），所以**扇出很高**。**三层**左右就可以存储`2kw`左右的数据
* 数据库单表**建议最大2kw**条数据这个说法。如果超过了，性能就会下降得比较厉害。（需要进行分表的操作）
* 也就是说查询一次数据，如果**这些数据页都在磁盘里**，那么最多需要查询**三次磁盘IO**。（如果有些数据页不在磁盘中而是在内存中，需要查找的次数可能更少）

###### 3 层 B+ 树最多存多少行数据

* 假设：

* > - 非叶子结点内指向其他内存页的指针数量为`x`
  >
  > - 叶子节点内能容纳的record数量为`y`
  >
  > - B+树的层数为`z`
  >
  > - ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiandvicKNoFbEtJibZ97kqPWvkPzbuItrzCmkKKERFNNIeLOpbByNP3EniaRQmYicpec9YDhTRusPpEib9g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
  >
  > - 那这棵B+树放的**行数据总量**等于 `(x ^ (z-1)) * y`。
  >
  > - x 如何计算：
  >
  > - > * ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiandvicKNoFbEtJibZ97kqPWvkO4CFg8ST4GniaC57pH68qCppCkic0msFhLnJHZke0rhJyTEFPQ3vxNlg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
  >   > * 非叶子节点里主要放索引查询相关的数据，放的是**主键和指向页号**。
  >   > * 主键假设是`bigint（8Byte）`，而页号在源码里叫`FIL_PAGE_OFFSET（4Byte）`，那么非叶子节点里的一条数据是`12Byte`左右。
  >   > * 整个数据页`16k`， 页头页尾那部分数据全加起来大概`128Byte`，加上页目录毛估占`1k`吧。那剩下的**15k**除以`12Byte`，等于`1280`，也就是可以指向**x=1280页**。
  >   > * 我们常说的二叉树指的是一个结点可以发散出两个新的结点。m叉树一个节点能指向m个新的结点。这个指向新节点的操作就叫**扇出（fanout）**。
  >   > * 而上面的B+树，它能**指向1280个新的节点**，恐怖如斯，可以说**扇出非常高**了。
  >
  > - y 如何计算
  >
  > - > * 叶子节点和非叶子节点的数据结构是一样的，所以也假设剩下`15kb`可以发挥。
  >   > * 叶子节点里放的是真正的行数据。**假设一条行数据`1kb`**，所以一页里能放**y=15行**。
  >
  > - z 如何计算
  >
  > - > * B+ 树是两层：
  >   > * 假设B+树是**两层**，那`z=2`。则是`(1280 ^ (2-1)) * 15 ≈ 2w`
  >   > * B+ 树是三层：
  >   > * 假设B+树是**三层**，那`z=3`。则是`(1280 ^ (3-1)) * 15 ≈ 2.5kw`

* **这个2.5kw，就是我们常说的单表建议最大行数2kw的由来。**毕竟再加一层，数据就大得有点离谱了。三层数据页对应最多三次磁盘IO，也比较合理。



* 行数超 1 亿就慢了吗？

* > * 上面假设单行数据用了1kb，所以一个数据页能放个15行数据。
  > * 如果我单行数据用不了这么多，比如只用了`250byte`。那么单个数据页能放60行数据。
  > * 同样是三层B+树，单表支持的行数就是 `(1280 ^ (3-1)) * 60 ≈ 1个亿`。
  > * 一个亿的数据，其实也就三层B+树，在这个B+树里要查到某行数据，最多也是三次磁盘IO。所以并不慢
  > * 主要看每一个数据行的大小，判断是否可以存放 1 亿的数据



###### B树可以承载的数据量

* 跟B+树最大的区别在于，**B+树只在末级叶子结点处放数据表行数据，而B树则会在叶子和非叶子结点上都放。**
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiandvicKNoFbEtJibZ97kqPWvksyicUZnAiaicOWAPbCh7YKB13QQjxKf4qibn2r12lVryEFUIrCsa3SricZA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
* B树将行数据都存在非叶子节点上，假设每个数据页还是16kb，掐头去尾每页剩15kb，并且一条数据表行数据还是占1kb，**就算不考虑各种页指针的情况下**，也只能放个15条数据。**数据页扇出明显变少了。**（B+ 树非页节点的扇出数量可以达到 1280）
* 计算可承载的总行数的公式也变成了一个**等比数列** ： 15 + 15^2 +15^3 + ... + 15^z （**z还是层数**）
* 为了能放`2kw`左右的数据，需要`z>=6`。也就是树需要有6层，查一次要访问6个页。假设这6个页并不连续，为了查询其中一条数据，最坏情况需要进行**6次磁盘IO**。
* 磁盘IO越多则越慢，这两者在性能上差距略大。
* 为此，**B+树比B树更适合成为mysql的索引。**

###### MySQL 选择 B+树的原因

**读性能：**

* **跳表**是链表结构，一条数据一个结点，如果最底层要存放`2kw`数据，且每次查询都要能达到**二分查找**的效果，`2kw`大概在`2的24次方`左右，所以，跳表大概高度在**24层**左右。最坏情况下，这24层数据会分散在不同的数据页里，也即是查一次数据会经历**24次磁盘IO**。
* B+树的高度比跳表的要少，如果放在mysql数据库上来说，就是**磁盘IO次数更少，因此B+树查询更快**。



**写性能：**

* B+树需要**拆分合并索引数据页**，跳表则独立插入，并根据随机函数确定层数，**没有旋转和维持平衡的开销**，因此**跳表的写入性能会比B+树要好。**



* mysql的**存储引擎是可以换的**，以前是`myisam`，后来才有的`innodb`，它们底层索引用的都是**B+树**。
* 你完全可以造一个索引为跳表的存储引擎装到mysql里。事实上，`facebook`造了个`rocksDB`的存储引擎，里面就用了**跳表**。（写入性能更好）







##### Redis 为什么使用 skiplist

* redis 是纯纯的内存数据库。
* 进行读写数据都是操作内存，跟磁盘没啥关系，因此也**不存在磁盘IO**了，所以层高就不再是跳表的劣势了。
* 并且前面也提到B+树是有一系列合并拆分操作的，换成红黑树或者其他AVL树的话也是各种旋转，目的也是**为了保持树的平衡**。
* 而跳表插入数据时，只需要随机一下，就知道自己要不要往上加索引，根本不用考虑前后结点的感受，也就**少了旋转平衡的开销**。

```cpp
// 跳表数据结构
typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    unsigned long length;
    int level;
} zskiplist;
```

跳表结构里包含了：

- 跳表的头尾节点，便于在O(1)时间复杂度内访问跳表的头节点和尾节点；
- 跳表的长度，便于在O(1)时间复杂度获取跳表节点的数量；
- 跳表的最大层数，便于在O(1)时间复杂度获取跳表中层高最大的那个节点的层数量；



* **跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数**。 （p = 0.25）

* **从内存占用上来比较，跳表比平衡树更灵活一些**。平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 1/(1-p)，具体取决于参数 p 的大小。如果像 Redis里的实现一样，取 p=1/4，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。
* **在做范围查找的时候，跳表比平衡树操作要简单**。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在跳表上进行范围查找就非常简单，**只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现**。
* **从算法实现难度上来比较，跳表比平衡树要简单得多**。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速。













#### MySQL 尽量不要使用 select *

* 原因：效率原因
* 原因一：使用组合索引，当我们进行一下查询 ` select * from Stu where job2 = ""`  存在一个联合索引 （job1, job2, job3) 那么我们可以采用在二级索引表中进行查询，不需要回表全表查询，使用语句： `select job1 job2 job3 from Stu where job2 = ""` 这个时候可以直接二级索引表找出需要查询的数据。（扫描二级索引的 B+ 树的叶子结点进行遍历扫描）
* 避免使用 SELECT * ，会使覆盖索引策略失效，**多索引情况下尽量使用联合索引减少开销。**





#### MySQL 有哪些锁

* 尽量不要使用表锁，因为表锁的颗粒度太大，会影响并发性能， InnoDB 可以使用颗粒度更细的行级锁。

##### 全局锁

> * ```sql
>   flush tables with read lock
>   ```
>
> * 执行上面语句就处于只读状态了，执行其他操作，都会被阻塞。
>
> * 应用场景：**全库逻辑备份**操作，这样不会在备份数据库期间，不会因为数据或者表结构的更新，二出现备份文件的数据和预期不同。
>
> * 解决方案：在备份数据库之前开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View ,而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新的操作.
>
> * 加上全局锁，意味着整个数据库都是**只读状态**。
>
>   那么如果数据库里有很多数据，备份就会花费很多的时间，关键是备份期间，业务只能读数据，而不能更新数据，这样会造成业务停滞。





##### 表级锁

> * 表锁
>
> * **innodb 不会对select、insert、delete、update语句加表锁的。**(只会加行锁)
>
>   > * 对学生表加表锁：
>   >
>   > * ```sql 
>   >   // 表级别的共享锁，也就是读锁
>   >   lock table t_student read;
>   >   // 表级别的独占锁，也就是写锁
>   >   lock table t_student write;
>   >   ```
>   >
>   > * 表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作(当前线程加读锁，之后当前线程没法执行写操作)
>   >
>   > * 释放表锁： unlock tables, 同时会话退出也会释放所有的表锁
>
> * 元数据锁
>
>   > * MDL 不需要手动的对数据表进行操作，会**自动加上这个 MDL**
>   > * 对一张表进行 CRUD 操作的时候，加的是 MDL 读锁，例如当有线程执行 select 语句的期间，有其他线程需要更改该表的结构（申请 MDL 写锁）阻塞，直到执行完 select 语句（释放 MDL 读锁）
>   > * 对一张表做结构变更的时候，加的是 MDL 写锁,例如，当在对一个表进行结构变更期间，有其他线程执行了 CRUD 操作（申请MDL读锁），那么会阻塞，直到表结构变更完成
>   > * **MDL 释放时间：在事务提交之后才释放。**
>   > * 申请MDL 锁会形成一个队列，**队列中的写锁优先级是比读锁优先级高的**，所以一旦MDL写锁出现阻塞等待，那么后序的 CRUD 操作会阻塞，因为没法申请到读锁，因为被前面的写锁给挡住了。
>   > * 对表结构进行变更的时候，先看一下表中是否存在长事务，**是否有事务已经对表加上了 MDL 读锁**，如果存在那么先删除这个长事务，然后再对表结构变更。（获取写锁，从而可以对表数据进行变更操作）
>
> * 意向锁：
>
>   > * **普通的 select 是不会加上行级锁**，普通的 select 使用的是 MVCC 实现的一致性读，是无锁的。
>   >
>   > * 当执行**插入、更新、删除的时候**，需要加上意向独占锁，然后对该记录加上独占锁，行记录加上**独占锁**(是行级锁)
>   >
>   > * 在使用 InnoDB 表里某些记录加上 共享锁 之前，需要先在表级别上加上一个 ：意向共享锁
>   >
>   > * 在使用 InnoDB 表里某些记录加上 独占锁 之前，需要先在表级别上加上一个 ：意向独占锁
>   >
>   > * 意向共享锁和意向独占锁是表级锁，**不会和行级的共享锁和独占锁发生冲突**，同时意向锁之间也是不会发生冲突的，只会和**共享表锁和独占表锁**发生冲突
>   >
>   > * select 也是可以对记录加共享锁和独占锁的(这个锁是行级锁)
>   >
>   > * ```sql
>   >   //先在表上加上意向共享锁，然后对读取的记录加共享锁
>   >   select ... lock in share mode;
>   >                       
>   >   //先表上加上意向独占锁，然后对读取的记录加独占锁
>   >   select ... for update;
>   >   ```
>   >
>   > * 
>   >
>   > * 因为没有意向锁，我们**加表级独占锁的时候，需要检查整个表中是否存在行级独占锁**(遍历的手段)，效率不行
>   >
>   > * 有了意向独占锁，可以先检查是否存在意向独占锁，因为在加表级独占锁之前需要加上意向独占锁，只要检查一次就行了，效率非常高
>   >
>   > * **意向锁存在的目的是为了快速判断表里是否有记录被加了锁**
>
> * AUTO-INC 锁
>
>   > * 





##### 行级锁

> * 记录锁：每条记录加上锁
>
> * 间隙锁： 锁定一个范围，不包含记录本身
>
> * next-key lock ： 记录锁 + 间隙锁
>
> * ```sql
>   //对读取的记录加共享锁
>   select ... lock in share mode;
>                       
>   //对读取的记录加独占锁
>   select ... for update;
>   ```
>
> * 



##### 乐观锁

* **使用版本号实现乐观锁**

* > * **一个是数据版本机制，一个是时间戳机制**
  > * 何谓数据版本？
  > * A : 即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，**如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。**
  > * ![img](https://images2017.cnblogs.com/blog/1075594/201712/1075594-20171224120703021-326642906.png)
  > * 时间戳机制:  同样是在需要乐观锁控制的table中增加一个字段，名称无所谓，**字段类型使用时间戳（timestamp）**, 和上面的version类似，也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK，否则就是版本冲突。

* **注意：乐观锁的更新操作，最好用主键或者唯一索引来更新,这样是行锁，否则更新时会锁表**





#### MySQL 怎么加锁的？（行级锁）

* next-key lock 是一个 ( , ] . 左开右闭的锁。间隙锁是左开右开 ( , )

* 数据库的列可能包含： 主键索引：唯一索引，普通索引： 非唯一索引，还有些列属于：普通的列



![图片](https://img-blog.csdnimg.cn/img_convert/954708d2f29c2a619e861e57cdf74c11.png)

* 唯一索引等值查询？

> * 当查询记录存在的时候, next-key  lock 会退化成：记录锁
>
>   > * ![图片](https://img-blog.csdnimg.cn/img_convert/2a944fb385d1de277dbfdc78102f36ba.png)
>   > * 加锁的基本单位是 next-key lock, 因此会话一会变成(8, 16];
>   > * 由于使用唯一索引，同时查询的记录也存在，所以 next-key lock 退化成记录锁，所以最终加锁的范围就是 id = 16 这一行.
>
> * 当查询的记录不存在的时候，next-key lock 会退化成：间隙锁
>
>   > * ![图片](https://img-blog.csdnimg.cn/img_convert/d82332993969a223fa3433eaf5185134.png)
>   > * 加锁的基本单位是 next-key lock ，因此主键索引 id 的加锁范围是 (8, 16]
>   > * 但由于查询的数据不存在，退化成间隙锁，所以 id = 16 就可以正常的查询， 因为锁住的范围是 (8, 16)











* 唯一索引范围查询？

```sql
select * from t_test where id=8 for update;
select * from t_test where id>=8 and id<9 for update;
```



![图片](https://img-blog.csdnimg.cn/img_convert/c12bdb8af1972d5f287978b489a83304.png)



> * 第一个语句等值查询 id = 8，next-key lock 的锁范围是： **(4, 8],由于能找到唯一主键索引，所以变成记录锁**，**只对 id = 8 这一行加锁**
> * 第二个语句是范围查询：所以 next-key lock 范围是 (8, 16], 由于 id = 16 不满足 id < 9 所以会变成间隙锁，所以最终是一个间隙锁，范围是 ： （8， 16）
> * 所以会话2阻塞，间隙锁原因，会话3阻塞行锁的原因，间隙四正常。





* 非唯一索引等值查询？

> * 当查询记录存在的时候，除了会加 next-lock key 外，还会额外加间隙锁，也就是会加两把锁
>
> * 当查询的记录不存在的时候，只会加 next-key lock，然后会退化成间隙锁，加一把锁
>
> * ![图片](https://img-blog.csdnimg.cn/img_convert/be6818bd1fd22c4e790b989bde11e6b1.png)
>
> * 索引存在：
>
>   > * 对普通索引 b 加上 next-key lock 范围是 (4, 8]
>   > * 因为是非唯一索引，还需要加上间隙锁，规则是向下遍历到第一个不符合条件的值才能停止, 返回是： （8， 16），所以最终的锁住的范围是 ： (4, 16)
>
> * 索引不存在
>
> * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/%E6%A1%88%E4%BE%8B5.drawio.png)
>
>   > * 对普通索引 b 加上 next-key lock 范围是：（8, 16]
>   > * 由于查询的记录不存在，所以不会额外的加上一个间隙锁，同时 next-key lock 退化成间隙锁：范围是： （8， 16）





* 非唯一索引范围查询？



> * ![图片](https://img-blog.csdnimg.cn/img_convert/ac6c09c4011259d3397e62e52a37d384.png)
>
> * 普通索引范围查询，**next-lock key 不会退化成间隙锁和记录锁**
>
>   > * 上面这个查询语句：一开始找 b = 8, 因此 next-key lock (4, 8], 但是由于 b 不是唯一索引，不会退化成记录锁
>   > * 范围查找，往后找，找到 b = 16 停止，所以 next-key lock （8， 16], 不会退化成间隙锁。







#### MySQL 死锁了怎么办？



* **间隙锁和间隙锁之间是兼容的**，**插入意向所和间隙锁不是兼容的，是冲突的**

* **间隙锁本质上是用于阻止其他事务在该间隙内插入新记录，而自身事务是允许在该间隙内插入数据的**。也就是说间隙锁的应用场景包括**并发读取、并发更新、并发删除和并发插入**。

* **插入意向锁是一种特殊的间隙锁，但不同于间隙锁的是，该锁只用于并发插入操作**。

* 间隙锁锁住的是一个区间，那么「插入意向锁」锁住的就是一个点。因而从这个角度来说，插入意向锁确实是一种特殊的间隙锁。

* 尽管「插入意向锁」也属于间隙锁，但**两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向**锁（当然，插入意向锁如果不在间隙锁区间内则是可以的）。

* ![图片](https://img-blog.csdnimg.cn/img_convert/90c1e01d0345de639e3426cea0390e80.png)

* 案例中的事务 A 和事务 B 在执行完后 `select ... for update` 语句后都持有范围为`(1006,+∞）`的间隙锁，而接下来的**插入操作为了获取到插入意向锁**，都在等待对方事务的间隙锁释放，于是就造成了循环等待，导致死锁。

* 死锁形成的四个条件：互斥，请求保持，不可剥夺，循环等待，必要的四个条件，破坏这四个条件中的一个就可以了。

* 数据库通常使用**打破循环等待条件**解除死锁：

  > * 设置事务等待锁的超时时间，当一个事务的等待超时时间超过该值后，就对这个**事务回滚**，于是锁就释放了，另外一个事务就可以执行了。
  > * 开启主动死锁检测：主动死锁检测就是在发生死锁之后，主动**回滚**死锁链条的某一个事务，让其他事务得以继续的运行。
  
* 如果 update 语句的 **where 条件没有用到索引列**，那么就会**全表扫描**，在一行行扫描的过程中，不仅给行记录加上了行锁，还给行记录两边的空隙也加上了间隙锁，相当于锁住整个表，然后直到事务结束才会释放锁。

* 













#### 聊聊索引失效？失效的原因是什么?

* 模型数空运最快：模糊差找、数据类型、函数、（NULL）、运算、最左原则、全表扫描最快

* 索引失效就是不能根据索引进行数据的查询，需要进行一个**全表扫描的查询**

* 对主键字段建立的索引叫做聚簇索引，对普通字段建立的索引叫做二级索引。

* 二级索引的叶子节点包含的是 ： 索引值 + 主键值





* 如果我们使用主键索引字段作为条件查询的时候，如果查询的数据是在 聚簇索引 的叶子节点中，那么就可以在聚簇索引的 B+ 树检索到对应的叶子节点，然后直接读取出需要的数据

```sql
// id 作为主键
select * from t_user where id = 1;
```

* 如果我们使用二级索引，那么如果查询的数据是在 聚簇索引的叶子节点中，那么我们需要检索两颗树：

> * 先在 二级索引 的B+树找到对应的叶子节点，获取主键值
>
> * 然后根据上一步获取的主键值，在聚簇索引中的 B+ 树找到对应的叶子节点，然后获取需要查询的数据
>
> * 以上的过程叫做 **回表**
>
> * ```sql
>   // name 作为二级索引字段
>   select * from t_user where name = "林某"
>   ```

* 如果我们使用二级索引，同时需要查找的数据是在二级索引的叶子节点中，那么可以直接查找这个二级索引的 B+ 树，这叫 **覆盖索引**, 比如我们只需要找索引值 + 主键值，那么使用二级索引就够了

```sql
// name 作为二级索引
select id from t_user where name = "林某"
```







* 索引存储结构长什么样？

> * InnoDB 存储引擎根据类型的不同，分成 **聚簇索引** 和 **二级索引** ，区别在于： 聚簇索引的叶子节点存放的是实际数据，所有的完整用户数据都存放在聚簇节点的叶子节点中，但是二级索引的**叶子节点存放的是主键值**，而不是实际数据。
>
> * ```sql
>   // id 字段是主键，所以是主键索引
>   select * from t_user where id = 1;
>   ```
>
>   ```sql
>   // name 字段是二级索引，我们会找到的是主键，然后主键再去聚簇索引中找数据
>   select * from t_user where name = "林某"
>   ```



* 对索引使用左或者左右模糊匹配

```sql
eg: like %xx 或者 like %xx%
// name 字段是二级索引，
select * from t_user where name like '%林' // 会出现错误
select * from t_user where name like '林%' // 右模式匹配，不会出现错误
```

> 原因是 B+ 树是按照索引值进行有序排序的，所以可以根据前缀进行比较的。如果左模糊就不行



* 对索引使用函数

```sql
// name 是二级索引
select * from t_user where length(name) = 6;// 如果我们查询条件中对索引字段使用函数，就会导致索引失效。
```

> * 原因是： 因为我们的索引保存的是索引的原始值，而不是经过函数计算后的值，自然就没法走索引这条路了。
>
> * MySQL 8.0 开始，增加了函数索引，就是可以根据函数计算后的值建立一个索引，就是说这个索引的值是函数计算之前的索引的值，这样就可以根据索引来查询数据



* 对索引进行表达式计算

```sql
select * from t_user where id + 1 = 10
```

> * mysql 不支持表达式的计算，因为索引字段保存的是原始字段，而不是 id + 1 表达式计算后的值，所以没法走索引，只能全表查询所有的索引，然后依次进行表达式的计算求出符合结果
>
> * MySQL 没有简单计算出来，可能是因为如果都需要计算出来那么就非常的复杂，代码非常的臃肿





* 对索引隐式类型转换

> * mysql 会自动将字符串转成数字，然后进行比较，但是不能将数字转换成字符串，所以当需要将数字转成字符串的时候会进行隐式的转换这是不行的，会造成索引失效，扫描全表。

```sql
eg : // phone 是 varchar 型，不能将数字转换成字符串，所以下面是不行的
select * from t_user where phone = 1300000001
select * from t_user where CAST(phone AS signed int) = 13000000001
// 上面这个语句也是不行的，因为上面这个语句在索引部分使用了函数，对索引使用函数是不行的，所以需要全表扫描

// 当使用字符串转换成数字的时候是可以的
select * from t_user where id = "1"
// 等价下面这条语句：
select * from t_user where id = CAST("1" AS signed int) // 注意我们并没有对索引使用函数，我们是对输入参数使用了函数，所以可以走索引扫描的。
```





* 联合索引非最左匹配

> * 多个普通字段组合在一起创建的索引就是联合索引，也叫组合索引
>
> * ```sql
>   // 创建一个组合索引：(a, b, c),如果查询是一下几种才是匹配上联合索引
>   where a = 1
>   where a = 1 and b = 2and c = 3
>   where a = 1 and b = 2
>   // 注意 a 只要存在就行，在 where 子句的顺序不重要
>   where b = 2
>   where c = 3
>   where b = 2 and c = 3
>   // 以上不是满足最左匹配
>   ```
>
> * 为什么会这样？
>
>   > * 原因是：在联合索引的情况下，**按照的是索引的第一列排序，第一列数据相同才会按照第二列进行排序**
>   >* 也就是说，如果我们想使用联合索引中的尽可能多的列，查询条件中的各个列必须是联合索引中最左边开始的连续的列，如果是第二列搜索，一定是无法走索引的。
>   
> * where a = 1 and c = 0 语句在 mysql 5.6 之后会采用索引下推的功能：目的是减少回表的次数.
>
> * 方法是：在进行索引的时候对索引中的字段先进行判断，直接过滤掉不满足条件的记录，减少回表操作。





* WHERE 子句中的 OR 

> * 如果早 OR 前的条件列是索引列，但是在 OR 后的条件列不是索引列，那么就会出现索引失效，后面的非索引列就需要走全表扫描了。
>
> * 原因是：OR 的含义是两个中只要满足一个就可以了，因此只有一个索引列是没有任何含义的，只要存在条件列而不是索引列，就会进行全表的扫描操作
>
> * 解决办法：将条件列设置成索引列就可以了。
>
> * ```sql
>   // age 不是索引列是条件列
>   select * from t_user where id = 1 or age = 18
>   // 这样就会扫描全表
>   ```



#### 左或左右模糊匹配一定失效吗？ like ''%x''

* 如果一个表有多个字段，其中 name  是索引字段，存在其他的非索引字段，id 为自增的主键索引，那么一定会失效，会扫描全表，因为必然需要对 聚簇索引 B+ 树进行全部的扫描 type = all,索引失效
* 如果一个表有两个字段，其中 name 是普通的索引字段(二级索引字段)，不存在其他的非索引字段，id 为自增的主键索引，**那么不会出现全表扫描**，只需要扫描二级索引的 B+ 树就可以了，因为二级索引表中包含了所有我们需要的数据 name + id 这个就是我们所需要的，type = index 表示需要扫描全部的二级索引表，**遍历整个索引树**







#### 事务隔离级别是如何实现的

* 事务**隔离性是隔离事务与事务之间的**，而不是隔离同一个事务自己

* 在做一个操作之前例如：转账之前我们需要先开启一个事务，等所有的数据库操作执行完成后，我们才提交事务，这样就可以保证转账业务在所有的数据库的操作是不可分割的了，要么全部执行要么全部失败，不可以出现中间状态

* 事务有哪些特性？

> * 原子性：事务的执行不会有中间环节，如果事务执行过程发生错误，会被回滚到事务开始前的状态，就好像这个事务完全没有发生一样。
> * 一致性：例如：表中有一个字段，它有一个约束，就是表中的姓名不能重复，但是如果一个事务对姓名字段进行修改，在事务提交的时候，表中的姓名就变得非唯一性了，这就破坏了事务的一致性要求，那么数据库就得撤销这个事务，返回其初始的状态。（**事务执行前后整个数据库的状态保持一致**）
> * 隔离性：数据库支持多个**并发的事务对其数据进行读写和修改的能力**，隔离是为了防止交叉执行导致的数据的不一致
> * 持久性：事务处理结束之后，对数据的修改是永久的，即使系统故障也不会丢失



* Innodb 如何保证事务的？

> * 首先REDO LOG是用来对已提交的事务做恢复，UNDO LOG 是为了对未提交的事务做回滚。把脏页刷入到磁盘中，这个就需要使用 undo log 进行相应的操作.
> * 持久性是通过： redo log （重做日志）来保证的
> * 原子性 和一致性是通过 (undo log) 回滚日志
> * 隔离性是通过 MVCC (多版本并发控制) 或锁机制来实现的.

* 并行事务会引发什么问题？

> * Mysql 的服务端可以和多个客户端连接，从而就可能同时处理多个事务的问题
>
> * 引发的问题：**脏读、不可重复读、幻读**等问题
>
> * 脏读：
>
>   > * 如果事务读到了另外一个**未提交事务修改过的数据**，就发生了脏读的现象
>   > * 事务 A 的操作还没有提交所以事务A可能出现回滚的情况，这个时候事务 B 读取的数据可能就是一个过期的数据。
>   > * ![图片](https://img-blog.csdnimg.cn/img_convert/10b513008ea35ee880c592a88adcb12f.png)
>
> * 不可重复读：
>
>   > * 事务 A 在当前这个事务中进行了**两次或者多次的数据读操作，但是这两次的读取结果不一样**(可能是 事务 B对数据进行修改操作，从而导致前两次读取的结果不一致），这就意味着发生了不可重复读现象.
>   > * ![图片](https://img-blog.csdnimg.cn/img_convert/f5b4f8f0c0adcf044b34c1f300a95abf.png)
>
> * 幻读：
>
>   > * 在一个事务中多次查询某个符合查询条件的**记录数量**，如果出现前后两次查询到的**记录数量不一样**(可能在这个过程执行插入操作）的情况称为幻读。（注意第二次读取数据时事务A已经提交事务了）（注意和不可重复读的区别，不可重复读是读取的具体的数据）
>   > * 例如：事务A对一个数据进行读取，之后进行修改然后提交，在提交前后事务B对数据进行两次读取，事务B 前后两次读取到数据不同，出现幻读情况。
>   > * ![图片](https://img-blog.csdnimg.cn/img_convert/d19a1019dc35dfe8cfe7fbff8cd97e31.png)





* 事务的隔离级别有哪些？

> * 为了解决并发事务出现的问题，提出隔离级别的概念。
>
> * 并发事务问题的严重性： 脏读 > 不可重复读 > 幻读
>
> * 四种隔离级别：
>
>   > * 读未提交：指一个事务还没有被提交时，它做的变更就可以被别的事务看见
>   > * 读提交： 指一个事务提交之后，它做的变更才能被其他事务看见
>   > * 可重复读：一个事务执行过程中看到的**数据**，一直跟这个事务启动的时候看到的数据是一致的，**MySQL Innodb 引擎的默认隔离级别**
>   > * 串行化：隔离级别最早， 会对记录加上读写锁，在多个事务对这个记录进行读写操作的时候，如果发生了读写冲突，后访问的事务必须等前一个事务完成才能继续执行。
>
> * 隔离水平高低排序： 串行化 > 可重复读 > 读已提交 > 读未提交
>
> * ![图片](https://img-blog.csdnimg.cn/img_convert/4e98ea2e60923b969790898565b4d643.png)
>
> * 上图表示的是读未提交可能出现的问题，读已提交可能出现的问题，可重复读可能出现的问题
>
> * 所以要解决脏读需要将隔离级别提到读已提交，要解决不可重复读需要将隔离级别提高到可重复读。
>
> * 要解决幻读，并不需要将隔离级别提高到串行化，因为可能导致并发性能比较差。
>
> * Innodb 解决幻读的方法： 使用next-key lock锁（行锁和间隙锁组合）来锁住记录之间的间隙和记录本身，**防止其他事务在这个记录之间插入新的记录**，避免幻读，**读取的数量不同**



* 四种隔离级别是如何实现的？

> * 读未提交：直接读取就完事了。
> * 串行化：通过加读写锁的方式实现避免并行化访问
> * 读提交：使用 Read view 实现的，在每一次 select 都会生成一个新的 read view 这就意味着，一个事务在多次读取一个数据的时候，前后两次结果不一样，因为这一期间另外一个事务修改了该记录，同时提交了事务
> * 可重复读： 启动事务的时候生成一个 Read view 之后一直使用这个 Read view 所以整个事务期间读取到的数据都是启动事务前的那个数据.



* 什么是 Read view :

  > * read view 可以理解成一个数据快照，定格数据的某一个瞬间，可以理解成版本数据.
  > * **启动事务后，在执行第一个查询语句操作后**，产生这个 read view
  > * ![图片](https://img-blog.csdnimg.cn/img_convert/11a65cbc2e97f6855d7692a265dc2651.png)
  > * ![图片](https://img-blog.csdnimg.cn/img_convert/f595d13450878acd04affa82731f76c5.png)
  > * 上图是聚簇索引中的两个隐藏的列：
  > * trx_id: 当一个事务对某条聚簇索引记录进行改动的时候，就会将当前的事务 id 记录在 trx_id 隐藏列中
  > * roll_pointer: 这个就是 undo 日志，当我们对每条聚簇索引记录进行改动的时候，都会把旧版本的记录写到 undo 日志中，然后通过这个隐藏列的指针找到这个就的记录，因为这个指针就是指向旧版本的记录。

  

  

  



* 可重复读隔离级别如何实现？

> * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/%E4%BA%8B%E5%8A%A1ab%E7%9A%84%E8%A7%86%E5%9B%BE2.png)
> * 事务 B 第一次读小林的账户余额记录，在找到记录后，它会先看这条记录的 trx_id，此时**发现 trx_id 为 50，比事务 B 的 Read View 中的 min_trx_id 值（51）还小，这意味着修改这条记录的事务早就在事务 B 启动前提交过了，所以该版本的记录对事务 B 可见的**，也就是事务 B 可以获取到这条记录。
> * 事务A 执行了修改操作， 这时候事务 B 去读取数据， **发现这条记录的 trx_id 值为 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，则需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录**，所以事务 B 能读取到的是 trx_id 为 50 的记录
>
> * 这样事务B在可重复读的情况下读取到的数据都是事务启动前的记录。
> * **这种通过记录的版本链来控制并发事务的访问，来访问同一个记录的行为，叫做 MVCC （多版本并发控制）**







* 读提交隔离级别如何实现的？

> * 读提交是在**每一次 select 都会产生一个新的 read view** ，这就意味着：当事务期间读取到同一条数据，前后两次数据可能出现不一致，因为可能这一期间另外一个事务修改了记录，同时提交了事务。
> * ![图片](https://img-blog.csdnimg.cn/img_convert/f9f086118d522d018719244d89efa639.png)
> * 事务A读取数据，同时发现这个数据源的 trx_id 要比 自己的 creator_trx_id 大，**同时不在 m_ids 中**，那么就意味着该记录已经提交了，那么事务A就可以读取这条数据了。（**只要保证这个数据是已近提交的就可以了，因为我们解决了脏读，但是没法解决重复读**）.







#### 幻读解决方案

* **innodb 不会对select、insert、delete、update语句加表锁的。**

* 在执行 update 语句的时候，**是会加上 next-key lock**(在原始 MVCC 基础上避免幻读问题) ，当事务结束之后，才会释放锁，期间是不能执行新的插入操作.

* **加锁的位置准确的说，锁是加在索引上的而非行上。**

* Q : 那 update 语句的 where 带上索引就能避免全表记录加锁了吗？

* A : **关键还得看这条语句在执行过程种，优化器最终选择的是索引扫描，还是全表扫描，如果走了全表扫描，就会对全表的记录加锁了**。

* 使用next-key lock锁（行锁和间隙锁组合）来锁住记录之间的间隙和记录本身，防止其他事务在这个记录之间插入新的记录，避免幻读，读取的记录数量不同

* 在可重复读的隔离级别下，普通的查询快照，是看不到别的事务的插入的数据的。

* Mysql  里除了普通的查询是快照，其他都是 **当前读**， 比如 update insert delete 这些语句执行前都会查询最新的版本，然后再做进一步的操作。因为 eg : update 数据，另外一个事务删除了记录并且提交了事务，这样再 update 就没法知道最新的数据了。

* select ... fro update 每次读取的都是最新的数据。

* 所以讨论可重复读隔离级别下的幻读，需要建立在 **当前读** 的情况下。

* **Innodb 引擎为了解决「可重复读」隔离级别使用「当前读」而造成的幻读问题，就引出了 next-key 锁**，就是记录锁和间隙锁的组合。

* 记录锁：锁的是记录本身

* 间隙锁：锁的是记录之间的间隙，以防止其他事务在两个间隙之间插入新的数据，从而避免幻读。

* ![img](https://img-blog.csdnimg.cn/3af285a8e70f4d4198318057eb955520.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)

* 上图就是事务在进行当前读状态，事务B想对这个锁住的 id > 2 这个范围插入数据，不行需要锁住，避免被插入数据.

* next-key lock 锁的是**索引**，而不是这个数据本身或者是行上，所以如果 update 语句的 where 条件没有用到索引列，那么就会全表扫描，在一行一行扫描的时候，不仅加了行锁，同时给行两边的空隙也加上了间隙锁，相当于锁住了整个表，直到这个事务结束才会释放锁。

* 执行 update 语句的时候，加的是独占锁，这个锁并不是执行完 update 这个语句就释放了，而是等到事务结束才释放.

* 如果 update 语句的 where 使用的是唯一的索引，那么锁住这个索引，等于锁住这一行的数据。

* 在 update 语句的 where 条件没有使用 索引的时候，就会全表扫描，于是就会对所有的记录加上 next-key 锁，相当于把全表锁住了。

* 在 update 的时候，或者说在当前读的时候，只要进行全表扫描了，那么就会对全表记录加锁了.

* 解决方案：

  > * 将 MySQL 中的参数 sql_safe_update 参数设置成1 ，开启安全更新模式，不符合条件的 没法执行 update 语句.
  >
  > * update 语句必须满足如下条件之一才能执行成功：
  >
  >   - 使用 where，并且 where 条件中必须有索引列；
  >   - 使用 limit；
  >   - 同时使用 where 和 limit，此时 where 条件中可以没有索引列；
  >
  >   - delete 语句必须满足如下条件之一才能执行成功：
  >
  >   - 使用 where，并且 where 条件中必须有索引列；
  >   - 同时使用 where 和 limit，此时 where 条件中可以没有索引列；







#### 数据库如何处理索引优化的？

* mysql **估计使用索引比使用全表扫描还要慢**，那么不会走索引。

* 最左模糊匹配不能走索引，需要全表查询。以及其他导致索引失效的方式都应该避免，才能做到优化操作

* Union or in 都能命中索引，那么使用 in 进行相应的操作。

* 负向条件查询不能命中索引，可以优化成 in 查询，负向条件查询方式：!=、<>、not in、not exists、not like等

* 利用**覆盖索引进行查询，避免进行回表操作。**

	>  因为status字段是索引列，所以直接从索引中就可以获取值，不必回表查询
	>
	> ```sql
	> EXPLAIN SELECT * FROM user where status=1;
	> ```
	>
	> ![img](https://pic2.zhimg.com/80/v2-711c2fa8f55b47123d2304a5410bbb39_1440w.png)

* 





#### Redo Log 是如何做的

* redo log 可以解决原子性以及持久性。

* 数据发生修改了，这个时候 Buffer pool 来不及写回到磁盘中，数据库重启的时候，可以从 redo log 中读取修改的数据, 重新再做一遍同样的操作(redo), 这样可以保证 BUffer pool 中丢失的数据还可以找回来

* 为什么 redo log 可以能及时写入到磁盘？

	> * redo log 也是需要写入到磁盘的，但是与 buffer pool **随机的写入**到磁盘相比，redo log 采用的是 顺序写入，而不是随机的写入，速度更快
	> * 引入 redo log buffer 是一组一组的写，不是一条一条的写。**按组**
	> * 以上两点提高了磁盘的 IO 效率

* redo log buffer 的刷盘，写入到磁盘：

	> * 刷盘时机：
	> * redo log buffer 空间不足的时候
	> * 事务提交的时候
	> * 后台线程在不停的刷
	> * 服务正常的关闭
	> * checkpoint  的时候
	> * 从刷盘时机来看，redo log buffer的刷盘时机更为频繁，尤其是`事务提交的时候`这一条保证了提交的事务redo log 就已经存储到了磁盘中。
	> * innodb_flush_log_at_trx_commit 取值范围 0，1，2
	> * 默认取 1 ，表示的是只要存在事务提交，就讲数据存储到磁盘中.**保证了提交的事务数据不会丢失。**
	> * 取 2 表示 将每次提交事务，将 redo log 写入操作系统的缓冲区，没有真正的写到磁盘，那么当数据库崩了，os 没崩，数据可恢复，两个都崩了，数据不能恢复
	> * 取 1 表示： 每次提交事务不做处理，由后台去刷新磁盘。
	> * redo log buffer 向磁盘刷入的时机更为频繁。可以通过`innodb_flush_log_at_trx_commit`控制事务提交是否刷盘
	> * redo log buffer 存储是有限的，这个时候可以使用checkpoint 标记，哪些数据可以进行覆盖操作。



#### MySQL 日志： undo log    redo log    binlog

```sql
UPDATE t_user SET name = 'xiaolin' WHERE id = 1;
```

* 执行过程

* > - 客户端先通过连接器建立连接，连接器自会判断用户身份；
  >
  > - 因为这是一条 update 语句，所以不需要经过查询缓存，但是表上有更新语句，是会把整个表的查询缓存情空的，所以说查询缓存很鸡肋，在 MySQL 8.0 就被移除这个功能了；
  >
  > - 解析器会通过词法分析识别出关键字 update，表名等等，构建出语法树，接着还会做语法分析，判断输入的语句是否符合 MySQL 语法；
  >
  > - **预处理器**会判断表和字段是否存在；
  >
  > - 优化器确定执行计划，因为 where 条件中的 id 是主键索引，所以决定要使用 id 这个索引；
  >
  > - 执行器负责具体执行，找到这一行，然后更新。具体流程： 
  >
  > - > * 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录：
  >   >
  >   > * > - 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；
  >   >   > - 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。
  >   >
  >   > * 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：
  >   >
  >   > * > - 如果**一样的话就不进行**后续更新流程；
  >   >   > - 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；
  >   >
  >   > * **开启事务**， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在修改该 Undo 页面前需要先记录对应的 redo log，所以**先记录修改 Undo 页面的 redo log ，然后再真正的修改 Undo 页面**。
  >   >
  >   > * InnoDB 层开始更新记录，根据 WAL 技术，**先记录修改数据页面的 redo log ，然后再真正的修改数据页面**。修**改数据页面的过程是修改 Buffer Pool 中数据所在的页**，然后将**其页设置为脏页**，为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。
  >   >
  >   > * 至此，一条记录更新完了。
  >   >
  >   > * 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在**事务提交时**才会统一将该**事务运行过程中的所有 binlog 刷新到硬盘**。
  >   >
  >   > * 事务提交，剩下的就是「两阶段提交」的事情了



##### Undo log(回滚日志)

######  **用于撤销回退的日志**

* (eg : 就是在执行一个插入操作之后，崩溃了之后只需要从日志中找到这条数据删除就好了)

* 定义： 是 Innodb 存储引擎层生成的日志，实现了事务中的**原子性**，主要**用于事务回滚和 MVCC**。

* 我们在执行执行一条“增删改”语句的时候，虽然没有输入 begin 开启事务和 commit 提交事务，但是 MySQL 会**隐式开启事务**来执行“增删改”语句的，执行完就自动提交事务的。（MySQL 增删改默认开启事务的）

* Q : 一个事务在执行过程中，在**还没有提交事务之前**，如果MySQL 发生了崩溃，要怎么回滚到事务之前的数据呢？

* A: 如果我们每次在事务执行过程中，都记录下回滚时需要的信息到一个**日志里**，那么在事务执行中途发生了 MySQL 崩溃后，就不用担心无法回滚到事务之前的数据，我们可以通过这个日志回滚到事务之前的数据。(将需要回滚的未提交的数据记录到日志中)

* ![回滚事务](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E5%9B%9E%E6%BB%9A%E4%BA%8B%E5%8A%A1.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

  

* eg : 在**插入**一条记录时，要把这条记录的主键值记下来，这样之后回滚时只需要把这个主键值对应的记录**删掉**就好了；(做与原来相反的操作)

* 日志里面记录了执行的命令，所以可以很好的执行相反的操作

###### ReadView + undo log 实现 MVCC

* 对于「读提交」（脏读）和「可重复读」（不可重复读）隔离级别的事务来说， 关键区别在于： **创建 Read View 的时机不同**

* > - 「读提交」隔离级别是在**每个 select 都会生成一个新的 Read View**，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。
  > - 「可重复读」隔离级别是**启动事务时生成一个 Read View**，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。

* Undo log 的作用：

* > - **实现事务回滚，保障事务的原子性**。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。
  > - **实现 MVCC（多版本并发控制）关键因素之一**。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。



###### Buffer pool

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/bufferpool%E5%86%85%E5%AE%B9.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

* Buffer Pool 除了缓存「索引页」和「数据页」，还包括了 Undo 页，插入缓存、自适应哈希索引、锁信息...
* Q： 查询一条记录，就只需要缓冲一条记录吗？
* A： 缓存中不存在这条记录，一定是加载整页的数据，进入内存中.(InnoDB 里的 B+ 树中的**每个节点都是一个数据页**)
* 



##### Redo log(重做日志)

* 是 Innodb 存储引擎层生成的日志，实现了事务中的**持久性**，主要**用于掉电等故障恢复**；

* Redo log 什么时候提交： 在**事务提交时**，只要先将 redo log 持久化到磁盘即可， 之后抽空通过后台线程将 buffer pool 中的数据持久化到磁盘中

* 为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会**先**把记录写到 redo log 里面，并更新内存，**这个时候更新就算完成了**。同时，InnoDB 引擎会在适当的时候，由后**台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里**（不是redo log 日志）

* **WAL （Write-Ahead Logging）技术**，**指的是 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上**。

* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/wal.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0" alt="img" style="zoom:90%;" />

* Q: 什么是 redo log？

* A: redo log 是物理日志，记录了某个数据页做了什么修改，对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条物理日志。

* Q: redo log 和 undo log 区别在哪？

* > - redo log 记录了此次事务**「完成后」**的数据状态，记录的是更新之**「后」**的值；
  > - undo log 记录了此次事务**「开始前」**的数据状态，记录的是更新之**「前」**的值；

* 事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务:

* ![事务恢复](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%BA%8B%E5%8A%A1%E6%81%A2%E5%A4%8D.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

* Q : redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？

* > * 写入 redo log 的方式使用了追加操作， 所以磁盘操作是**顺序写**，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是**随机写**。
  > * 磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。
  > * **MySQL 的写操作从磁盘的「随机写」变成了「顺序写」**，提升语句的执行性能。这是因为 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上 (后台写线程）。

* Q： 产生的 redo log 是直接写入磁盘的吗？

* > * 不是的，先**写到 redo log buffer 中**，之后再持久化到磁盘中
  > * 产生的 redo log 也不是直接写入磁盘的，因为这样**会产生大量的 I/O 操作**，而且磁盘的运行速度远慢于内存。

###### Redo log 什么时候刷盘

* redo log buffer 在内存中,刷盘时机很重要

* 几个可能的时机：

* > - MySQL 正常关闭时；
  > - 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘；
  > - **InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。**
  > - 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘 （主要方式，通过控制参数： innodb_flush_log_at_trx_commit 实现）

* Q : Redo log 文件写满怎么办？

* A： ![重做日志文件组写入过程](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E9%87%8D%E5%81%9A%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E7%BB%84%E5%86%99%E5%85%A5%E8%BF%87%E7%A8%8B.drawio.png)

* 重做日志文件组是以**循环写**的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。











##### Binlog(归档日志)

* 是 **Server 层**生成的日志，主要**用于数据备份和主从复制**；
* MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的**所有 binlog 统一写 入 binlog 文件**。
* binlog 文件是记录了所有数据库表**结构变更和表数据修改**的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。
* Q: **为什么有了 binlog， 还要有 redo log**？
* A: 这个问题跟 MySQL 的时间线有关系。[redo log 需要的原因](https://xiaolincoding.com/mysql/log/how_update.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-binlog)



###### Redo log 和 binlog 的区别

1. 适用对象不同：

- binlog 是 MySQL 的 Server 层实现的日志，**所有存储引擎都可以使用**；
- redo log 是 Innodb 存储引擎实现的日志；

2. 文件格式不同

* binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED
* redo log 是**物理日志，记录的是在某个数据页做了什么修改**，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；

3. 写入方式不同

* binlog 是**追加写**，写满一个文件，就**创建一个新的文件继续写**，不会覆盖以前的日志，保存的是**全量**的日志。
* redo log 是**循环写**，日志空间大小是固定，**全部写满就从头开始**，保存未被刷入磁盘的脏页日志。

4. 用途不同

* binlog 用于**备份恢复、主从复制**；
* redo log 用于掉电等故障恢复。





###### 主从复制如何实现

* MySQL 的主从复制依赖于 **binlog** ，也就是**记录 MySQL 上的所有变化**并**以二进制形式保存在磁盘上**。

* 复制的过程就是将 binlog 中的数据从**主库传输到从库**上。

* ![MySQL 主从复制过程](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E8%BF%87%E7%A8%8B.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

* **异步执行**上面的操作,主库不会去等待复制 binlog 线程同步完成.

* MySQL 集群的主从复制过程梳理成 3 个阶段：

* > - **写入 Binlog**：主库写 binlog 日志，**提交事务**，并更新本地存储数据。
  > - **同步 Binlog**：把 binlog **复制到所有从库上**，每个从库把 binlog 写到暂存日志中。
  > - **回放 Binlog**：回放 binlog，并更新存储引擎中的数据。最终实现数据的一致性

* Q : MySQL 主从复制还有哪些模型？

* > - **同步复制**：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：**一是性能很差，因为要复制到所有节点才返回响应**；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。
  > - **异步复制**（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。（性能比较好，有比较好的可用性）
  > - **半同步复制**：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要**一部分复制成功**响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种**半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险**。



###### binlog 什么时候落盘

* 事务执行过程中，**先把日志写到 binlog cache**（Server 层的 cache），事务提交的时候，**再把 binlog cache 写到 binlog 文件**中。
* ![binlog cach](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/binlogcache.drawio.png)
* 虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件：
  - 图中的 write，指的就是指把**日志写入到 binlog 文件**，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里(在**内核里面**），**write 的写入速度还是比较快的，因为不涉及磁盘 I/O。**
  - 图中的 fsync，才是将数据持久化到磁盘的操作（和 Redis 的 AOF 持久化到磁盘的方式类似），这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。



* Q: 从库越多越好吗？

* A : 不是的

* > * 因为从库数量增加，从库连接上来的 I/O 线程也比较多，**主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽**。
  > * 一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是**一主多从**的 MySQL 集群结构。



##### 什么是两阶段提交





















#### 执行一条 select 语句，期间发生了什么?

![查询语句执行流程](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/mysql%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B.png)

* **Server 层负责建立连接、分析和执行 SQL**。
* **存储引擎层负责数据的存储和提取**。



##### 连接器**注意：**

* 执行 mysql 连接部分的
* MySQL 长连接： 和 HTTP 的长连接是一个意思
* Q： 怎么解决长连接占用内存的问题？
* A：
* 1. 第一种，**定期断开长连接**。既然断开连接后就会释放连接占用的内存资源，那么我们可以定期断开长连接。
* 2. 第二种，**客户端主动重置连接**。释放内存，这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。



##### 查询缓存

* 如果 SQL 是查询语句（select 语句），MySQL 就会先去查询缓存（ Query Cache ）里查找缓存数据，看看之前有没有执行过这一条命令，这个查询缓存是以 key-value 形式保存在内存中的，**key 为 SQL 查询语句，value 为 SQL 语句查询的结果**。
* **注意：** 这个缓存中存储的是之前 SQL 查询出来的数据，key 是直接的 **SQL 语句**，但是 value 是查询的结果，（这命中概率得多低啊......）
* MYSQL 8.0 已删除



##### 解析 SQL

###### 解析器

* 执行内容：

* > * **词法分析**: MySQL 会根据你输入的字符串识别出关键字出来，构建出 SQL 语法树，这样方面后面模块获取 SQL 类型、表名、字段名、 where 条件等等 （单词提取）
  > * **语法分析**: 词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。(语法检查)





#### 执行 SQL

经过解析器后，接着就要进入**执行 SQL 查询语句的流程**了，每条`SELECT` 查询语句流程主要可以分为下面这三个阶段：

- prepare 阶段，也就是**预处理阶段**；
- optimize 阶段，也就是**优化阶段**；
- execute 阶段，也就是**执行阶段**；

###### 预处理器

* 工作内容：

* > - 检查 SQL 查询语句中的**表或者字段**是否存在；
  > - 将 `select *` 中的 `*` 符号，扩展为表上的所有列；

*  MySQL 5.7 判断**表或字段是否存在的工作**，是在**词法分析&语法分析之后**，**prepare 阶段之前做的**





###### 优化器

* 目标： **优化器主要负责将 SQL 查询语句的执行方案确定下来**，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。
* 查询语句是**覆盖索引**，直接在**二级索引就能查找到结果**（因为二级索引的 B+ 树的叶子节点的数据存储的是主键值），就没必要在主键索引查找了，因为**查询主键索引的 B+ 树的成本会比查询二级索引的 B+ 的成本大**，优化器基于查询成本的考虑，会选择查询代价小的普通索引。
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/%E9%80%89%E6%8B%A9%E7%B4%A2%E5%BC%95.png)





###### 执行器

* 在执行的过程中，执行器就会和存储引擎交互了，交互是以记录为单位的。

* 交互的过程： 

* > - 主键索引查询 : 使用主键作为索引进行查询操作
  > - 全表扫描 : 没有使用任何的索引机制，所以执行的是全表扫描
  > - 索引下推

* 索引下推：

* > * 索引下推能够减少**二级索引**在查询时的回表操作，提高查询的效率，因为它将 Server 层部分负责的事情，交给存储引擎层去处理了。
  >
  > * 举一个具体的例子，方便大家理解，这里一张用户表如下，我对 age 和 reword 字段建立了联合索引（age，reword）：
  >
  > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/%E8%B7%AF%E9%A3%9E%E8%A1%A8.png)
  >
  > * ```sql
  >   select * from t_user  where age > 20 and reward = 100000;
  >   ```
  >
  > * **联合索引当遇到范围查询** (>、<、between、like) 就会停止匹配，也就是 **a 字段能用到联合索引，但是 reward 字段则无法利用到索引**
  >
  > * **不使用索引下推**的执行流程是：
  >
  > * > - Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age > 20 的第一条记录(eg: age = 21)；
  >   > - 存储引起根据二级索引的 B+ 树快速定位到这条记录后，获取主键值，然后**进行回表操作**，将完整的记录返回给 Server 层；
  >   > - **Server 层在判断该记录的 reward 是否等于 100000**( reward 字段没法走索引操作），如果成立则将其发送给客户端；否则跳过该记录；(就是每一次都需要 server 判断一下 reward 是否等于 100000)
  >   > - 接着，继续向存储引擎索要下一条记录(eg : age = 23)，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给 Server 层；
  >   > - 如此往复，直到存储引擎把表中的所有记录读完。
  >
  > * 使用 **索引下推功能** 之后：判断记录的 **reward 是否等于 100000 的工作交给了存储引擎层**
  >
  > * > - Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age > 20 的第一条记录；
  >   > - 存储引擎定位到二级索引后，**先不执行回表**操作，而是先判断一下该索引中包含的列（reward列）的条件（reward 是否等于 100000）是否成立。如果**条件不成立**，则直接**跳过该二级索引**（就是刚才的 age 索引记录，查下一条 age 记录）。如果**成立**，则**执行回表**操作，将完成记录返回给 Server 层。
  >   > - Server 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。
  >   > - 如此往复，直到存储引擎把表中的所有记录读完。
  >
  > * 但是因为它包含在联合索引（age，reward）里，所以直接在存储引擎过滤出满足 reward = 100000 的记录后，才去执行回表操作获取整个记录使用 索引下推， **reward 列还是无法使用联合索引**。



















####  数据库为什么不使用 hashmap 作为索引

* 本质来说 hashmap 适合单个数据通过 hash 表的查找，当遇到需要范围查找的时候，排序时的 hashmap 处理起来非常的麻烦，因为数据是无序的且是 hash 分散的。
* 当数据量增加的时候，还可能导致 hash 冲突，大量重复的键值的情况，hash 的效率是极低的。
* 键值不是唯一的，需要先找到键值的位置，然后使用根据链表往后扫描，直到找到最终的数据
* hashmap 不能采用范围查找操作。因此无法做到模糊匹配的操作
* B+ 树的索引结构对关键字的检索效率是比较平均的。
* 等值查询 hash 索引具有明显的优势，只需要计算一次就可以查到具体需要的数据。



#### InnoDB & Myisam

##### 索引

* MyISAM引擎使用B+树作为索引结果，叶节点的data域存放的是数据记录的地址。下图为MyISAM表的主索引，Col1为主键。

* ![img](https://images2017.cnblogs.com/blog/679616/201801/679616-20180115165534912-4452270.png)

* 同样是B+树，实现方式却完全不同。InnoDB表数据文件本身就是一个索引结构，树的叶节点data域保存了完整的数据记录，这种索引叫做**聚集索引**。

* ![img](https://images2017.cnblogs.com/blog/679616/201801/679616-20180115170146521-491912290.png)

* 索引： 

* InnoDB的数据文件本身就是主索引文件。而MyISAM的主索引和数据是分开的。

  InnoDB的辅助索引data域存储相应记录主键的值而不是地址。而MyISAM的辅助索引和主索引没有多大区别。

  innoDB是聚簇索引，数据挂在主键索引之下。

* 锁：

* MyISAM使用的是**表锁**  InnoDB使用**行锁**

* 事务：

* MyISAM没有事务支持和MVCC

  InnoDB支持事务和MVCC

* 主键

* MyISAM允许**没有任何索引和主键的表存在**，索引都是保存行的地址

  InnoDB如果没有设定主键或非空唯一索引，就会自动生成一个6字节的主键，数据是主索引的一部分，**二级索引保存的是主索引的值**







#### MySQL 除了索引还有什么影响性能

* 平常写的应用代码（go或C++之类的），这时候就叫**客户端**了。客户端底层会带着账号密码，尝试向mysql**建立一条TCP长链接**。
* 客户端会将sql语句通过网络连接给mysql。
* mysql收到sql语句后，会在**分析器**中先判断下SQL语句有没有语法错误，比如select，如果少打一个`l`，写成`slect`，则会报错`You have an error in your SQL syntax;`。这个报错对于我这样的手残党来说可以说是很熟悉了。
* 接下来是**优化器**，在这里会**根据一定的规则选择该用什么索引**。
* 之后，才是通过**执行器**去调用**存储引擎**的接口函数
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianbibkNQ7b3dWIYFDypVnpK30llfbC5U0KXGEicjeVnLPkiclPicZGzc6G1MsGSrRbSaXBqAbwvTfTtuQ/640?wxfrom=5&wx_lazy=1&wx_co=1)
* **存储引擎**类似于一个个组件，它们才是mysql真正获取一行行数据并返回数据的地方，存储引擎是可以替换更改的，既可以用**不支持事务的MyISAM**，也可以替换成支持事务的Innodb。这个可以在建表的时候指定。
* InnoDB中，因为直接操作磁盘会比较慢，所以加了一层内存提提速，叫**buffer pool**，这里面，放了很多内存页，每一页16KB，**有些内存页放的是数据库表里看到的那种一行行的数据，有些则是放的索引信息**。
* 查询SQL到了InnoDB中。会根据前面**优化器里计算得到的索引**，去**查询相应的索引页**，如果不在buffer pool里则从磁盘里加载索引页。**再通过索引页加速查询，得到数据页**的具体位置。
* **![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianbibkNQ7b3dWIYFDypVnpK3j17LhKgzqicwfKNrQjPff37PPhlo4yfR3zvhc2YAO6hgwtyUtG8OYuA/640?wxfrom=5&wx_lazy=1&wx_co=1)**



##### 索引相关的影响

* 一般主要考虑几个因素，比如：

* > * 选择这个索引大概要扫描**多少行**（rows）
  > * 为了把这些行取出来，需要读**多少个16kb的页**
  > * 走**普通索引需要回表**，主键索引则不需要，**回表成本**大不大？

* ```sql
  mysql> explain select * from users where id = 1;
  +----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
  | id | select_type | table | partitions | type  | possible_keys | key     | key_len | ref   | rows | filtered | Extra |
  +----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
  |  1 | SIMPLE      | users | NULL       | const | PRIMARY       | PRIMARY | 8       | const |    1 |   100.00 | NULL  |
  +----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
  1 row in set, 1 warning (0.56 sec)
  
  ```

* 我们使用`explain select * from user where age>=60` 分析一下。

* 下面的这条语句，使用的`type`为ALL，意味着是**全表扫描**，`possible_keys`是指**可能用得到的索引**，这里可能使用到的索引是为age建的普通索引，但实**际上数据库使用的索引是在`key`那一列**，是`NULL`。也就是说**这句sql不走索引，全表扫描**。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianbibkNQ7b3dWIYFDypVnpK384CyKs22B0E3ORxicOAnq0mFeFaCENC3HSo4HvwGnVdREZWCh9Gfib0g/640?wxfrom=5&wx_lazy=1&wx_co=1)

* 这个是因为数据表里，符合条件的数据行数（`rows`）太多，如果使用age索引，那么需要将它们从age索引中读出来，并且age索引是**普通索引**，还需要**回表**找到对应的**主键**才能找到对应的**数据页**。**算下来还不如直接走主键划算。于是最终选择了全表扫描**。

######  索引不符合预期

* 随时时间边长，开发的人变多了，数据量也变大了，甚至还可能会加入一些其他重复多余的索引，就有可能出现用着用着，用到了不符合你预期的其他索引了。从而导致查询突然变慢。
* 可以通过`force index`**指定索引**
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianbibkNQ7b3dWIYFDypVnpK3hlL9hzg416jtwRJAlWhZEUZ5oM9tRRicibeHiauTg4PZTtflJnszUwepg/640?wxfrom=5&wx_lazy=1&wx_co=1)



######  走索引还是很慢

* 1. 索引区分度太低

* 比如网页全路径的url链接，这拿来做索引，一眼看过去全都是同一个域名，如果**前缀索引**的长度建得不够长，那这走索引跟走**全表扫描**似的，正确姿势是尽量让索引的**区分度**更高，比如域名去掉，只拿后面URI部分去做索引。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianbibkNQ7b3dWIYFDypVnpK3EzMC7fictT9d9s3uBMja0BsPrueIuOLmeagruypRS6w9GduFwFhOPkA/640?wxfrom=5&wx_lazy=1&wx_co=1)

* 2. 索引中匹配到的数据太大，这时候需要关注的是explain里的**rows**字段了。

*  **rows** :是用于**预估**这个查询语句需要查的行数的，它不一定完全准确，但可以体现个大概量级。

* 可能出现的情况：

* > * 如果这个字段具有**唯一**的属性，比如电话号码等，一般是不应该有大量重复的，那可能是你代码逻辑出现了**大量重复插入**的操作，你需要检查下代码逻辑，或者需要加个**唯一索引**限制下。
  > * 如果**这个字段下的数据就是会很大，是否需要全部拿？**如果不需要，加个`limit`限制下。如果确实要拿全部，那也不能一次性全拿，今天你数据量小，可能一次取一两万都没啥压力，万一哪天涨到了十万级别，那一次性取就有点吃不消了。你可能需要**分批次取**，具体操作是先用`order by id`排序一下，拿到一批数据后取`最大id`作为下次取数据的起始位置。



##### 连接数过少

* mysql的server层里有个**连接管理**，它的作用是管理客户端和mysql之间的长连接。

* 客户端与server层如果只有**一条**连接，那么在执行sql查询之后，只能阻塞等待结果返回，如果有大量查询同时并发请求，那么**后面的请求都需要等待前面的请求执行完成**后，才能开始执行。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianbibkNQ7b3dWIYFDypVnpK3c9WU7FyGe4jzn3caJWMOJPWOSPWGE6bp3TyV88ow9swXTlSFdCoczg/640?wxfrom=5&wx_lazy=1&wx_co=1)

* 很多时候我们的应用程序，比如go或java这些，**会打印出sql执行了几分钟的日志，但实际上你把这条语句单独拎出来执行，却又是毫秒级别的。**这都是因为这些sql语句在**等待**前面的sql执行完成。

* 解决方案：

* 如果我们能**多建几条连接**，那么请求就可以并发执行，后面的连接就不用等那么久了。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianbibkNQ7b3dWIYFDypVnpK36d77KUv53ibMkSkLg1b2El8qjdg0taeW2NiaLVbwvPbdNYuBzVyaIX6w/640?wxfrom=5&wx_lazy=1&wx_co=1)

* 而连接数过小的问题，**受数据库和客户端两侧同时限制**。

* > * 数据库端：可以通过设置mysql的`max_connections`参数，更改数据库的最大连接数。
  >
  > * 应用侧（客户端侧）： 应用侧与mysql底层的连接，是**基于TCP协议的长链接**，而TCP协议，需要经过**三次握手和四次挥手**来实现**建连和释放**。如果我每次执行sql都重新建立一个新的连接的话，那就要不断握手和挥手，这很**耗时**。所以一般会建立一个**长连接池**，连接用完之后，塞到连接池里，下次要执行sql的时候，再从里面捞一条连接出来用，非常环保。
  >
  > * ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianbibkNQ7b3dWIYFDypVnpK3HLmE3tERpYBAzmzgpy93vn33lxBycGUgg0mZsOe5nmZTV7tluToXCw/640?wxfrom=5&wx_lazy=1&wx_co=1)
  >
  > * 我们一般写代码的时候，都会通过第三方的**orm库**来对数据库进行操作，而成熟的orm库，**百分之一千万都会有个连接池。**
  >
  > * ```go
  >   func Init() {
  >     db, err := gorm.Open(mysql.Open(conn), config)
  >       sqlDB, err := db.DB()
  >       // SetMaxIdleConns 设置空闲连接池中连接的最大数量
  >       sqlDB.SetMaxIdleConns(200)
  >       // SetMaxOpenConns 设置打开数据库连接的最大数量
  >       sqlDB.SetMaxOpenConns(1000)
  >   }
  >   ```
  >
  > * 



##### buffer pool 太小

* 我们在前面的数据库查询流程里，提到了进了innodb之后，会有一层内存buffer pool，用于**将磁盘数据页加载到内存页中**，只要查询到buffer pool里有，就可以直接返回，否则就要**走磁盘IO**，那就慢了。

* 也就是说，**如果我的buffer pool 越大，那我们能放的数据页就越多，相应的，sql查询时就更可能命中buffer pool，那查询速度自然就更快了。**

* ```sql
  mysql> show global variables like 'innodb_buffer_pool_size';
  +-------------------------+-----------+
  | Variable_name           | Value     |
  +-------------------------+-----------+
  | innodb_buffer_pool_size | 134217728 |
  +-------------------------+-----------+
  1 row in set (0.01 sec)
  // 设置成 512MB
  mysql> set global innodb_buffer_pool_size = 536870912;
  Query OK, 0 rows affected (0.03 sec)
  
  mysql> show global variables like 'innodb_buffer_pool_size';
  +-------------------------+-----------+
  | Variable_name           | Value     |
  +-------------------------+-----------+
  | innodb_buffer_pool_size | 536870912 |
  +-------------------------+-----------+
  1 row in set (0.00 sec)
  
  ```

* 一般情况下**buffer pool命中率**都在`99%`以上，如果低于这个值，才需要考虑加大innodb buffer pool的大小。



##### 总结

- **数据查询过慢一般是索引问题**，可能是因为选错索引，也可能是因为查询的行数太多。
- 客户端和数据库连接数过小，会限制sql的查询并发数，增大连接数可以提升速度。
- innodb里会有一层内存buffer pool用于提升查询速度，命中率一般>99%，如果低于这个值，可以考虑增大buffer pool的大小，这样也可以提升速度。
- 查询缓存（query cache）确实能为查询提速，但一般不建议打开，因为限制比较大，并且8.0以后的Mysql里已经将这个功能干掉了。





#### Mysql 为什么插入失败









#### 分库分表带来的读扩散如何解决









#### MySQL 主库更新了，从库都读到最新值，主库可能读到旧值吗？









#### 两个事务并发写，可以保证数据唯一吗











#### 关系型数据库和非关系型数据库的区别

* 关系型数据库：

	> * Mysql , Oracle
	>
	> * 容易理解，二维表的结构非常接近真实世界，容易理解
	>
	> * 使用方便，通过操作 sql 语句，使得操作关系型数据库非常的方便
	>
	> * 数据库具有 ACID 特性，大大降低了数据冗余以及数据的不一致特性
	>
	> * 银行系统，需要这种强一致性，所以依旧会采用 关系型数据
	>
	> * 缺点：
	>
	> 	> * 海量数据读写效率，因为需要保证一致性，所以一定的牺牲了性能
	> 	> * 数据库没法横向的扩展，即像 webserver 一样简单的通过添加更多的硬件从而来扩展性能和负载的能力。
	> 	> * 固定的表结构，这个时候随着业务的提升，功能的增加，往往意味着数据结构的重大变化，关系型数据库难以应对。

* 非关系型数据库：

	> * MongoDB, Redis
	> * 应对高并发的场景，对于一致性没有特比的额重要，对**并发读写的能力**要去极高，这个时候使用 NOSQL.
	> * NOSQL 是一种结构化的存储方法的集合，使用键值来存储，且结构是不稳定的。
	> * 每一个元祖都可以有不同的字段，不会局限于固定的结构，减少一些时间和空间的开销。



#### LSM tree

* HDD 时代：（机械硬盘）

* > * 顺序写与随机操作性能不对称
  > * 由于机械磁盘需要磁盘的旋转和机械臂的移动操作进行读写操作，**顺序写**吞吐量是**随机读**的 25 倍

* SSD 时代：

* > * 顺序写与随机写性能不对称
  > * 由于 SSD 的随机写会给主控带来 GC 的压力，**顺序写**的吞吐是**随机写**的 6 倍

* HDD 时代， 顺序操作远快于随机操作

* SDD 时代， 顺序写操作远快于随机写操作

* 这两者的共性都是顺序写是一个对设备很友好的操作，LSMT 符合这一点，而 **B+Tree 依赖原地更新，导致随机写**。







### 智力题

#### 使用一个天平找8个球中其中一个重量不一致的球

##### 不一样重的球轻重不清楚

* 有一个天平和8个球，7个的重量一样，有一个与其他的重量***\*不一致(并不知道比其他7个重还是轻)\****，求需要称多少次才能找到重量不一致的球？
* A:3 次
* **第一次**： 将八个球每两两进行分组，分为4个组，假设12为A组、34为B组、56为C组、78为D组；
* 将A组放在天平左盘，B组放在右盘，如果不平衡，可以判断出不一致的球在AB两组中，即在1234这四个球中，且5678四个球重量是一样的球；
* **第二次**： 假设不一致的球在AB两组(在CD两组也可一样)中，即在1234四个球中；
* 将1球放在左盘，2球放在右盘，如果不平衡，可知不一致的球在12这两个球中；如果平衡，则可推断出不一致的球在34这两个球中；
* **第三次**： 假设不一致的球在A组，即在12两个球中；将1球放在左盘，3(45678任一个都可以)放在右盘，如果不平衡，可判断出不一致的球为1球；

##### 不一样重的球轻重清楚

* A： 2 次
* **第一次**：
* 分别拿3个球放在天平的两边，如果天平平衡，则再称剩余的两个球；
* 如果天平不平衡，则把重的一边的3个球中任选2个球放在天平两边，
* **第二次：** 
* 若天平平衡，则剩余的那个球是比较重的那个，若不平衡，则天平下沉的一端是那个重一些的球







### 系统设计

#### 12306 设计逻辑

##### 大型并发系统架构





##### 秒杀抢购的系统选择









##### 扣库存的艺术









##### 如何解决超卖问题？





#### 高并发系统基础知识

* 基本实现方案：

* > * 池化技术——避免创建资源的性能损耗以及有效保护系统 
  > * 异步化技术——比如：MQ，系统解藕，削峰填谷 
  > * 缓存技术——提速大杀器 
  > * 多线程技术——充分利用计算机多核的能力 
  > * RPC技术——提高开发并行度，系统解藕 
  > * 分布式技术——提高系统性能和存储容量

* 系统瓶颈：

* > * 如果是数据库访问慢，那么就要看是不是有锁表的情况、是不是有全表扫描、索引加的是否合适、是否有 JOIN 操作、需不需要加缓存，等等；
  > * 如果是网络的问题，就要看网络的参数是否有优化的空间，抓包来看是否有大量的超时重传，网卡是否有大量丢包等。
  > * **IO 密集型系统**指的是系统的大部分操作是在等待 IO 完成，这里 IO 指的是**磁盘 IO 和网络 IO**。我们熟知的系统大部分都属于 IO 密集型，比如数据库系统、缓存系统、Web 系统。这类系统的性能瓶颈可能出在系统内部，也可能是依赖的其他系统，而发现这类性能瓶颈的手段主要有两类。
  > * **CPU 密集型系统**中，需要处理大量的 CPU 运算，那么选用更高效的算法或者减少运算次数就是这类系统重要的优化手段。比方说，如果系统的主要任务是计算 Hash 值，那么这时**选用更高性能的 Hash 算法就可以大大提升系统的性能**。

* 提高并发度。要么加硬件，要么降低服务响应时间。做为开发，我们的目光更聚焦在降低响应时间这块。 

* > 1. 采用非阻塞的rpc调用（高效的远端请求模式，采用容器的覆盖网络我认为也算） 
  > 2. 将计算密集和io密集的的逻辑分割开，单独线程池，调整线程比例压榨单机性能（或者说找拐点）。 
  > 3. 做缓存，io耗时的缓存和计算耗时的缓存（多级缓存，数据压缩降低带宽）。 
  > 4. 采用享元模式，用好对象池和本地线程空间，尽量减少对象创建与销毁的开销，提高复用。 
  > 5. 业务拆分，像状态变化后的外部系统通知，业务监控，es或solr等副本数据同步等操作，无需在主流程中做的事都拆掉。走canal监听表数据变化，推mq保最终一致的方式从业务项目完全解偶出来。 
  > 6. fork_join，分而治之的处理大任务。并发编程，采用多线程并行的方式处理业务。（规避伪共享，减小锁力度，采用合适的锁）。 
  > 7. 数据库配置优化，查询优化。（存储优化比较头疼，毕竟不按业务拆单点跑不掉，单点性能就要命。基本只能内存库先行，后台同步数据做持久。然后内存库多副本，自修复，保留一系列自修复失败的修复手段）

* 发生故障如何解决：

* failover（故障转移）、超时控制以及降级和限流

* > 1. 故障转移：集群式部署，自动上下线（无状态设计） 
  > 2. 超时控制：自我保护机制，超时请求直接断掉 (超时时间都不是一成不变的，需要在后面的系统维护过程中不断地修改。)
  > 3. 降级：高并发场景下，节省资源给核心服务 
  > 4. 限流：高并发场景下，限制外部访问频次，保护服务 (它通过对并发的请求进行限速来保护系统)。(虽然这种做法损害了用户的使用体验，但是它是在极端并发下的无奈之举，是短暂的行为，因此是可以接受的。)
  > 5. 熔断机制：依赖的第三方服务宕机，系统自动熔断，自我保护，防止雪崩

* 电商系统基本流程

* ![img](https://static001.geekbang.org/resource/image/2d/f3/2d95823d39676e18a43ab3328ce0d0f3.jpg?wh=1142*288)

##### 通用设计方式

* Scale-out（横向扩展）：分而治之是一种常见的高并发系统设计方法，采用分布式部署的方式把流量分流开，让每个服务器都承担一部分并发和流量。
* 缓存：使用缓存来提高系统的性能，就好比用“拓宽河道”的方式抵抗高并发大流量的冲击。
* 异步：在某些场景下，未处理完成之前我们可以让请求先返回，在数据准备好之后再通知请求方，这样可以在单位时间内处理更多的请求。

###### Scale-out

* **Scale-up** 通过购买性能更好的硬件来提升系统的并发处理能力，比方说目前系统 4 核 4G 每秒可以处理 200 次请求，那么如果要处理 400 次请求呢？很简单，我们把机器的硬件提升到 8 核 8G（硬件资源的提升可能不是线性的，这里仅为参考）。

* **Scale-out** 则是另外一个思路，它通过将多个低性能的机器组成一个分布式集群来共同抵御高并发流量的冲击。沿用刚才的例子，我们可以使用两台 4 核 4G 的机器来处理那 400 次请求。

* Scale-out 的问题？

* > * 如果某个节点出现故障如何保证整体可用性？
  > * 当多个节点有状态需要同步时如何保证状态信息在不同节点的一致性？
  > * 如何做到使用方无感知的增加和删除节点？



###### 缓存

* 我们可以将任何降低响应时间的中间存储都称为缓存。缓存的思想遍布很多设计领域，比如在操作系统中 CPU 有多级缓存，文件有 Page Cache 缓存



###### 异步处理

* 那么什么是同步，什么是异步呢？以方法调用为例，**同步调用代表调用方要阻塞等待被调用方法中的逻辑执行完成**。这种方式下，当被调用方法响应时间较长时，会造成调用方长久的阻塞，在高并发下会造成整体系统性能下降甚至发生雪崩。
* 异步调用恰恰相反，调用方不需要等待方法逻辑执行完成就可以返回执行其他的逻辑，**在被调用方法执行完毕后再通过回调、事件通知等方式将结果反馈给调用方。**
* 采用异步的方式，**后端处理时会把请求丢到消息队列中，同时快速响应用户，告诉用户我们正在排队处理，然后释放出资源来处理更多的请求。**订票请求处理完之后，再通知用户订票成功或者失败。
* ![img](https://static001.geekbang.org/resource/image/07/09/0756d48f746590894b6e96ae4e4f7609.jpg?wh=1142*459)



* 模块拆分：

* 而部署方式按照最简单的三层部署架构，负载均衡负责请求的分发，应用服务器负责业务逻辑的处理，数据库负责数据的存储落地。这时，所有模块的业务代码都混合在一起了，数据也都存储在一个库里。

* ![img](https://static001.geekbang.org/resource/image/58/a6/5803451931917e0806c37c39802410a6.jpg?wh=1142*920)

* 存储拆分：

* > * 拆分之后，这个简单的社区系统就有了用户库、内容库、评论库、点赞库和关系库。这么做还能隔离故障，某一个库“挂了”不会影响到其它的数据库。
  > * ![img](https://static001.geekbang.org/resource/image/5e/b6/5ee6e1350e2d4d5514a05032b10bd3b6.jpg?wh=1142*854)
  > * 水平拆分之后，我们就可以让数据库突破单机的限制了。但这里要注意，我们**不能随意地增加节点**，因为一旦增加节点就需要**手动地迁移数据**，成本还是很高的。所以基于长远的考虑，我们**最好一次性增加足够的节点以避免频繁的扩容**。
  > * 当数据库按照业务和数据维度拆分之后，我们**尽量不要使用事务**。因为当**一个事务中同时更新不同的数据库时**，需要使用二阶段提交，来协调所有数据库**要么全部更新成功，要么全部更新失败**。这个协调的成本会随着资源的扩展不断升高，最终达到无法承受的程度。

* 业务层的拆分：

* > * 我们需要把**相同业务的服务拆分成单独的业务池**，比方说上面的社区系统中，我们可**以按照业务的维度拆分**成用户池、内容池、关系池、评论池、点赞池和搜索池。
  > * ![img](https://static001.geekbang.org/resource/image/a6/b5/a62e9add7797fc8e55c06fa8a21065b5.jpg?wh=1142*664)
  > * 我们还可以根据业务接口的重要程度，把**业务分为核心池和非核心池**。我们可以优先保证核心池的性能，当**整体流量上升时优先扩容核心池，降级部分非核心池的接口**，从而保证整体系统的稳定性。
  > * ![img](https://static001.geekbang.org/resource/image/ce/28/ce6e856238d8af7059c44b3a47eced28.jpg?wh=1142*499)
  > * 





#### 数据库篇

##### 池化技术

###### 线程池

* 在使用线程池时，我们尽量不用要使用无界队列，因为一旦任务执行缓慢，就会造成**任务大量堆积，占据大量内存资源**，也会造成频繁 Full GC，影响系统可用性.
* 原生的 Java 线程池适用于处理 **CPU 密集型的任务**，而我们的系统中的任务，**大部分是 I/O 密集型的**，所以最好对 Java 原生的线程池做一些改造来使用

###### MySQL 连接过程

* 第一部分是前三个数据包（TCP 三次握手过程）。第一个数据包是客户端向服务端发送的一个“SYN”包，第二个包是服务端回给客户端的“ACK”包以及一个“SYN”包，第三个包是客户端回给服务端的“ACK”包，熟悉 TCP 协议的同学可以看出**这是一个 TCP 的三次握手过程**。
* 第二部分是 **MySQL 服务端校验客户端密码的过程**。其中第一个包是服务端发给客户端要求认证的报文，第二和第三个包是客户端将加密后的密码发送给服务端的包，最后两个包是服务端回给客户端认证 OK 的报文。从图中，你可以看到**整个连接过程大概消耗了 4ms**
* **单条 SQL 的平均执行时间大概是 1ms**，如果按照原来的方式建立一次连接只执行一条 SQL 的话，1s 只能执行 200 次数据库的查询，而数据库建立连接的时间占了其中 4/5

![img](https://static001.geekbang.org/resource/image/3d/1b/3d2f10c8fb21873f482688dba6f4f71b.jpg?wh=1142*190)



* 因为你们数据库的调用方式是先获取数据库的连接，然后依靠这条连接从数据库中查询数据，**最后关闭连接释放数据库资源**。

* **每次执行 SQL 都需要重新建立连接**，所以你怀疑，是不是频繁地建立数据库连接耗费时间长导致了访问慢的问题。

* 数据库连接池有两个最重要的配置：最小连接数和最大连接数，它们控制着从连接池中获取连接的流程：

* > * 如果当前连接数小于最小连接数，则创建新的连接处理数据库请求；
  > * 如果连接池中**有空闲连接则复用空闲连接**；
  > * 如果空闲池中没有连接并且当前连接数小于最大连接数，则**创建新的连接处理请求**；
  > * 如果当前连接数已经大于等于最大连接数，则**按照配置中设定的时间（C3P0 的连接池配置是 checkoutTimeout）等待旧的连接可用；**
  > * 如果等待超过了这个设定时间则向用户抛出错误。

* 连接池可能出现的错误：

* > * 数据库的域名对应的 IP 发生了变更，池子的连接还是使用旧的 IP，当旧的 IP 下的数据库服务关闭后，再使用这个连接查询就会发生错误；
  > * MySQL 有个参数是“wait_timeout”，控制着当数据库连接闲置多长时间后，数据库会主动地关闭这条连接。这个机制对于数据库使用方是无感知的，所以**当我们使用这个被关闭的连接时就会发生错误**.

* 解决方式：

* > * **启动一个线程来定期检测连接池中的连接是否可用**，比如使用连接发送“select 1”的命令给数据库看是否会抛出异常，如果抛出异常则将这个连接从连接池中移除，并且尝试关闭。目前 C3P0 连接池可以采用这种方式来检测连接是否可用，也是我比较推荐的方式。
  > * **在获取到连接之后，先校验连接是否可用，如果可用才会执行 SQL 语句**。比如 DBCP 连接池的 testOnBorrow 配置项，就是控制是否开启这个验证。这种方式在获取连接时会引入多余的开销，在线上系统中还是尽量不要开启，在测试服务上可以使用。

* 线程池： 如果**使用无界队列的话，最大线程数就没有意义了**，因为永远不会用到，所以尽量不要使用无界队列

* 池化技术的特点：

* 它的核心思想是空间换时间，期望使用**预先创建好的对象来减少频繁创建对象的性能开销**，同时还可以对对象进行统一的管理，**降低了对象的使用的成本**，总之是好处多多。

* 池化技术缺点：

* > * 比方说存储池子中的对象肯定需要**消耗多余的内存**，如果对象没有被频繁使用，就会造成内存上的浪费。
  > * 池子中的对象需要在**系统启动的时候就预先创建完成**，这在一定程度上**增加了系统启动时间**。

* 可**这些缺陷相比池化技术的优势来说就比较微不足道了**，只要我们确认要使用的对象在创建时确实比较耗时或者消耗资源，并且这些对象也确实会被频繁地创建和销毁，我们就可以使用池化技术来优化。



###### 总结

* 池子的最大值和最小值的设置很重要，初期可以依据经验来设置，后面还是需要根据实际运行情况做调整。
* **池子中的对象需要在使用之前预先初始化完成**，这叫做池子的预热，比方说使用线程池时就需要预先初始化所有的核心线程。如果池子未经过预热可能会导致系统重启后产生比较多的慢请求。
* 池化技术核心是一种**空间换时间优化方法的实践**，所以要关注空间占用情况，**避免出现空间过度使用出现内存泄露或者频繁垃圾回收等问题**。







##### 主从分离 / 读写分离

* 两个关键的技术： 

* > * 一个是数据的拷贝，我们称为主从复制；
  > * 在主从分离的情况下，我们如何屏蔽主从分离带来的访问数据库方式的变化，让开发同学像是在使用单一数据库一样。

###### 主从复制

* MySQL 的主从复制是依赖于 **binlog 的**，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上二进制日志文件。主从复制就是将 binlog 中的数据从主库传输到从库上，一般这个过程是**异步**的，即主库上的操作不会等待 binlog 同步的完成。

* 使用独立的 **log dump 线程是一种异步的方式**，可以**避免对主库的主体更新流程产生影响**，而从库在接收到信息后并不是写入从库的存储中，是写入一个 **relay log**，是**避免写入从库实际存储会比较耗时，最终造成从库和主库延迟变长**。

* ![img](https://static001.geekbang.org/resource/image/57/4d/575ef1a6dc6463e4c5a60a3752d8554d.jpg?wh=1142*506)

* Q : 是不是我无限制地增加从库的数量就可以抵抗大量的并发呢？

* A : 实际上并不是的。因为随着从库数量增加，从库连接上来的 IO 线程比较多，**主库也需要创建同样多的 log dump 线程来处理复制的请求，对于主库资源消耗比较高，同时受限于主库的网络带宽**，所以在实际使用中，一般一个主库最多挂 3～5 个从库。

* 主从复制的缺陷：**主从同步的延迟**

* > * 部署的复杂度。
  >
  > * 还有就是会带来一定的主从同步的延迟，这种延迟有时候会对业务产生一定的影响。
  >
  > * 主从复制最大的问题是主从延迟问题，同一时刻**主库写入后读取数据，如果此时从库还没进行同步则会出现读不到数据的情况**
  >
  > * <img src="https://static001.geekbang.org/resource/image/d0/44/d06716649d3894e8c2b2bf242b1ab544.jpg?wh=1142*479" alt="img" style="zoom:80%;" />
  >
  > * 主从存在延迟，会导致在**从库中获取不到微博信息，整个流程会出现异常**。
  >
  > * 解决方案： 
  >
  > * > * **第一种方案是数据的冗余**。你可以在发送消息队列时不仅仅发送微博 ID，而是**发送队列处理机需要的所有微博信息**，借此避免从数据库中重新查询数据。（可能造成单条消息比较大，从而增加了消息发送的带宽和时间。）（优先选择）
  >   > * **第二种方案是使用缓存**。我可以在同步写数据库的同时，也把微博的数据写入到 Memcached 缓存里面，这样队列处理机在获取微博信息的时候会**优先查询缓存，这样也可以保证数据的一致性**。
  >   > * **最后一种方案是查询主库**。我可以在队列处理机中不查询从库而改为查询主库。不过，这种方式使用起来要慎重，要明确查询的量级不会很大，是在主库的可承受范围之内，否则会**对主库造成比较大的压力**。（避免使用这种方式）





###### 如何访问数据库

* 为了降低实现的复杂度，业界涌现了很多**数据库中间件**来解决数据库的访问问题，这些中间件可以分为两类。
* 业界有很多的方案可以**屏蔽主从分离之后数据库访问的细节**，让**开发人员像是访问单一数据库一样**，包括有像 TDDL、Sharding-JDBC 这样的嵌入应用内部的方案，也有像 Mycat 这样的独立部署的代理方案。
* 当有一个数据库请求时，**中间件将 SQL 语句发给某一个指定的数据源来处理，然后将处理结果返回**。
* ![img](https://static001.geekbang.org/resource/image/e7/ff/e7e9430cbcb104764529ca5e01e6b3ff.jpg?wh=1142*714)
* 由于它是独立部署的，所以也比较方便进行维护升级，比较适合有一定运维能力的大中型团队使用。
* 它的缺陷是所有的 **SQL 语句都需要跨两次网络**：从应用到代理层和从代理层到数据源，所以在性能上会有一些损耗。



###### MySQL主从同步延时是受什么影响

* 备库部署的**硬件性能低**，以为从库，就不需要那么好的硬件。
* 如果主备的硬件性能一致，那么可能是**运营团队对备库随意使用**，反而造成从库比主库压力更大。主库克制，备库就是后妈生的。（哈哈）
* 主库的大事务，传到备库执行，备库也是大事务，例如大表的DDL事务，或者一次删除很多的delete操作等。 
* 备库是**单线程 SQL_Thread** 执行relay_log，主库的生成的binlog，从库消费跟不上。在mysql 5.7.22 之后有**并行复制的功**能，可以解决。



###### 总结：

* 使用主从复制这个技术点时，你一般会考虑两个问题：

* > * 主从的一致性和写入性能的权衡，如果你要保证所有从节点都写入成功，那么写入性能一定会受影响；如果你只写入主节点就返回成功，那么**从节点就有可能出现数据同步失败的情况，从而造成主从不一致**，**而在互联网的项目中，我们一般会优先考虑性能而不是数据的强一致性。**
  > * 主从的**延迟问题**，很多诡异的读取不到数据的问题都可能会和它有关，如果你遇到这类问题不妨先看看主从延迟的数据。
  > * **一般我们会把从库落后的时间作为一个重点的数据库指标做监控和报警，正常的时间是在毫秒级别**，一旦落后的时间达到了秒级别就需要告警了。









##### 分库分表

* 订单量突破了五千万。订单数据都是**单表存储**的，你的压力倍增，因为无论是数据库的**查询还是写入性能都在下降**，数据库的磁盘空间也在报警。

* 考虑的点：

* > * 系统正在持续不断地发展，注册的用户越来越多，产生的订单越来越多，数据库中存储的数据也越来越多，单个表的数据量超过了千万甚至到了亿级别。这时**即使你使用了索引，索引占用的空间也随着数据量的增长而增大，数据库就无法缓存全量的索引信息**，那么就需要从磁盘上读取索引数据，就会影响到查询的性能了。那么这时你要如何提升查询性能呢？
  > * 数据量的增加也占据了磁盘的空间，**数据库在备份和恢复的时间变长**，你如何让数据库系统支持如此大的数据量呢？
  > * 不同模块的数据，比如用户数据和用户关系数据，全都存储在一个主库中，一旦主库发生故障，所有的模块都会受到影响，那么**如何做到不同模块的故障隔离呢？**
  > * 在 4 核 8G 的云服务器上对 MySQL 5.7 做 Benchmark，大概可以支撑 500TPS 和 10000QPS，你可以看到数据库对于写入性能要弱于数据查询的能力，那么随着系统写入请求量的增长，数据库系统如何来处理更高的并发写入请求呢？

* 数据库的写入请求量大造成的**性能和可用性方面的问题**，要解决这些问题，你所采取的措施就是**对数据进行分片**。这样可以很好地分摊数据库的读写压力，也可以突破单机的存储瓶颈，而常见的一种方式是对数据库做“分库分表”。

* 可能的坑点：

* > * 对如何使用正确的分库分表方式一知半解，没有明白使用场景和方法。比如，**一些同学会在查询时不使用分区键**；
  > * 分库分表引入了一些问题后，没有找到合适的解决方案。比如，会在查询时使用大量连表查询等等。



###### 如何做垂直拆分

* 专库专用！
* ![图片](https://mmbiz.qpic.cn/mmbiz_jpg/UtWdDgynLdbNxCVbibjnwGOWpFELTfVT5Cyktbp6hCXlyDyaUzwHeTVWibcGsqIzE30kYaRSvc2jf4tw3hb9ViaCw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
* 基本思想是依照某一种策略**将数据尽量平均地分配到多个数据库节点或者多个表中**。
* 分库分表后，每个节点只保存部分的数据，这样可以有效地减少单个数据库节点和单个数据表中存储的数据量，在**解决了数据存储瓶颈的同时也能有效地提升数据查询的性能**。
* 因为数据被分配到多个数据库节点上，那么数据的**写入请求**也从请求单一主库变成了请求多个数据分片节点，在一定程度上也会提升并发写入的性能。
* **垂直拆分**： 顾名思义就是对数据库竖着拆分，也就是将**数据库的表拆分到多个不同的数据库中**。
* 垂直拆分的原则一般是按照业务类型来拆分，**核心思想是专库专用**，将**业务耦合度比较高的表拆分到单独的库**中。
* **针对数据表进行垂直拆分的**, 就是将不同的**列字段拆分到不同的表中**.
* eg : 
* ![img](https://static001.geekbang.org/resource/image/77/40/7774c9393a6295b2d5e0f1a9fa7a5940.jpg?wh=1142*765)
* 缺陷： 数据库垂直拆分后依然**不能解决某一个业务模块的数据大量膨胀的问题**，一旦你的系统遭遇某一个业务库的数据量暴增，在这个情况下，你还需要继续寻找可以弥补的方式。
* 水平拆分： 比如微博关系量早已经过了千亿，单一的数据库或者数据表已经远远不能满足存储和查询的需求了，这个时候，你需要**将数据拆分到多个数据库和数据表中**，也就是对数据库和数据表做**水平拆分**了。



###### 如何做水平拆分

* ![图片](https://mmbiz.qpic.cn/mmbiz_jpg/UtWdDgynLdbNxCVbibjnwGOWpFELTfVT51dVLlIU5ibsQvjibrNCTuz2P44LlQicP4GraIbwNCIlkEo3Lgic8QBF9icg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

* 和垂直拆分的关注点不同，**垂直拆分**的关注点在于**业务相关性**，而**水平拆分指的是将单一数据表按照某一种规则拆分到多个数据库和多个数据表中**，关注点在数据的特点。

* 拆分规则：

* 1. **按照某个字段的 hash 值进行拆分**

* > * 这种拆分规则比较适用于实体表. eg : 用户表，内容表。我们一般按照这些**实体表的 ID 字段来拆分**
  > * 比如说我们想把用户表拆分成 16 个库，每个库是 64 张表，那么可以先对用户 ID 做哈希，哈希的目的是将 ID 尽量打散，然后再对 16 取余，这样就得到了分库后的索引值；对 64 取余，就得到了分表后的索引值。
  > * **如果要做，就尽量一次到位，比如说 16 库，每个库 64 表就基本能够满足几年内你的业务的需求。**
  > * ![img](https://static001.geekbang.org/resource/image/7c/39/7c6af43da41bb197be753207d4b9e039.jpg?wh=1142*652)

* 2. **按照某一个字段的区间来拆分：**

* > * 比较常用的是时间字段.
  > * 我们可能会要看昨天的内容，也可能会看一个月前发布的内容，这时就可以**按照创建时间的区间来分库分表**，
  > * 比如说可以把**一个月的数据放入一张表中**，这样在查询时就可以根据创建时间先定位数据存储在哪个表里面，再**按照查询条件来查询**。
  > * 使用这种拆分规则后，**数据表要提前建立好**，否则如果时间到了 2020 年元旦，DBA（Database Administrator，数据库管理员）却忘记了建表，那么 2020 年的数据就没有库表可写了，就会发生故障了。
  > * ![img](https://static001.geekbang.org/resource/image/40/c4/40ec1287d871d656f508d5e108f675c4.jpg?wh=1142*458)
  > * 



###### 引入的问题

* 分库分表引入的一个最大的问题就是引入了**分库分表键，也叫做分区键**，也就是**我们对数据库做分库分表所依据的字段**。

* **无论是哈希拆分还是区间段的拆分，我们首先都需要选取一个数据库字段**。

* 我们**之后所有的查询都需要带上这个字段**，才能找到数据所在的库和表，否则就只能向所有的数据库和数据表发送查询命令。

* **分库分表引入的另外一个问题是一些数据库的特性在实现时可能变得很困难： **

* > * 比如说多表的 JOIN 在**单库时**是可以通过一个 SQL 语句完成的.
  > * 但是拆分到**多个数据库之后**就无法跨库执行 SQL 了.
  > * 一般是把两个表的数据取出后在**业务代码里面做筛选**，复杂是有一些，不过是可以实现的(业务层实现)
  > * 再比如说在未分库分表之前查询数据总数时只需要在 SQL 中执行 count() 即可，现在数据被分散到多个库表中，我们可能要考虑其他的方案，比方说将**计数的数据单独存储在一张表中**或者**记录在 Redis 里面**。

* **可以选择带有自动分库分表功能的nosql替代存储**，比如Hbase、MongoDB等。

* 



##### 分库分表如何做数据迁移

* 总结： 可以搭建新的库之后，先在业务上**双写**，然后**校验**两边的数据，再灰度**切读**，再**全量切读**

* 双写，校验，切库，基本上都是这个套路

* 迁移的目标： 

* > * 迁移应该是在线的迁移，也就是在迁移的同时还会有数据的写入；
  > * 数据应该保证完整性，也就是说在**迁移之后需要保证新的库和旧的库的数据是一致的**；
  > * 迁移的过程需要做到可以回滚，这样**一旦迁移的过程中出现问题**，可以立**刻回滚到源库**不会对系统的可用性造成影响。



###### 双写方案

* 步骤： 

* > * 将新的库配置为源库的从库用来同步数据；如果需要将数据同步到多库多表，那么可以使用一些第三方工具获取 Binlog 的**增量日志**（比如开源工具 Canal），在获取**增量日志之后就可以按照分库分表的逻辑写入到新的库表中了**。
  > * 同时我们需要改造业务代码，在**数据写入的时候不仅要写入旧库也要写入新库**。当然，基于性能的考虑，我们可以异步地写入新库，只要保证旧库写入成功即可。
  > * 但是我们需要注意的是，需要将写入新库失败的数据记录在单独的日志中，这样方便后续对这些数据补写，保证**新库和旧库的数据一致性。**
  > * 然后我们就可以开始校验数据了。由于数据库中数据量很大，做全量的数据校验不太现实。你可以**抽取部分数据**，具体数据量依据总体数据量而定，只要保证这些数据是一致的就可以。
  > * 如果一切顺利，我们就可以将读流量切换到新库了。由于担心一次切换全量读流量可能会对系统产生未知的影响，所以这里最好采用灰度的方式来切换，比如开始切换 10% 的流量，如果没有问题再切换到 50% 的流量，最后再切换到 100%。
  > * 由于有双写的存在，所以在切换的过程中出现任何的问题都可以将**读写流量随时切换到旧库去**，保障系统的性能。
  > * 在观察了**几天**发现数据的迁移没有问题之后，就可以**将数据库的双写改造成只写新库**，数据的迁移也就完成了。
  > * ![img](https://static001.geekbang.org/resource/image/ad/30/ad9a4aa37afc39ebe0c91144d5ef7630.jpg?wh=1142*871)
  > * 如果是将数据从自建机房迁移到云上，你也可以使用这个方案(以上这个双写的策略)
  > * 只是你需要考虑的一个重要的因素是：**自建机房到云上的专线的带宽和延迟**，你需要尽量**减少跨专线的读操作**，所以在切换读流量的时候你需要保证**自建机房的应用服务器读取本机房的数据库**，**云上的应用服务器读取云上的数据库**。这样在完成迁移之前，只要将自建机房的应用服务器停掉并且将写入流量都切到新库就可以了。
  > * ![img](https://static001.geekbang.org/resource/image/b8/54/b88aefdb07049f2019c922cdb9cb3154.jpg?wh=1142*749)
  > * 这种方案是一种比较通用的方案，无论是**迁移 MySQL 中的数据还是迁移 Redis 中的数据，甚至迁移消息队列**都可以使用这种方式，你在实际的工作中可以直接拿来使用。
  > * 这种方式的好处是：迁移的过程可以随时回滚，**将迁移的风险降到了最低**。劣势是：**时间周期比较长**，应用有改造的成本。



###### 级联同步方案

* 实现简单

* 步骤： 

* > * 先将新库配置为旧库的从库，用作**数据同步**；
  > * 再将一个备库配置为新库的从库，用作**数据的备份**；
  > * 等到三个库的写入一致后，将数据库的读流量切换到新库；
  > * 然后暂停应用的写入，将业务的写入流量切换到新库（由于这里需要**暂停应用的写入，所以需要安排在业务的低峰期**）。
  > * <img src="https://static001.geekbang.org/resource/image/3a/2b/3a2e08181177529c3229c789c2081b2b.jpg?wh=1142*663" alt="img" style="zoom:80%;" />

* 回滚的思路：

* 可以先将**读流量切换到备库再暂停应用的写入，将写流量切换到备库**，这样所有的流量都切换到了备库，也就是又回到了自建机房的环境，就可以认为已经回滚了。

* ![img](https://static001.geekbang.org/resource/image/ad/b9/ada8866fda3c3264f495c97c6214ebb9.jpg?wh=1142*843)

* 这种方案优势是简单易实施，在业务上基本没有改造的成本；缺点是: **在切写的时候需要短暂的停止写入**，对于业务来说是有损的，不过如果在业务低峰期来执行切写，可以将对业务的影响降至最低。



###### 数据迁移如何预热缓存

* 如果你部署一个空的缓存，那么**所有的请求就都穿透到数据库**，数据库可能因为承受不了这么大的压力而宕机，这样你的服务就会不可用了。
* 我曾经提到为了保证缓存的可用性，我们可以**部署多个副本组来尽量将请求阻挡在数据库层之上**。
* 数据的写入流程是写入 Master、Slave 和所有的**副本组**，而在**读取数据的时候，会先读副本组的数据**，如果读取不到再到 Master 和 Slave 里面加载数据，再写入到副本组中。
* 那么，我们就可以在云上部署一个副本组，这样，云上的应用服务器读取云上的副本组，如果**副本组没有查询到数据，就可以从自建机房部署的主从缓存上加载数据，回种到云上的副本组上**。
* 

![img](https://static001.geekbang.org/resource/image/ab/c6/abc0b5e4c80097d8e02000b30e7ea9c6.jpg?wh=1142*473)

* 当云上部署的副本组足够热之后，也就是缓存的命中率达到至少 90%，就可以将云机房上的缓存服务器的**主从都指向这个副本组**，这时迁移也就完成了。
* 这种方式足够简单，不过有一个**致命的问题是**：如果云上的请求穿透云上的副本组，到达自建机房的主从缓存时，这个过程是**需要跨越专线**的。(注意缓存是一步一步进行预热的)



###### 改造副本组方案预热缓存

* 步骤：

* > * 在云上部署多组 mc 的副本组，自建机房在接收到写入请求时，会优先写入自建机房的缓存节点，异步写入云上部署的 mc 节点；
  > * 在处理自建机房的读请求时，会**指定一定的流量（比如 10%）优先走云上的缓存节点**，这样虽然也会走专线穿透回自建机房的缓存节点，但是流量是可控的；（放少量流量到云上缓存节点）
  > * 当**云上缓存节点的命中率达到 90% 以上时，就可以在云上部署应用服务器**，让云上的应用服务器完全走云上的缓存节点就可以了。(注意之前是没有在云上部署应用服务器，都是在本地的应用服务器上执行任务的)
  > * ![img](https://static001.geekbang.org/resource/image/7f/f4/7f41a529a322e396232ac7963ec082f4.jpg?wh=1142*435)
  > * 















##### 分库分表后 ID 的全局唯一性

* 我们通过**分库分表和主从读写分离**的方式解决了数据库的扩展性问题
* 数据库在分库分表之后，我们在使用数据库时存在的许多限制，比方说查询的时候必须带着分区键；
* 一些聚合类的查询（像是 count()）性能较差，需要考虑使用计数器等其它的解决方案，
* 其实分库分表还有一个问题：就是**主键的全局唯一性的问题**。



###### 数据库主键如何选择

* > * 使用业务字段作为主键，比如说对于用户表来说，可以使用手机号，email 或者身份证号作为主键。
  > * **使用生成的唯一 ID 作为主键**。(主要的方式)

* 在单库单表的场景下，我们可以使用数据库的自增字段作为 ID，因为这样最简单，对于开发人员来说也是透明的。但是当数据库分库分表后，使用自增字段就无法保证 ID 的全局唯一性了。



######  snowflake 

* 时间戳按照时间戳递增的，序列号还是随机的。
* ![img](https://static001.geekbang.org/resource/image/2d/8d/2dee7e8e227a339f8f3cb6e7b47c0c8d.jpg?wh=1142*304)
* 第一位不用，因为需要保证生成的是正整数，避免出现负数。
* 也满足 ID 所需要的全局唯一性，**单调递增性**，还包含一定的业务上的意义。
* 

























##### NOSQL 使用

* NoSQL 数据库**一般不支持事务**，并且对于一些复杂的 SQL 查询支持程度不高
* 首先不管是sql数据库还是nosql数据库，**提升写性能都是靠的将随机写转化为顺序写**！在这方面**mysql使用WAL机制**已经做的很好了！但是关系形数据库主要是在**扩展性方面有缺陷**！相对于除了kv型的nosql数据库，其他类型的和mysql性能都差不多！
* 如果说**mysql自动扩展性方面的缺陷**，不想分库分表的话，了解下TIDB

* 优化改进思路：

* > * 提升它的**读写性能，尤其是读性能**，因为我们面对的多是一些读多写少的产品。比方说，你离不开的微信朋友圈、微博和淘宝，都是查询 QPS 远远大于写入 QPS。
  > * 增强它在存储上的扩展能力，从而**应对大数据量的存储需求**。

* 就可以利用 NoSQL，因为**它有着天生分布式的能力**，能够提供**优秀的读写性能**，可以很好地补充传统关系型数据库的短板。

* NoSQL 例子：

* > * Redis、LevelDB 这样的 KV 存储。这类存储相比于传统的数据库的优势是**极高的读写性能**，一般对性能有比较高的要求的场景会使用。
  > * Hbase、**Cassandra 这样的列式存储数据库**。这种数据库的特点是数据不像传统数据库以行为单位来存储，而是以列来存储，适用于一些离线数据统计的场景。
  > * 像 **MongoDB、CouchDB 这样的文档型数据库**。这种数据库的特点是 Schema Free（模式自由），数据表中的字段可以任意扩展，比如说电商系统中的商品有非常多的字段，并且不同品类的商品的字段也都不尽相同，使用关系型数据库就需要不断增加字段支持，而用文档型数据库就简单很多了。

######  NOSQL 提升性能

* 数据库系统大多使用的是传统的机械磁盘，对于机械磁盘的访问方式有两种：一种是随机 IO；另一种是顺序 IO。**随机 IO 就需要花费时间做昂贵的磁盘寻道**。
* 随机 IO 的读写效率要比顺序 IO 小两到三个数量级，所以我们想要**提升写入的性能就要尽量减少随机 IO**。
* 以 MySQL 的 InnoDB 存储引擎来说，**更新 binlog、redolog、undolog 都是在做顺序 IO**，
* 而更新 **datafile 和索引文件则是在做随机 IO**，而为了减少随机 IO 的发生，关系数据库已经做了很多的优化，比如说写入时先写入内存，然后**批量刷新到磁盘上**，但是随机 IO 还是会发生。



###### NOSQL 实现原理

* 很多 NoSQL 数据库都在使用的**基于 LSM 树的存储引擎**

* **lsm很多时候借鉴了存储介质的物理特性**，并不全是因为内存的速度快，我以前工作时候写ssd storage就是lsm，因为lsm的工作原理特别适合ssd的存储特性，**例如不删除修改数据，只合并数据**。

* LSM 树（Log-Structured Merge Tree）**牺牲了一定的读性能来换取写入数据的高性能**，Hbase、Cassandra、LevelDB 都是用这种算法作为存储的引擎。（2000年之后的存储引擎都采用这个的，因为 LSMT 是 1996 年出来的）

* 数据首先会写入到一个叫做 MemTable 的内存结构中，在 MemTable 中数据是按照写入的 **Key 来排序**的。为了防止 MemTable 里面的数据因为机器掉电或者重启而丢失，一般会通过写 **Write Ahead Log 的方式将数据备份在磁盘上**。

* MemTable 在累积到一定规模时，它会被刷新生成一个新的文件，我们把这个文件叫做 SSTable（Sorted String Table）。当 SSTable 达到一定数量时，我们会将这些 SSTable 合并，减少文件的数量，**因为 SSTable 都是有序的，所以合并的速度也很快**。

* 当从 LSM 树里面读数据时，我们**首先从 MemTable 中查找数据**，如果数据没有找到，再从 **SSTable 中查找数据**。因为存储的数据都是有序的，所以查找的效率是很高的，只是因为数据被拆分成多个 SSTable（**sstable指的是磁盘上的结构**），所以读取的效率会低于 B+ 树索引。

* ![img](https://static001.geekbang.org/resource/image/b4/eb/b4c9c93f22edae091740fa1606d109eb.jpg?wh=1142*652)

* 核心思想就是将随机 IO 变成顺序的 IO，从而提升写入的性能。

* “select * from product where name like ‘% 电冰箱’”就**没有使用到字段“name”上的索引**，一旦没有使用索引就会扫描全表的数据，在性能上是无法接受的。

* 开源组件 Elasticsearch 来**支持搜索的请求**，它本身是基于“倒排索引”来实现的。

* 倒排索引：（支持全文搜索）

* > * 倒排索引是指将记录中的某些列做分词，然后形成的分词与记录 ID 之间的映射关系。比如说，你的垂直电商项目里面有以下记录：
  > * ![img](https://static001.geekbang.org/resource/image/20/57/201ffbb6da51e04894d8dee7eaeb5d57.jpg?wh=1142*311)
  > * 我们将商品名称做简单的分词，然后建立起分词和商品 ID 的对应关系，就像下面展示的这样：
  > * ![img](https://static001.geekbang.org/resource/image/c9/2f/c919944bcdfd1f1ce576790fc496a62f.jpg?wh=1142*452)
  > * 上面就是倒排索引。
  > * 而 Elasticsearch 作为一种常见的 **NoSQL 数据库**，就**以倒排索引作为核心技术原理**，为你提供了分布式的全文搜索服务，这在传统的关系型数据库中使用 SQL 语句是很难实现的。



###### MongoDB 解决扩展性

* 其一是 Replica，也叫做副本集，你可以理解为主从分离，也就是通过将数据拷贝成多份来保证当主挂掉后数据不会丢失。同时呢，Replica 还可以分担读请求。Replica 中有主节点来承担写请求，并且把数据变动记录到 oplog 里（类似于 binlog）；从节点接收到 oplog 后就会修改自身的数据以保持和主节点的一致。一旦主节点挂掉，MongoDB 会从从节点中选取一个节点成为主节点，可以继续提供写数据服务。
* 其二是 Shard，也叫做分片，你可以理解为分库分表，即将数据按照某种规则拆分成多份，存储在不同的机器上。MongoDB 的 Sharding 特性一般需要三个角色来支持，一个是 Shard Server，它是实际存储数据的节点，是一个独立的 Mongod 进程；二是 Config Server，也是一组 Mongod 进程，主要存储一些元信息，比如说哪些分片存储了哪些数据等；最后是 Route Server，它不实际存储数据，仅仅作为路由使用，它从 Config Server 中获取元信息后，将请求路由到正确的 Shard Server 中。
* ![img](https://static001.geekbang.org/resource/image/e8/80/e8cb47c8cc556fce058f7c5cf06d4780.jpg?wh=1142*663)
* 其三是负载均衡，就是当 MongoDB 发现 Shard 之间数据分布不均匀，会启动 Balancer 进程对数据做重新的分配，最终让不同 Shard Server 的数据可以尽量的均衡。当我们的 Shard Server 存储空间不足需要扩容时，数据会自动被移动到新的 Shard Server 上，减少了数据迁移和验证的成本。



* NOSQL 是 传统关系性数据库的补充，**弥补关系型数据库在性能、扩展性和某些场景下的不足**，所以你在使用或者选择时要结合自身的场景灵活地运用。



###### 总结

* 在性能方面，**NoSQL 数据库使用一些算法将对磁盘的随机写转换成顺序写**，提升了写的性能；
* 在某些场景下，比如全文搜索功能，关系型数据库并不能高效地支持，需要 NoSQL 数据库的支持；
* 在扩展性方面，NoSQL 数据库**天生支持分布式**，支持数据冗余和数据分片的特性。



#### 缓存篇

* **缓存比较适合读多写少的业务场景，并且数据最好带有一定的热点属性.**
* 随着并发的增加，存储数据量的增多，数据库的**磁盘 IO 逐渐成了系统的瓶颈**，我们需要一种访问更快的组件来降低请求响应时间，提升整体系统性能。
* 凡是位于**速度相差较大的两种硬件之间**，用于协调两者数据传输速度差异的结构，均可称之为缓存。
* <img src="https://static001.geekbang.org/resource/image/01/ad/0134f4cd9e0d6e8d57ebe35eb28c32ad.jpg?wh=1142*853" alt="img" style="zoom:50%;" />
* 做一次**内存寻址大概需要 100ns**，而做一次**磁盘的查找则需要 10ms**。
* **缓存和缓冲区**对应的英语是cache和buffer，**buffer的存在是为了解决数据不能一次性读写完成或某次的数据量太小，io成本又太高的折中方案**

##### 缓存加速

###### 缓存案例

* **缓存一定会引入不一致的，解决的办法需要权衡一致性和性能**。

* **TLB**（Translation Lookaside Buffer）的组件来缓存最近转换过的虚拟地址，和物理地址的映射。TLB 就是一种缓存组件

* **抖音**：平台上的短视频实际上是使用内置的网络播放器来完成的。网络播放器接收的是数据流，将数据下载下来之后经过分离音视频流，解码等流程后输出到外设设备上播放。

* 我们打开抖音，**服务端可能一次会返回三个视频信息**，我们在播放第一个视频的时候，播放器已经帮我们缓存了第二、三个视频的部分数据，这样在看第二个视频的时候就可以给用户“秒开”的感觉。

* **HTTP 协议也是有缓存机制的**。

* > * 当我们第一次请求静态的资源时，比如一张图片，服务端除了返回图片信息，在响应头里面还有一个“Etag”的字段。浏览器会缓存图片信息以及这个字段的值。当下一次再请求这个图片的时候，浏览器发起的请求头里面会有一个“If-None-Match”的字段，并且把缓存的“Etag”的值写进去发给服务端。服务端比对图片信息是否有变化，如果没有，则**返回浏览器一个 304 的状态码**，浏览器会继续使用缓存的图片信息。通过这种缓存协商的方式，可以**减少网络传输的数据大小，从而提升页面展示的性能。**
  > * **![img](https://static001.geekbang.org/resource/image/7a/81/7a2344bd27535936b4ad4d8519d9fd81.jpg?wh=1142*637)**



* 对于**静态的资源的缓存你可以选择静态缓存**，对于**动态的请求你可以选择分布式缓存**。

###### 缓存的不足

* 缓存比较适合于**读多写少的业务场景**，并且数据最好带有一定的热点属性
* 这是因为缓存毕竟会**受限于存储介质不可能缓存所有数据**，那么当数据有**热点属性的时候才能保证一定的缓存命中率**。
* 缓存会给整体系统带来复杂度，并且会有**数据不一致的风险**
* **之前提到缓存通常使用内存作为存储介质，但是内存并不是无限的**。（缓存**一定要设置过期时间**，这样可以保证**缓存中的会是热点数据**。）



###### 总结

* 缓存可以有多层，比如上面提到的静态缓存处在负载均衡层，分布式缓存处在应用层和数据库层之间，本地缓存处在应用层。我们需要将请求尽量挡在上层，因为**越往下层，对于并发的承受能力越差**；
* 缓存命中率是我们对于缓存最重要的一个监控项，**越是热点的数据，缓存的命中率就越高**。





##### 缓存读写策略

###### Cache Aside（旁路缓存）策略

[先更新数据库，再删缓存](#先更新数据库，再删缓存)



###### Read/Write Through（读穿 / 写穿）策略

* 这个策略的核心原则是用户只与缓存打交道，由缓存和数据库通信，写入或者读取数据。
* Write Through 的策略是这样的：先查询要写入的数据在缓存中是否已经存在，如果已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，如果缓存中数据不存在，我们把这种情况叫做“Write Miss（写失效）”。
* 一般来说，我们可以选择两种**“Write Miss”**方式：一个是“**Write Allocate（按写分配）**”，做法是**写入缓存相应位置，再由缓存组件同步更新到数据库中**；另一个是“No-write allocate（不按写分配）”，做法是**不写入缓存中，而是直接更新到数据库中**。
* Read Through 策略就简单一些，它的步骤是这样的：先查询缓存中数据是否存在，如果存在则直接返回，如果不存在，则由缓存组件(消息队列）负责从数据库中同步加载数据。
* ![img](https://static001.geekbang.org/resource/image/90/d1/90dc599d4d2604cd5943584c4d755bd1.jpg?wh=1142*888)
* 开发过程中**相比 Cache Aside 策略要少见一些**，原因是我们经常使用的分布式缓存组件，无论是 **Memcached 还是 Redis 都不提供写入数据库**，**或者自动加载数据库中的数据的功能**。
* **Write Through 策略中写数据库是同步的**， **异步写入数据库采用的是 write back 方式**



###### Write Back（写回）策略

* 核心思想是在**写入数据时只写入缓存**，并且把缓存块儿标记为“脏”的。而脏块儿只有被**再次使用时才会将其中的数据写入到后端存储中**。
* 有点类似 CPU 的缓存不一致如何处理的方式：
* ![img](https://static001.geekbang.org/resource/image/59/9f/59f3c4caafd4c3274ddb7e0ac37f429f.jpg?wh=1142*997)
* 读策略：
* ![img](https://static001.geekbang.org/resource/image/a0/59/a01bbf953088eef6695ffb1dc182b559.jpg?wh=1142*1036)
* **OS 大量的使用 write back 技术**： （write back 是 OS 的策略，所以这个是固定的思路）
* 比如我们在向磁盘中写数据时采用的就是这种策略。无论是操作系统层面的 **Page Cache**，还是**日志的异步刷盘**，亦或是**消息队列中消息的异步写入磁盘**，大多采用了这种策略。
* **MySQL的buffer pool使用了WriteBack策略**，但为了防止系统崩溃后数据丢失，**MySQL使用了WAL（Write-Ahead Logging）机制，写先日志**。
* 因为这个策略在性能上的优势毋庸置疑，它**避免了直接写磁盘造成的随机写问题**，毕竟写内存和写磁盘的随机 I/O 的延迟相差了几个数量级呢。
* 在向低速设备写入数据的时候，可以在**内存里先暂存一段时间的数据**，甚至做一些统计汇总（批量写入操作），然后定时地刷新到低速设备上。
* **WriteBack策略相当于缓存和缓冲区合二为一了**



###### 总结

* Cache Aside 是我们在使用分布式缓存时最常用的策略，你可以在实际工作中直接拿来使用。
* Read/Write Through 和 Write Back 策略**需要缓存组件的支持**，所以比较适合你在实现本地缓存组件的时候使用；
* Write Back 策略是**计算机体系结构中的策略**，不过写入策略中的只写缓存，异步写入后端存储的策略倒是有很多的应用场景。





##### 缓存高可用

* **分布式缓存的高可用方案。**
* 实现高可用的**核心依旧是集群**。多个缓存节点，提高容错率。
* **客户端实现**：由客户端的策略决定如何写缓存，如何读缓存。性能高，但是逻辑复杂，无法跨平台。 
* **中间件实现**：所有客户端先访问中间件，然后中间件决定了缓存策略。因为引入了中间件，所以性能较差，但是可以跨平台，并且有能力的公司还可以自研中间件。 
* **服务端实现**：主从切换由服务端实现。最大的缺点是增加了运维成本。



* 主要选择的方案有客户端方案、中间代理层方案和服务端方案三大类：

* > * **客户端方案**就是在客户端配置多个缓存的节点，通过缓存写入和读取算法策略来实现分布式，从而提高缓存的可用性。
  > * **中间代理层方案**是在应用代码和缓存节点之间增加代理层，客户端所有的写入和读取的请求都通过代理层，而代理层中会内置高可用策略，帮助提升缓存系统的高可用。
  > * **服务端方案**就是 Redis 2.4 版本后提出的 Redis Sentinel 方案。



###### 客户端方案

* 关注缓存的写和读两个方面： 

* > * 写入数据时，需要把被**写入缓存的数据分散到多个节点中**，即进行**数据分片**；
  > * 读数据时，可以利用多组的缓存来做容错，提升缓存系统的可用性。关于读数据，这里可以**使用主从和多副本两种策略**，两种策略是为了解决不同的问题而提出的。

* 缓存数据如何分片?

* > * 单一的缓存节点受到机器内存、网卡带宽和单节点请求量的限制，不能承担比较高的并发，因此我们考虑将数据分片，依照分片算法将数据打散到多个不同的节点上，**每个节点上存储部分数据**。
  > * **分片算法**常见的就是 **Hash 分片算法和一致性 Hash 分片算法**两种。

* Memcached 的主从机制

* > * Redis 本身支持主从的部署方式，但是 Memcached 并不支持，所以我们今天主要来了解一下 Memcached 的主从机制是如何在客户端实现的。
  > * 读取时，优先从 Slave 中读数据，如果读取不到数据就穿透到 Master 读取，并且将数据回种到 Slave 中以保持 Slave 数据的热度。
  > * 主从机制最大的优点就是当某一个 Slave 宕机时，还会有 Master 作为兜底，不会有大量请求穿透到数据库的情况发生，提升了缓存系统的高可用性。
  > * ![img](https://static001.geekbang.org/resource/image/54/60/5468eb8779396b38c3731839f3d8d960.jpg?wh=1142*588)

* 多副本

* > * 主从方式已经能够解决大部分场景的问题，但是对于极端流量的场景下，一组 Slave 通常来说并不能完全承担所有流量，**Slave 网卡带宽可能成为瓶颈**。
  > * 我们考虑在 Master/Slave 之前增加一层副本层，整体架构是这样的：
  > * ![img](https://static001.geekbang.org/resource/image/67/03/6779f9b6741b7767068df767218bcd03.jpg?wh=1142*537)
  > * 当客户端发起查询请求时，请求**首先会先从多个副本组中选取一个副本组发起查询**，如果查询失败，就继续查询 Master/Slave，并且将查询的结果回种到所有副本组中，避免副本组中脏数据的存在。

###### 中间代理层方案

* 客户端方案已经能解决大部分的问题，但是**只能在单一语言系统之间复用**。例如微博使用 Java 语言实现了这么一套逻辑，我使用 PHP 就难以复用，需要重新写一套，很麻烦。
* 将**客户端解决方案的经验移植到代理层中**，通过通用的协议（如 Redis 协议）来实现在其他语言中的复用。
* 用户在使用你的代理层的时候就不需要关心缓存的高可用是如何做的，只需要依赖你的代理层就好了。（缓存层对客户端来说是透明的）
* ![img](https://static001.geekbang.org/resource/image/c5/43/c517437faf418e7fa085b1850e3f7343.jpg?wh=1142*487)
* **所有缓存的读写请求都是经过代理层完成的**。**代理层是无状态**的，主要负责读写请求的路由功能，并且在其中内置了一些高可用的逻辑，**不同的开源中间代理层方案中使用的高可用策略各有不同**。



###### 服务端方案

* Redis Sentinel 模式来解决主从 Redis 部署时的高可用问题，它可以在**主节点挂了以后自动将从节点提升为主节点**，**保证整体集群的可用性**。
* ![img](https://static001.geekbang.org/resource/image/94/e1/94ae214f840d2844b7b43751aab6d8e1.jpg?wh=1142*531)
* 在 Sentinel 中会配置 Master 的地址，Sentinel 会时刻监控 Master 的状态，当发现 Master 在配置的时间间隔内无响应，就认为 Master 已经挂了，**Sentinel 会从从节点中选取一个提升为主节点，并且把所有其他的从节点作为新主的从节点**。
* Sentinel 集群内部在仲裁的时候，会根据配置的值来决定当**有几个 Sentinel 节点认为主挂掉可以做主从切换的操作**，也就是**集群内部需要对缓存节点的状态达成一致才行**。
* Redis Sentinel 不属于代理层模式，因为**对于缓存的写入和读取请求不会经过 Sentinel 节点**。
* Sentinel 节点在架构上和主从是平级的，是**作为管理者存在的**，所以可以认为是在**服务端提供的一种高可用方案**。











##### CDN

* 我们的系统中存在着大量的静态资源请求：

* > * 对于移动 APP 来说，这些静态资源主要是**图片、视频和流媒体信息**；
  > * 对于 Web 网站来说，则包括了 JavaScript 文件、CSS 文件、静态 HTML 文件等等。

* 电商系统来说，商品的图片、介绍商品使用方法的视频等等静态资源都放在了 Nginx 等 Web 服务器上，它们的读**请求量极大并且对访问速度的要求很高还占据了很高的带宽**，这时会出现访问速度慢带宽被占满影响动态请求的问题.

###### 静态资源加速的考虑点

* **静态资源访问的关键点是就近访问**，即北京用户访问北京的数据，杭州用户访问杭州的数据，这样才可以达到性能的最优。
* **考虑在业务服务器的上层增加一层特殊的缓存**，用来承担绝大部分对于静态资源的访问，这一层**特殊缓存的节点需要遍布在全国各地**，这样可以让用户选择最近的节点访问。

###### CDN 的关键技术

* CDN 就是**将静态的资源分发到位于多个地理位置机房中的服务器上**，因此它能很好地**解决数据就近访问的问题**，也就加快了静态资源的访问速度。

* 如何将**用户的请求映射到 CDN 节点上**:

* > * 我们要做的事情是将**第三方厂商提供的 IP 隐藏起来**，给到用户的最好是一个本公司域名的子域名。
  >
  > * DNS 来帮我们解决域名映射的问题了.
  >
  > * DNS（Domain Name System，域名系统）实际上就是一个**存储域名和 IP 地址对应关系的分布式数据库**
  >
  > * 域名解析的结果一般有两种:
  >
  > * > * 一种叫做“A 记录”，返回的是域名对应的 IP 地址；
  >   > * 另一种是“CNAME 记录”，返回的是另一个域名，也就是说当前域名的解析要跳转到另一个域名的解析上。实际上 www.baidu.com 域名的解析结果就是一个 CNAME 记录，域名的解析被跳转到 www.a.shifen.com 上了，我们正是利用 **CNAME 记录来解决域名映射问题的**
  >
  > * CNAME 解决域名映射问题：
  >
  > * > * 比如你的公司的一级域名叫做 example.com，那么你可以把你的图片服务的域名定义为“img.example.com”.
  >   > * 然后将这个域名的**解析结果的 CNAME 配置到 CDN 提供的域名上**，比如 uclound 可能会提供一个域名是“80f21f91.cdn.ucloud.com.cn”这个域名.
  >   > * 这样你的电商系统使用的图片地址可以是“http://img.example.com/1.jpg”。
  >   > * 用户在请求这个地址时，**DNS 服务器会将域名解析到 80f21f91.cdn.ucloud.com.cn 域名上**
  >   > * 然后再将这个**域名解析为 CDN 的节点 IP**，这样就可以得到 CDN 上面的资源数据了

* **经过了向多个 DNS 服务器做查询之后**，整个 DNS 的解析的时间有可能会到**秒级别**，那么我们如何来解决这个**性能问题**呢？

* A : 缓存解决。

* 在 APP 启动时对**需要解析的域名做预先解析**，然后把**解析的结果缓存到本地的一个 LRU 缓存里面**。

* 这样当我们要使用这个域名的时候，只需要从缓存中直接拿到所需要的 IP 地址就好了，如果缓存中不存在才会走整个 DNS 查询的过程。

* 同时为了**避免 DNS 解析结果的变更造成缓存内数据失效**，我们可以启动一个**定时器定期地更新缓存中的数据**。

* **![img](https://static001.geekbang.org/resource/image/1a/c9/1a692c89b0bcaa8106a8ba045be835c9.jpg?wh=1142*511)**

* 将用户的请求映射到 CDN 服务器上是使用 CDN 时需要解决的一个核心的问题，而 **CNAME 记录在 DNS 解析过程中可以充当一个中间代理层的角色**，可以把**用户最初使用的域名代理到正确的 IP 地址上**。

* **CDN中的核心技术DNS**，可将我们**自己命名的域名映射到CDN厂商域名CNAME**，再由**CDN厂商域名映射至厂商自己内部的IP**，再去指定IP访问静态资源。



###### 如何找到离用户最近的 CDN 节点

GSLB（Global Server Load Balance，全局负载均衡）的含义是对于**部署在不同地域的服务器之间做负载均衡**，下面可能管理了很多的本地负载均衡组件。它有两方面的作用：

> * 一方面，它是一种负载均衡服务器，负载均衡，顾名思义嘛，指的是让**流量平均分配使得下面管理的服务器的负载更平均**；
> * 另一方面，它还需要**保证流量流经的服务器与流量源头在地缘上是比较接近的**。

* eg : 比如说可以将用户的 IP 地址按照地理位置划分为若干个区域，然后将 CDN 节点对应到一个区域上，**根据用户所在区域来返回合适的节点**
* eg : 也可以通过**发送数据包测量 RTT 的方式来决定返回哪一个节点**。
* ![img](https://static001.geekbang.org/resource/image/fc/01/fcc357ff674b4abdc00dc9c33cbf9a01.jpg?wh=1142*676)



###### 总结

* DNS 技术是 CDN 实现中使用的核心技术，可以将用户的请求映射到 CDN 节点上；
* DNS 解析结果需要做本地缓存，降低 DNS 解析过程的响应时间；
* GSLB 可以给用户返回一个离着他更近的节点，加快静态资源的访问速度。



* Q：第三方CDN厂商提供我们的是域名还是IP地址?
* A: 域名



#### 消息队列篇



##### 延迟队列

* 普通队列中的元素是有序的，先进入队列中的元素会被优先取出进行消费；
* 延时队列相比于普通队列最大的区别就体现在其延时的属性上，普通队列的元素是先进先出，按入队顺序进行处理，而延时队列中的元素在入队时会指定一个延迟时间，**表示其希望能够在经过该指定时间后处理**。从某种意义上来讲，延迟队列的结构并不像一个队列，而更像是一种以时间为权重的有序堆结构。

* 延迟队列的应用场景其实也非常的广泛，比如说以下的场景：

* > * 新建的订单，如果用户在 15 分钟内未支付，则自动取消。
  > * 公司的会议预定系统，在会议预定成功后，会在会议开始前半小时通知所有预定该会议的用户。
  > * 安全工单超过 24 小时未处理，则自动拉企业微信群提醒相关责任人。
  > * 用户下单外卖以后，距离超时时间还有 10 分钟时提醒外卖小哥即将超时。

* 如果仅仅是服务单一的小程序，那也许**起个定时任务**，或者甚至人工的定时去执行能够最便捷最快速的去完成这项需求，但我们希望能够**抽象出一个消息订阅的模块服务出来给所有业务使用，这时候就需要一种通用的系统的解决方案**，这时候便需要使用到延迟队列了。

* 对于数据量比较少并且时效性要求不那么高的场景，一**种比较简单的方式是轮询数据库**，比如每秒轮询一下数据库中所有数据，处理所有到期的数据，比如如果我是**公司内部的会议预定系统的开发者**，我可能就会采用这种方案，因为整个系统的数据量必然不会很大并且会议开始前提前 30 分钟提醒与提前 29 分钟提醒的差别并不大。

* 但是如果需要处理的**数据量比较大实时性要求比较高，比如淘宝每天的所有新建订单 15 分钟内未支付的自动超时**，数量级高达百万甚至千万，这时候如果你还敢轮询数据库怕是要被你老板打死，不被老板打死估计也要被运维同学打死。

* 延迟队列为我们提供了一种**高效的处理大量需要延迟消费消息的解决方案**





###### redis zset

* Redis 有一个有序集合的数据结构 ZSet，ZSet 中每个元素都有一个对应 Score，**ZSet 中所有元素是按照其 Score 进行排序的。**

* 通过以下这几个操作使用 Redis 的 ZSet 来实现一个延迟队列：

* > * 入队操作：`ZADD KEY timestamp task`, 我们将需要处理的任务，按其需要延迟处理时间作为 Score 加入到 ZSet 中。Redis 的 ZAdd 的时间复杂度是`O(logN)`，`N`是 ZSet 中元素个数，因此我们能相对比较高效的进行入队操作。
  >
  > * 起一个进程定时（比如每隔一秒）通过`ZREANGEBYSCORE`方法查询 ZSet 中 Score 最小的元素，具体操作为：`ZRANGEBYSCORE KEY -inf +inf limit 0 1 WITHSCORES`。查询结果有两种情况：
  >
  > * > * 查询出的分数小于等于当前时间戳，说明到这个任务需要执行的时间了，则去异步处理该任务；
  >   > * 查询出的分数大于当前时间戳，由于刚刚的查询操作取出来的是分数最小的元素，所以说明 ZSet 中所有的任务都还没有到需要执行的时间，则休眠一秒后继续查询；
  >
  > * ![img](https://pic1.zhimg.com/80/v2-e9c7893d647d79cbd788a40afc1e6c58_720w.jpg)
  >
  > * 其核心设计思路：
  >
  > * > * 将延迟的消息任务通过 hash 算法路由至不同的 Redis Key 上，这样做有两大好处：
  >   >   * 避免了当一个 KEY 在存储了较多的延时消息后，入队操作以及查询操作速度变慢的问题（两个操作的时间复杂度均为`O(logN)`）。
  >   >   * 系统具有了更好的**横向可扩展性**，当数据量激增时，我们可以通过增加 Redis Key 的数量来快速的扩展整个系统，来抗住数据量的增长。
  >   > * 每个 Redis Key 都对应建立一个处理进程，称为 Event 进程，通过上述步骤 2 中所述的 ZRANGEBYSCORE 方法轮询 Key，查询是否有待处理的延迟消息。
  >   > * 所有的 Event 进程只负责分发消息，具体的业务逻辑通过一个额外的消息队列异步处理，这么做的好处也是显而易见的：
  >   >   * 一方面，Event 进程只负责分发消息，那么其处理消息的速度就会非常快，就不太会出现因为业务逻辑复杂而导致消息堆积的情况。
  >   >   * 另一方面，采用一个额外的**消息队列后，消息处理的可扩展性也会更好**，我们可以通过增加**消费者进程数量来扩展整个系统的消息处理能力。**
  >   >
  >   > * Event 进程采用 Zookeeper 选主单进程部署的方式，**避免 Event 进程宕机后，Redis Key 中消息堆积的情况**。一旦 Zookeeper 的 leader 主机宕机，Zookeeper 会自动选择新的 leader 主机来处理 Redis Key 中的消息。

* 

###### Rabbitmq

* RabbitMQ 本身并不直接提供对延迟队列的支持，我们依靠 RabbitMQ 的**TTL**以及**死信队列**功能，来实现延迟队列的效果。

* **死信队列**

* 死信队列实际上是一种 RabbitMQ 的**消息处理机制**，当 RabbmitMQ 在生产和消费消息的时候，消息遇到如下的情况，就会变成“死信”：

* > * 消息被拒绝`basic.reject/ basic.nack` 并且不再重新投递 `requeue=false`
  > * 消息超时未消费，也就是 TTL 过期了
  > * 消息队列到达最大长度

* 消息一旦变成一条死信，便会被**重新投递到死信交换机（Dead-Letter-Exchange）**，然后死信交换机根据绑定规则转发到对应的死信队列上，**监听该队列就可以让消息被重新消费**。

* **消息生存时间 TTL **
* TTL（Time-To-Live）是 RabbitMQ 的一种高级特性，表示了一条消息的最大生存时间，单位为毫秒。
* 如果一条消息在 TTL 设置的时间内没有被消费，那么它就会变成一条死信，进入我们上面所说的死信队列。
* 不过需要注意的是，使用这种方式设置的 TTL，消息可能不会按时死亡，因为 **RabbitMQ 只会检查第一个消息是否过期**。比如这种情况，第一个消息设置了 20s 的 TTL，第二个消息设置了 10s 的 TTL，那么 RabbitMQ 会等到第一个消息过期之后，才会让第二个消息过期。
* 实现细节：
* TTL 不就是延迟队列中消息要延迟的时间么？如果我们把需要延迟的消息，将 TTL 设置为其延迟时间，投递到 RabbitMQ 的普通队列中，**一直不去消费它**，那么**经过 TTL 的时间后，消息就会自动被投递到死信队列**，这时候我们使用消费者进程实时地去消费死信队列中的消息，不就实现了延迟队列的效果。
* 解决办法： 使用 插件解决： 安装好这个插件后，所有的消息就都能按照被设置的 TTL 过期了。
* **![img](https://pic4.zhimg.com/80/v2-fb8a36a6aeaff389d584de5a71d2ad7f_720w.jpg)**
* **通过 RabbitMQ 集群的特性，可以很好的解决单点故障问题**，不会因为单个节点挂掉导致延迟队列不可用或者消息丢失。

























* **消息队列适合写多的场景**

**消息队列看作暂时存储数据的一个容器，认为它是一个平衡低速系统和高速系统处理任务时间差的工具**

**消息队列保证不一下子打到db，做消峰**



##### 秒杀如何处理上万次请求

* 一些存在**高并发写请求**的场景，其中秒杀抢购就是最典型的场景.

* 因为用户查询的是少量的商品数据，属于查询的热点数据，你可以采用**缓存策略将请求尽量挡在上层的缓存中**，能被静态化的数据（比如商城里的图片和视频数据）尽量做到静态化，这样就可以命中 CDN 节点缓存减少 Web 服务器的查询量和带宽负担。

* 当然，你可以加上一些限流的策略，比如对**短时间之内来自某一个用户、某一个 IP 或者某一台设备的重复请求做丢弃处理**。

* 00:00 分秒杀活动准时开始，用户瞬间向电商系统请求生成订单，扣减库存，用户的这些**写操作都是不经过缓存直达数据库的**。

* 1 秒钟之内，有 1 万个数据库连接同时达到，系统的数据库濒临崩溃，寻找能够应对如此高并发的写请求方案迫在眉睫。

* 消息队列的案例：

* > * 在 Java 线程池中我们就会使用一个**队列来暂时存储提交的任**务，等待有**空闲的线程处理这些任务**；
  > * 操作系统中，中断的下半部分也会使用**工作队列**来实现**延后执行**；
  > * 我们在实现一个 RPC 框架时，也会将从**网络上接收到的请求写到队列里**，再启动若干个工作线程来处理。



###### 削去秒杀场景下的峰值写流量

* 将**秒杀请求暂存在消息队列中**，然后业务服务器会响应用户“秒杀结果正在计算中”，释放了系统资源之后再处理其它用户的请求。
* 我们会在**后台启动若干个队列处理程序消费消息队列中的消息**，再执行校验库存、下单等逻辑。
* 因为**只有有限个队列处理线程在执行**，所以落入后端数据库上的并发请求是有限的。
* 而请求是可以在**消息队列中被短暂地堆积**，当库存被消耗完之后，消息队列中堆积的请求就可以被丢弃了。
* ![img](https://static001.geekbang.org/resource/image/de/ad/de0a7a65a0bf51e1463d40d666a034ad.jpg?wh=1142*365)
* 以上就是秒杀系统中的常见使用场景： **削峰填谷**。
* 也就是说它可以削平短暂的流量高峰，虽说堆积会造成请求被短暂延迟处理，但是只要我们时刻监控消息队列中的堆积长度，在**堆积量超过一定量时，增加队列处理机数量**来提升消息的处理能力就好了。
* 而且秒杀的用户对于短暂延迟知晓秒杀的结果也是有一定容忍度的。
* “短暂”延迟需要合理的评估，使用消息队列应对流量峰值时，**需要对队列处理的时间、前端写入流量的大小、数据库处理能力做好评估**，然后根据不同的量级来**决定部署多少台队列处理程序**。



###### 通过异步处理简化秒杀请求中的业务流程

* 可以实现异步处理来简化秒杀请求中的业务流程，提升系统的性能。
* 这里面会有主要的业务逻辑，也会有次要的业务逻辑：比如说，**主要的流程是生成订单、扣减库存**；**次要的流程可能是我们在下单购买成功之后会给用户发放优惠券，会增加用户的积分**。
* 经过将一些业务流程异步处理之后，我们的秒杀系统部署结构也会有所改变：
* ![img](https://static001.geekbang.org/resource/image/3b/aa/3b19c4b5e93eeb32fd9665e330e6efaa.jpg?wh=1142*282)
* 假如发放优惠券的耗时是 50ms，增加用户积分的耗时也是 50ms，那么如果**我们将发放优惠券、增加积分的操作放在另外一个队列处理机中执行**，那么整个流程就缩短到了 400ms，性能提升了 20%， 使用 8 个队列程序处理秒杀活动的整个流程。
* 



###### 解耦实现秒杀系统模块之间松耦合

* 我们可以考虑使用消息队列降低业务系统和数据系统的直接耦合度。
* 秒杀系统产生一条购买数据后，我们可以先把全部数据发送给消息队列，然后**数据团队再订阅这个消息队列的话题**，这样它们就**可以接收到数据，然后再做过滤和处理了**。
* 秒杀系统在这样解耦合之后，**数据系统的故障就不会影响到秒杀系统了**，同时当数据系统需要新的字段时，只需要解析消息队列中的消息，拿到需要的数据就好了。
* ![img](https://static001.geekbang.org/resource/image/6e/f6/6e096e287f2c418f663ab201f435a5f6.jpg?wh=1142*356)
* **异步处理、解耦合和削峰填谷**是消息队列在秒杀系统设计中起到的主要作用，其中异步处理可以简化业务流程中的步骤，提升系统性能；削峰填谷可以削去到达秒杀系统的峰值流量，**让业务逻辑的处理更加缓和**；解耦合可以将秒杀系统和数据系统解耦开，这样两个系统的任何变更都不会影响到另一个系统，



###### 总结

* 削峰填谷是消息队列最主要的作用，但是会造成请求处理的延迟。
* 异步处理是提升系统性能的神器，但是你需要**分清同步流程和异步流程的边界**，同时消息存在着丢失的风险，我们需要考虑如何确保消息一定到达。
* 解耦合可以提升你的整体系统的鲁棒性。













##### 如何保证只消费一次

![img](https://static001.geekbang.org/resource/image/c9/a6/c9f44acbc4025b2ff1f0a4b9fd0941a6.jpg?wh=1142*634)

* 消息执行的基本流程：

* > * 消息从生产者写入到消息队列的过程；
  > * 消息在消息队列中的存储场景；
  > * 消息被消费者消费的过程。
  > * ![img](https://static001.geekbang.org/resource/image/87/ea/876fe51e24a49b9751588cc0f8e106ea.png?wh=2510x938)

###### 消息丢失

* **网络中丢失**:

* > * **消息的生产者一般是我们的业务服务器，消息队列是独立部署在单独的服务器上的**。两者之间的网络虽然是内网但是也会存在抖动的可能，而一旦发生抖动，消息就有可能因为网络的错误而丢失。
  > * 解决方案：**消息重传**(可能重复消费）。也就是当你发现发送超时后就将消息重新发一次，但也不能无限制地重传消息。一般来说，如果不是消息队列发生故障或者是到消息队列的网络断开了，重试 2～3 次就可以了。

* 在**消息队列中丢失**消息:

* > * 拿 Kafka 举例，**消息在 Kafka 中是存储在本地磁盘上的**，而为了减少消息存储时对磁盘的随机 I/O，我们一般会将**消息先写入到操作系统的 Page Cache 中**，然后再找合适的时机刷新到磁盘上。
  >
  > * Kafka 可以**配置当达到某一时间间隔或者累积一定的消息数量的时候再刷盘**，也就是所说的**异步刷盘**。
  >
  > * 不过如果发生机器掉电或者机器异常重启，**Page Cache 中还没有来得及刷盘的消息就会丢失了**。
  >
  > * 如果你的电商系统对消息丢失的容忍度很低，你可以考虑**以集群方式部署 Kafka 服务，通过部署多个副本备份数据保证消息尽量不丢失**。
  >
  > * ![img](https://static001.geekbang.org/resource/image/64/3f/648951000b3c7e969f8d04e42da6ac3f.jpg?wh=1142*628)
  >
  > * 当设置“acks=all”时，需要同步执行 1、3、4 三个步骤，对于消息生产的性能来说也是有比较大的影响的，所以你在实际应用中需要仔细地权衡考量。
  >
  > * 合理方案：
  >
  > * > * **如果你需要确保消息一条都不能丢失**，那么建议**不要开启消息队列的同步刷盘**，而是用集群的方式来解决，可以配置当所有 ISR Follower 都接收到消息才返回成功。
  >   > * **如果对消息的丢失有一定的容忍度**，那么建议不部署集群，即使以集群方式部署，也建议配置只发送给一个 Follower 就可以返回成功了。
  >   > * 我们的**业务系统一般对于消息的丢失有一定的容忍度**，比如说以上面的红包系统为例，如果红包消息丢失了，我们只要后续给没有发送红包的用户补发红包就好了。

* 在**消费的过程中存在消息丢失**的可能:

* 消费的过程分为三步：**接收消息、处理消息、更新消费进度**。

* 一定要等到**消息接收和处理完成后**才能更新消费进度.



###### 如何保证消息只被消费一次

* **kafka本身只能保证生产的幂等，消费的幂等需要消费者来保证。**

* 避免消息丢失我们需要付出两方面的代价：**一方面是性能的损耗（能接受），一方面可能造成消息重复消费**（严重）。

* **消息一旦被重复消费就会造成业务逻辑处理的错误**。

* 只要保证**即使消费到了重复的消息**，**从消费的最终结果来看和只消费一次是等同的就好了**，也就是保证在消息的生产和消费的过程是“幂等”的。

* 幂等： 意思是多次执行相同的操作，结果都是「相同」的。

* 生产者端： 

* > * 它的做法是给每一个生产者一个唯一的 ID，并且为生产的每一条消息赋予一个唯一 ID，**消息队列的服务端会存储 < 生产者 ID，最后一条消息 ID>** 的映射。
  > * 当某一个生产者产生新的消息时，消息队列服务端会比对**消息 ID 是否与存储的最后一条 ID 一致**，如果**一致就认为是重复的消息**，服务端会**自动丢弃**。
  > * ![img](https://static001.geekbang.org/resource/image/aa/bd/aab832cee23258972c41e03493b8e0bd.jpg?wh=1142*497)

* 消费者端：

* > * 而在消费端，幂等性的保证会稍微复杂一些，你可以从**通用层和业务层**两个层面来考虑。
  >
  > * 在**通用层面**，你可以在消息被生产的时候使用发号器给它生成一个全局唯一的消息 ID，消息被处理之后把这个 ID 存储在数据库中，在处理下一条消息之前先从数据库里面查询这个全局 ID 是否被消费过，如果被消费过就放弃消费。
  >
  > * 无论是生产端的幂等性保证方式还是消费端通用的幂等性保证方式，它们的共同特点都是为**每一个消息生成一个唯一的 ID**，然后在使用这个消息的时候，**先比对这个 ID 是否已经存在，如果存在则认为消息已经被使用过**。所以这种方式是一种标准的实现幂等的方式.
  >
  > * ```cpp
  >   
  >   boolean isIDExisted = selectByID(ID); // 判断ID是否存在
  >   if(isIDExisted) {
  >     return; //存在则直接返回
  >   } else {
  >     process(message); //不存在，则处理消息
  >     saveID(ID);   //存储ID
  >   }
  >   ```
  >
  > * **在业务层面怎么处理呢**？这里有很多种处理方式，其中有一种是增加乐观锁的方式。比如你的消息处理程序需要给一个人的账号加钱，那么你可以通过乐观锁的方式来解决。
  >
  > * 你给每个人的账号数据中增加一个版本号的字段，在生产消息时先查询这个账户的版本号，并且将**版本号连同消息一起发送给消息队列**。消费端在拿到消息和版本号后，在执行更新账户金额 SQL 的时候带上版本号，类似于执行：
  >
  > * ```sql
  >   
  >   update user set amount = amount + 20, version=version+1 where userId=1 and version=1;
  >   ```
  >
  > * 



###### 总结

* 消息的丢失可以通过**生产端的重试、消息队列配置集群模式以及消费端合理处理消费进度**三种方式来解决；
* 为了解决消息的丢失通常会造成**性能上的问题以及消息的重复问题**；
* 通过保证**消息处理的幂等性**可以**解决消息的重复问题**。







##### 如何降低消息延迟



###### 如何监控消息延迟

* 使用消息队列提供的工具，通过监控消息的堆积来完成；
* 通过生成监控消息的方式来监控消息的延迟情况。

###### 减少消息延迟的正确姿势

* 需要在**消费端和消息队列**两个层面来完成。

* **优化消费代码提升性能**；

* **增加消费者的数量**（这个方式比较简单）。

* > * **受限于消息队列的实现**。如果消息队列使用的是 Kafka 就**无法通过增加消费者数量的方式来提升消息处理能力**。
  > * Kafka 中，**一个 Partition 中的数据，只能被一个消费者所消费**，所以**增加消费者的数量而不增加 Partition 的数量**是不能够提升消费的并行度的，也就不能提升消费的性能。
  > * 如果有多个 consumer（消费者）可以**消费一个分区的数据**，那么在操作这个消费进度的时候就需要**加锁**，可能会对性能有一定的影响。
  > * **话题的分区数量决定了消费的并行度**，增加多余的消费者也是没有用处的，你可以通过增加分区来提高消费者的处理能力。
  > * ![img](https://static001.geekbang.org/resource/image/cd/39/cdd960f49f982f8b96ab466d7e4b2739.jpg?wh=1142*495)
  > * 

* **消息队列本身**在读取性能优化方面做了哪些事情:

* 消息的存储 : **数据库是随机写，磁盘是顺序写**，顺序写比随机写要快很多。

* 我最初在设计的时候为了实现简单，使用了普通的数据库来存储消息，但是**受限于数据库的性能瓶颈**，读取 QPS 只能到 2000，后面我重构了存储模块，**使用本地磁盘作为存储介质**。**Page Cache 的存在就可以提升消息的读取速度**，即使要读取磁盘中的数据，由于消息的读取是顺序的并且不需要跨网络读取数据，所以读取消息的 QPS 提升了一个数量级。

* 零拷贝技术: 零拷贝，其实我们不可能消灭数据的拷贝，只是**尽量减少拷贝的次数**。

* 读取消息队列的数据的时候，其实就是把**磁盘中的数据通过网络发送给消费客户端**，在实现上会有四次数据拷贝的步骤：

* > * 数据从磁盘拷贝到内核缓冲区；
  > * 系统调用将内核缓存区的数据拷贝到用户缓冲区；
  > * 用户缓冲区的数据被写入到 Socket 缓冲区中；
  > * 操作系统再将 Socket 缓冲区的数据拷贝到网卡的缓冲区中。
  > * ![img](https://static001.geekbang.org/resource/image/52/8f/52c74ecac57e7a437442860029476d8f.jpg?wh=1142*383)
  > * 操作系统提供了 **Sendfile 函数可以减少数据被拷贝的次数**。
  > * 使用了 Sendfile 之后，在内核缓冲区的数据不会被拷贝到用户缓冲区而是**直接被拷贝到 Socket 缓冲区**(内核里面又一次拷贝），节省了一次拷贝的过程提升了消息发送的性能。
  > * sendfile 在linux 2.1的时候，是**文件与socket两个内核缓冲区间copy** ，在更高linux版本，已经能从**文件缓冲区直接写到网卡了，socket缓冲区省掉了**
  > * ![img](https://static001.geekbang.org/resource/image/e3/ed/e38d36c7f077c6ce5b0b276efb8d4eed.jpg?wh=1142*488)



###### 总结

* 我们可以使用消息队列提供的工具，或者通过发送监控消息的方式来监控消息的延迟情况；
* **横向扩展消费者**是提升消费处理能力的重要方式；
* 选择高性能的数据存储方式配合零拷贝技术，可以提升消息的消费性能。



##### 如何保证消息有序性

* 要想实现消息有序，**需要从 Producer 和 Consumer 两方面来考虑。**（均满足有序才可以）
* 首先，Producer 生产消息的时候就必须要有序。
* 然后，Consumer 消费的时候，也要按顺序来，不能乱。

###### Producer 有序

* ![img](https://segmentfault.com/img/remote/1460000040004349)

* 像 RabbitMQ 这类普通的消息系统，队列结构简单，Producer 向队列中发送消息就完了，**进入队列的消息肯定是有序的**。

* ![img](https://segmentfault.com/img/remote/1460000040004350)

* Kafka 比较特殊，因为它的一个 **Topic（就是队列的概念）**实际上分为了多个 Partition。

  **Producer 发送消息的时候，是分散在不同 Partition 的。**

  Producer 按顺序发消息，但进入 Kafka Topic 之后，这些消息就不一定进到哪个 Partition 了，所以**顺序肯定是乱的**。

* ![img](https://segmentfault.com/img/remote/1460000040004351)

* 如果想 Topic 内的消息**全局有序**，就只能设置一个 Partition 了，这就变成了 RabbitMQ 那种结构。

* 但这种结构不符合 Kafka 的设计理念，**Topic 只有一个 Partition 就失去了扩展性**。

* kafka 还支持一种**局部有序**的方式，就是把某一类的消息都**放入同一个 Partition，就保证了这组消息的顺序**。

* **Partition Key 一样的消息肯定是在一起的**。

* ![img](https://segmentfault.com/img/remote/1460000039956308)

* 例如使用用户 ID 做 key，这样**同一个用户的消息肯定是在一起的**，就保证了这一组的消息的有序。

###### Consumer 有序

* MQ 内消息有序，那么 Consumer 自然也是按顺序接收的。(只能是**单一消费者才可以**)
* 如果使用了**多个 Consumer**，就可能出现乱序。
* ![img](https://segmentfault.com/img/remote/1460000040004352)
* 例如 RabbitMQ 的一个 Queue 有 3个 Consumer，虽然会按顺序接收到消息，但是它们**各自的处理速度是不同的，所以，出来的结果很可能是乱序的**。
* **如果想严格按顺序来，就只能使用一个 Consumer**。
* 如果可以使用**局部有序**，那么就把之前的一个队列拆为多个队列，就像 Kafka 的 Partition Key 一样，把同组数据放入同一个队列。
* ![img](https://segmentfault.com/img/remote/1460000040004353)
* Kafka 中**一个 Partition 只能对应一个 Consumer**，但如果 Consumer 使用了多线程，就和多个 Consumer 一个效果了，还是**会造成乱序**。
* ![img](https://segmentfault.com/img/remote/1460000040004354)
* 为每个线程创建一个内存队列，Consumer 收到消息后，把**同组的消息都放在同一个内存队列**，由**同一个线程处理即可**。

###### 总结

* **消息的有序需要 Producer 和 Consumer 都有序。**
* RabbitMQ 的队列结构简单，Producer 发送的消息是有序的。但 Kafka 特殊，一个 Topic 有多个 Partition，如果要求全局有序，就只能使用一个 Partition。
* 如果可以接受局部有序，就可以为消息设置 Partition Key，其 Hash 计算结果相同的消息都会在同一个 Partition。
* Consumer 消费时需要注意多 Consumer 的情况，例如多个消费线程。
* 可以在 **Consumer 收到消息后再细化分组**，同组的消息交给同一个消费线程处理。









#### 分布式服务篇

##### 系统架构服务拆分

* 目的： **支撑过百万的 DAU**

* ![img](https://static001.geekbang.org/resource/image/61/e7/612173bc83b332bef201e4ad7056f5e7.jpg?wh=1142*661)

* 电商系统中的**订单模块、用户模块、支付模块、物流模块等等都被打包到一个大的 Web 工程中**，然后部署在应用服务器上。

* 一体化架构的特点：

* > * 开发简单直接，代码和项目集中式管理；
  > * 只需要维护一个工程，节省维护系统运行的人力成本；
  > * 排查问题的时候，只需要排查这个应用进程就可以了，目标性强。

* 缺点：

* > * 代码部署在一起，每个人都向同一个代码库提交代码，代码冲突无法避免；
  > * 同时**功能之间耦合严重**，可能你只是更**改了很小的逻辑却导致其它功能不可用**，从而在测试时需要对整体功能回归，延长了交付时间。
  > * 模块之间互相依赖，一个**小团队中的成员犯了一个错误，就可能会影响到其它团队维护的服务**，对于**整体系统稳定性**影响很大。
  > * 当你的系统扩充到几十万行甚至上百万行代码的时候，一次构建的过程包括**编译、单元测试、打包和上传到正式环境**，**花费的时间可能达到十几分钟**，并且任何小的修改，都需要构建整个项目，**上线变更的过程非常不灵活**。

* 虽然我们做了业务池的拆分，但实际上每一个业务池子都需要连接**用户库并且请求量都很大**，这就造成了**用户库的连接数比其它都要多一些，容易成为系统的瓶颈**。

* ![img](https://static001.geekbang.org/resource/image/94/11/9417a969ce19be3e70841b8d51cf8011.jpg?wh=1142*447)

* 其实可以把与**用户相关的逻辑部署成一个单独的服务**，其它无论是用户池、内容池还是互动池都连接这个服务来获取和更改用户信息，也就是说只有这个服务可以连接用户库，**其它的业务池都不直连用户库获取数据**。

* **这个服务只处理和用户相关的逻辑**，所以不需要部署太多的实例就可以承担流量，这样就可以**有效地控制用户库的连接数，提升了系统的可扩展性**。

* 拆分前，多**个业务多个连接池连接用户库**。拆分后只有**用户服务连接数据库这个连接池**，**用户服务部署的机器越少，连接池就越少，能达到最佳复用，自然就能降低用户库的瓶颈**。

* **![img](https://static001.geekbang.org/resource/image/89/f9/897bcb5e27c6492484b625fc06599ff9.jpg?wh=1142*698)**

* 如果每一个模块都实现这么一套逻辑就会导致代码不够重用。因此我们可以把将 IP 信息或者经纬度信息转换为城市信息，包装成单独的服务供其它模块调用，也就是**我们可以将与业务无关的公用服务抽取出来，下沉成单独的服务**。



###### 总结

* 系统中使用的资源出现扩展性问题，尤其是数据库的连接数出现瓶颈；
* 大团队共同维护一套代码，带来研发效率的降低和研发成本的提升；
* 系统部署成本越来越高。



如果没有分库的话，微服务主要还是解决**高可用的问题**：

* 微服务要和网关、服务治理服务发现框架结合使用，这保证了请**求只会分发到可用的节点**。
* 相对于soa，**微服务的的粒度更小，流量大起来了受影响的范围也会变小**，比如我soa有个oms系统单独部署，有查询订单和插入订单两个接口，当查询量激增的情况下，很有可能因为查询耗尽了机器资源，导致插入的接口也不可用，如果用微服务，查询和插入分别单独部署，就不会相互影响，解耦的粒度更细了。
* 横向拓展机器资源分配更精准的问题，同样是上面的例子，查询激增如果soa的横向拓展，拓展出来的实例资源可能要同时用对插入和查询，如果是微服务我们只需要拓展查询接口的实例，能精准的把资源分配给查询。





##### 微服务改造

* 微服务化之后垂直电商系统的架构将会变成下面这样：

* ![img](https://static001.geekbang.org/resource/image/1d/e9/1d5f1212017c6c22818e413ab74f88e9.jpg?wh=1142*791)

* 基本原则：

* > * 原则一，做到单一服务内部功能的高内聚和低耦合。
  > * 原则二，你需要关注服务拆分的粒度，先粗略拆分再逐渐细化。
  > * 原则三，拆分的过程，要尽量避免影响产品的日常功能迭代。
  > * 原则四，服务接口的定义要具备可扩展性。
  >
  > 

* 剥离的顺序你可以参考以下几点：

* > * **优先剥离比较独立的边界服务**（比如短信服务、地理位置服务），从非核心的服务出发减少拆分对现有业务的影响，也给团队一个练习、试错的机会；
  > * **当两个服务存在依赖关系时优先拆分被依赖的服务**。比如内容服务依赖于用户服务获取用户的基本信息，那么如果先把内容服务拆分出来，内容服务就会依赖于一体化架构中的用户模块，这样还是无法保证内容服务的快速部署能力。







##### RPC框架

* 系统做了服务化拆分之后会引入一些新的问题:

* > * 服务拆分单独部署后，引入的服务跨**网络通信的问题**；
  > * 在拆分成多个小服务之后，服务如何治理的问题。

* 场景： 你的垂直电商系统的 QPS 已经达到了每秒 2 万次，在做了服务化拆分之后，由于我们把**业务逻辑都拆分到了单独部署的服务中**，那么假设你在完成**一次完整的请求时需要调用 4～5 次服务**，计算下来，**RPC 服务需要承载大概每秒 10 万次的请求**。而你该如何设计 RPC 框架承载如此大的请求量呢？

* > * 选择**合适的网络模型**，有针对性地调整网络参数优化网络传输性能；
  > * 选择**合适的序列化方式**，以提升封包、解包的性能。

* RPC 并不是互联网时代的产物，也不是**服务化之后才衍生出来的技术**，而是一种规范，只要是**封装了网络调用的细节能够实现远程调用其他服务**，就可以算作是一种 RPC 技术了。

* **性能**上的变化是不可忽视的，我给你举个例子。 比方说，你的电商系统中商品详情页面需要商品数据、评论数据还有店铺数据，如果在一体化的架构中，你只需要从商品库、评论库和店铺库获取数据就可以了，不考虑缓存的情况下有**三次网络请求**。

* ![img](https://static001.geekbang.org/resource/image/1d/ce/1dba9b34e2973ec185b353becfc64fce.jpg?wh=1142*392)

* 但是如果独立出商品服务、评论服务和店铺服务之后，那么就需要分别**调用这三个服务**，而**这三个服务又会分别调用各自的数据库**，这就是**六次网络请求**。

* 如何优化性能：

* > * 在一次 RPC 调用过程中，客户端首先会将调用的**类名、方法名、参数名、参数值**等信息，序列化成二进制流；
  > * 然后客户端将二进制流通过网络发送给服务端；
  > * 服务端接收到二进制流之后将它反序列化，得到需要调用的类名、方法名、参数名和参数值，再**通过动态代理的方式调用对应的方法得到返回值**；
  > * 服务端将**返回值序列化**，再通过**网络发送给客户端**；
  > * 客户端对结果反序列化之后，就可以得到调用的结果了。
  > * ![img](https://static001.geekbang.org/resource/image/f9/3e/f98bd80af8a4e7258251db1084e0383e.jpg?wh=1142*330)

* **网络传输和序列化**两方面来优化



###### 如何提升网络传输性能

* **选择一种高性能的 I/O 模型**。所谓 I/O 模型，就是我们处理 I/O 的方式

* 首先，I/O 会经历一个**等待资源的阶段**，比方说，等待网络传输数据可用，在这个过程中我们对 I/O 会有两种处理方式：

* > * 阻塞。指的是**在数据不可用时 I/O 请求一直阻塞，直到数据返回**；
  > * 非阻塞。指的是数据**不可用时 I/O 请求立即返回**，直到**被通知资源可用为止**。

* 然后是**使用资源的阶段**，比如说从网络上接收到数据，并且拷贝到应用程序的缓冲区里面。在这个阶段我们也会有两种处理方式：

* > * **同步处理**。指的是 I/O 请求在**读取或者写入数据时会阻塞**，直到读取或者写入数据完成；
  > * **异步处理**。指的是 I/O 请求在**读取或者写入数据时立即返回**，当操作系统处理完成 I/O 请求并且将数据拷贝到用户提供的缓冲区后，再通知应用 I/O 请求执行完成。

* 常见的 5 种 IO

* > * 同步阻塞 I/O；
  > * 同步非阻塞 I/O；
  > * 同步多路 I/O 复用；
  > * 信号驱动 I/O；
  > * 异步 I/O。

* 我们把 I/O 过程比喻成烧水倒水的过程，**等待资源（就是烧水的过程）**，**使用资源（就是倒水的过程）**：

* > * 如果你站在灶台边上一直等着（等待资源）水烧开，然后倒水（使用资源），那么就是同步阻塞 I/O；
  > * 如果你偷点儿懒，在烧水的时候躺在沙发上看会儿电视（不再时时刻刻等待资源），但是还是要时不时地去看看水开了没有，一旦水开了，马上去倒水（使用资源），那么这就是同步非阻塞 I/O；
  > * 如果你想要洗澡，需要同时烧好多壶水，那你就在看电视的间隙去看看哪壶水开了（等待多个资源），哪一壶开了就先倒哪一壶，这样就加快了烧水的速度，这就是同步多路 I/O 复用；
  > * 不过你发现自己总是跑厨房去看水开了没，太累了，于是你考虑给你的水壶加一个**报警器（信号）**，只要水开了就马上去倒水，这就是信号驱动 I/O；
  > * 最后一种就高级了，你发明了一个智能水壶，在水烧好后自动就可以把水倒好，这就是异步 I/O。

* **网络参数的调优**也是非常重要的一部分， 例如： 如果你要自己实现一套网络框架，**tcp_nodelay 这个参数最好是要开启的**。





###### 如何提升序列化性能

* 一次 RPC 调用需要经历**两次数据序列化的过程和两次数据反序列化**的过程，可见它们对于 RPC 的性能影响是很大的。

* 那么我们在选择序列化方式的时候需要考虑哪些因素呢？

* > * 性能包括**时间上的开销和空间上的开销**
  > * 时间上的开销就是**序列化和反序列化的速度**，这是显而易见需要重点考虑的
  > * 空间上的开销则是序列化后的二进制串的大小，过大的二进制串也会占据传输带宽影响传输效率。
  > * 我们需要考虑的是它**是否可以跨语言、跨平台**



* 序列化方案选择： 

> * 如果对于**性能要求不高，在传输数据占用带宽不大**的场景下可以使用 JSON (简单，传输的是文本类型）作为序列化协议；
> * 如果对于**性能要求比较高**，那么使用 Thrift 或者 Protobuf 都可以。而 Thrift 提供了配套的 RPC 框架，所以想要一体化的解决方案，你可以优先考虑 Thrift；
> * 在一些存储的场景下，比如说你的**缓存中存储的数据占用空间较大**，那么你可以考虑使用 Protobuf 替换 JSON 作为存储数据的序列化方式.

###### 总结

* 选择高性能的 I/O 模型，这里我推荐使用同步**多路 I/O 复用模型**；
* **调试网络参数**，这里面有一些经验值的推荐。比如将 tcp_nodelay 设置为 true，也有一些参数需要在运行中来调试，比如接受缓冲区和发送缓冲区的大小，客户端连接请求缓冲队列的大小（back log）等等；
* **序列化协议**依据具体业务来选择。如果对性能要求不高可以选择 JSON，否则可以从 Thrift 和 Protobuf 中选择其一。











#### 实战篇：计数器设计

* 计数场景：

> * 微博的评论数、点赞数、转发数、浏览数、表态数等等；
> * 用户的粉丝数、关注数、发布微博数、私信数等等。

* 可能的问题：

* > * **数据量巨大**。据我所知，微博系统中微博条目的数量早已经超过了千亿级别，仅仅计算微博的转发、评论、点赞、浏览等核心计数，其数据量级就已经在几千亿的级别。
  > * **访问量大**，对于性能的要求高。微博的日活用户超过 2 亿，月活用户接近 5 亿，核心服务（比如首页信息流）访问量级达到每秒几十万次，计数系统的访问量级也超过了每秒百万级别，而且在性能方面，它要求要毫秒级别返回结果。
  > * **对于可用性、数字的准确性要求高**。一般来讲，用户对于计数数字是非常敏感的，比如你直播了好几个月，才涨了 1000 个粉，突然有一天粉丝数少了几百个，那么你是不是会琢磨哪里出现问题，或者打电话投诉直播平台？

##### 基本设计逻辑：

* > * 可以这么设计表结构：以微博 ID 为主键，转发数、评论数、点赞数和浏览数分别为单独一列，这样在获取计数时用一个 SQL 语句就搞定了。
  >
  > * ```sql
  >   
  >   select repost_count, comment_count, praise_count, view_count from t_weibo_count where weibo_id = ?
  >   ```
  >
  > * 微博用户量和发布的**微博量增加迅猛，计数存储数据量级也飞速增长**，而 MySQL 数据库单表的存储量级达到几千万的时候，性能上就会有损耗。所以我们考虑使用**分库分表的方式分散数据量**，提升读取计数的性能。
  >
  > * 我们用“weibo_id”作为分区键，在选择分库分表的方式时，考虑了下面两种：
  >
  > * > * 一种方式是选择一种**哈希算法对 weibo_id 计算哈希值**，然后**根据这个哈希值计算出需要存储到哪一个库哪一张表中**，具体的方式你可以回顾一下第 9 讲数据库分库分表的内容；
  >   > * **另一种方式是按照 weibo_id 生成的时间来做分库分表**，我们在第 10 讲谈到发号器的时候曾经提到，ID 的生成最好带有业务意义的字段，比如生成 ID 的时间戳。所以在分库分表的时候，可以先依据发号器的算法反解出时间戳，然后按照时间戳来做分库分表，比如，一天一张表或者一个月一张表等等。
  >   > * ![img](https://static001.geekbang.org/resource/image/50/9c/508201de80dd909d8b7dff1d34be9f9c.jpg?wh=1142*306)
  >   > * 因为**越是最近发布的微博，计数数据的访问量就越大**，所以虽然我考虑了两种方案，但是按照时间来分库分表会造成数据访问的不均匀，最后用了**哈希的方式来做分库分表**。
  >
  > * 而**信息流的访问量巨大，仅仅靠数据库已经完全不能承担如此高的并发量了。**(读性能)
  >
  > * 我们考虑使用 Redis 来加速读请求，通过**部署多个从节点来提升可用性和性能**，并且通过 **Hash 的方式对数据做分片**，也基本上可以保证计数的读取性能。
  >
  > * 数据库 + 缓存的方式有一个弊端：无法保证**数据的一致性**，比如，如果**数据库写入成功而缓存更新失败**，就会导致数据的不一致，影响计数的准确性。
  >
  > * 我们完全抛弃了 MySQL，**全面使用 Redis 来作为计数的存储组件**。
  >
  > * 可行的原因是： **因为redis通过rdb和aof来支持持久化，可以通过设置保证至少有一台从redis机器同步了数据**，从redis来做相应的那个持久化操作达到数据不丢失，因为原生的redis数据结构会占用比较多的字节，这里直接进行改造，**让redis的数据结构占用内存加少**。
  >
  > * **![img](https://static001.geekbang.org/resource/image/7c/62/7c8ed7992ec206671a18b8d537eaef62.jpg?wh=1142*599)**
  >
  > * 除了考虑计数的读取性能之外，由于热门微博的计数变化频率相当高，也需要考虑**如何提升计数的写入性能**。（写性能）
  >
  > * 方法：
  >
  > * > * 用**消息队列来削峰填谷了**，也就是说，我们在转发微博的时候向消息队列写入一条消息，然后在**消息处理程序中给这条微博的转发计数加 1**。
  >   >
  >   > * 我们可以通过**批量处理消息的方式进一步减小 Redis 的写压力**(可以等待一段时间将多次更新同步到一次更新中)
  >   >
  >   > * ```sql
  >   >   // 多次合并
  >   >   UPDATE t_weibo_count SET repost_count = repost_count + 1 WHERE weibo_id = 1; 
  >   >   UPDATE t_weibo_count SET repost_count = repost_count + 1 WHERE weibo_id = 1;  
  >   >   UPDATE  t_weibo_count SET repost_count = repost_count +1 WHERE weibo_id = 1; 
  >   >               
  >   >   ->
  >   >               
  >   >   // 一次合并
  >   >   UPDATE t_weibo_count SET repost_count = repost_count + 3 WHERE weibo_id = 1; 
  >   >   ```

##### 如何降低存储成本

* 先对**原生 Redis 做一些改造**，采用新的数据结构和数据类型来存储计数数据。（内存是有限的）

* > * 一是**原生的 Redis 在存储 Key 时是按照字符串类型来存储的**，比如一个 8 字节的 Long 类型的数据，需要 8（sdshdr 数据结构长度）+ 19（8 字节数字的长度）+1（’\0’）=28 个字节，如果我们**使用 Long 类型来存储就只需要 8 个字节，会节省 20 个字节的空间；**
  > * 二是**去除了原生 Redis 中多余的指针**，如果要存储一个 KV 信息就只需要 8（weibo_id）+4（转发数）=12 个字节，相比之前有很大的改进。
  > * 微博计数的数据具有明显的**热点属性**：越是最近的微博越是会被访问到，**时间上久远的微博被访问的几率很小**。
  > * 所以为了尽量减少服务器的使用，我们考虑给计数服务增加 SSD 磁盘，然后将**时间上比较久远的数据 dump 到磁盘上**，内存中只保留最近的数据。
  > * 当我们要读取冷数据的时候，使用**单独的 I/O 线程异步地将冷数据从 SSD 磁盘中加载到一块儿单独的 Cold Cache 中**。
  > * ![img](https://static001.geekbang.org/resource/image/16/93/16cb144c96a0ab34214c966f686c9693.jpg?wh=1142*677)

##### 海量数据总结

* 数据库 + 缓存的方案是计数系统的初级阶段，**完全可以支撑中小访问量和存储量的存储服务**。如果你的项目还处在初级阶段，量级还不是很大，那么你一开始可以考虑使用这种方案。
* 通过对原生 Redis 组件的改造，我们可以极大地减小存储数据的内存开销。
* 使用 SSD+ 内存的方案可以最终解决存储计数数据的成本问题。这个方式适用于**冷热数据明显的场景**，你在使用时需要考虑如何将内存中的数据做换入换出。





##### 如何设计未读数系统

* 基本思路： 可以在计数系统中**增加一块儿内存区域**，以用户 ID 为 Key 存储多个未读数，当有人 @ 你时，增加你的未读 @的计数；

* 当有人评论你时，增加你的未读评论的计数，以此类推。

* 当你**点击了未读数字进入通知页面**，查看 @ 你或者评论你的消息时，**重置这些未读计数为零**。

* 假如你的系统中只有 A、B、C 三个用户，那么你可以在**通用计数系统中增加一块儿内存区域**，并且以用户 ID 为 Key 来存储这三个用户的未读通知数据，当**系统发送一个新的通知时，我们会循环给每一个用户的未读数加 1**，这个处理逻辑的伪代码就像下面这样：

* ```cpp
  List<Long> userIds = getAllUserIds();
  for(Long id : userIds) {
    incrUnreadCount(id);
  }
  ```

* 缺点：

* > * 获取全量用户就是一个比较耗时的操作，相当于**对用户库做一次全表的扫描**，这不仅会对数据库造成很大的压力，而且**查询全量用户数据的响应时间是很长的，对于在线业务来说是难以接受的**。
  > * 如果你的用户库已经做了**分库分表，那么就要扫描所有的库表，响应时间就更长了**。

###### 系统消息未读设计逻辑

* 系统通知实际上是存储在一个大的列表中的，这个列表对所有用户共享，也就是**所有人看到的都是同一份系统通知的数据**。

* 不过**不同的人最近看到的消息不同，所以每个人会有不同的未读数**。

* 你可以记录一下在这个列表中每个人看过最后一条消息的 ID，然后统计这个 ID 之后有多少条消息，这就是未读数了。

* ![img](https://static001.geekbang.org/resource/image/a5/10/a5f0b6776246dc6b4c7e96c72d74a210.jpg?wh=1142*850)

* 关键点： 

* > * 用户访问系统通知页面需要设置未读数为 0，我们需要将用户最近看过的通知 ID 设置为最新的一条系统通知 ID；
  > * 如果最近看过的通知 ID 为空，则认为是一个新的用户，返回未读数为 0；
  > * 对于非活跃用户，比如**最近一个月都没有登录和使用过系统的用户**，可以把用户最近看过的**通知 ID 清空**，节省内存空间。

* 这个方案适用的另一个业务场景是**全量用户打点的场景**，比如像下面这张微博截图中的红点。

* ![img](https://static001.geekbang.org/resource/image/ae/3f/ae6a5e9e04be08d18c493729458d543f.jpg?wh=1142*771)

* 根据时间戳决定是否显示这个 红点。如果用户**时间戳小于全局时间戳**，代表在用户最后一次点击红点之后又有新的红点推送，那么就要展示红点，反之，就不展示红点了。

* ![img](https://static001.geekbang.org/resource/image/55/98/553e7da158a7eca56369e23c9b672898.jpg?wh=1142*560)

* 这两个场景的**共性是全部用户共享一份有限的存储数据**，每个人**只记录自己在这份存储中的偏移量**，就可以得到未读数了。

* 

###### 如何为信息流的未读数设计方案

* 复杂的原因：

* > * 首先，微博的信息流是基于关注关系的，未读数也是基于关注关系的
  > * 就是说，你**关注的人发布了新的微博**，那么你**作为粉丝未读数就要增加 1**
  > * 对于一些动辄几千万甚至上亿粉丝的微博大 V 就麻烦了，**增加未读数可能需要几个小时**。
  > * 杨幂的粉丝，想了解她实时发布的博文，那么如果当她发布博文几个小时之后，你才收到提醒，这显然是不能接受的。所以**未读数的延迟是你在设计方案时首先要考虑的内容**。
  > * 其次，信息流未读数请求量极大、并发极高，这是因为接口是客户端轮询请求的，不是用户触发的。
  > * 也就是说，用户**即使打开微博客户端什么都不做，这个接口也会被请求到**。
  > * 最后，它不像系统通知那样有共享的存储，因为**每个人关注的人不同**，信息流的列表也就不同，所以也就**没办法采用系统通知未读数的方案。**

* 设计思路：(只需要返回未读数的数量就可以，不需要返回所有未读数的具体数据内容)

* > * 首先，在通用计数器中记录每一个用户发布的博文数；
  > * 然后在 Redis 或者 Memcached 中记录一个人**所有关注人的博文数快照**，当用户点击未读消息重置未读数为 0 时，将他关注所有人的博文数刷新到快照中；
  > * 这样，**他关注所有人的博文总数减去快照中的博文总数就是他的信息流未读数**。
  > * ![img](https://static001.geekbang.org/resource/image/a5/8a/a563b121ae1147a2d877a7bb14c9658a.jpg?wh=1142*546)
  > * 这个**方案设计简单**，并且是**全内存操作，性能足够好，能够支撑比较高的并发**，事实上微博团队仅仅用 16 台普通的服务器就支撑了每秒接近 50 万次的请求，这就足以证明这个方案的性能有多出色，因此，它完全能够满足信息流未读数的需求。
  > * **redis单机能抗10W**，具体测试数据取决于分配的内存大小（不宜超过10G），CPU和磁盘



* 设计建议：

* > * **缓存是提升系统性能和抵抗大并发量的神器**，像是微博信息流未读数这么大的量级我们仅仅使用十几台服务器就可以支撑，这全都是缓存的功劳；
  > * 要围绕系统设计的关键困难点想解决办法，就像我们解决系统通知未读数的延迟问题一样；
  > * 合理分析业务场景，**明确哪些是可以权衡的，哪些是不行的，会对你的系统设计增益良多**，比如对于长久不登录用户，我们就会记录未读数为 0，通过这样的权衡，可以极大地减少内存的占用，减少成本。



###### 总结

* 评论未读、@未读、赞未读等一对一关系的未读数可以使用上节课讲到的通用计数方案来解决；
* 在系统通知未读、全量用户打点等存在**有限的共享存储的场景下**，可以**通过记录用户上次操作的时间或者偏移量**，来实现未读方案；
* 信息流未读方案最为复杂，采用的是**记录用户博文数快照的方式**。



##### 通用信息流系统的推模式

* 最早的信息流系统起源于微博，我们知道，微博是基于**关注关系来实现内容分发的**，也就是说，如果用户 A 关注了用户 B，那么用户 A 就需要在自己的信息流中，实时地看到用户 B 发布的最新内容，这是微博系统的基本逻辑，也是**它能够让信息快速流通、快速传播的关键**。

###### 设计信息流系统关注的点

* 首先，我们需要**关注延迟数据**，也就是说，你关注的人发了微博信息之后，信息需要在短时间之内出现在你的信息流中。
* 其次，我们需要**考虑如何支撑高并发的访问**。信息流是微博的主体模块，是用户进入到微博之后最先看到的模块，因此它的**并发请求量是最高的，可以达到每秒几十万次请求**。
* 最后，**信息流拉取性能直接影响用户的使用体验。**微博信息流系统中需要聚合的数据非常多，你打开客户端看一看，想一想其中需要聚合哪些数据？主要是微博的数据，用户的数据，除此之外，还需要查询微博是否被赞、评论点赞转发的计数、是否被关注拉黑等等。
* 如何保证**在 100ms 之内完成这些查询操作，展示微博的信息流呢？**这是微博信息流系统最复杂之处，也是技术上最大的挑战。



###### 如何基于推模式实现信息流系统

* Q: 什么是推模式呢？

* A： 推模式是指**用户发送一条微博后，主动将这条微博推送给他的粉丝**，从而实现微博的分发，也能以此实现微博信息流的聚合。

* 假如用户 A 有三个粉丝 B、C、D，如果用 SQL 表示 A 发布一条微博时系统做的事情，那么就像下面展示的这个样子：

* ```sql
  insert into outbox(userId, feedId, create_time) values("A", $feedId, $current_time); //写入A的发件箱
  insert into inbox(userId, feedId, create_time) values("B", $feedId, $current_time); //写入B的收件箱
  insert into inbox(userId, feedId, create_time) values("C", $feedId, $current_time); //写入C的收件箱
  insert into inbox(userId, feedId, create_time) values("D", $feedId, $current_time); //写入D的收件箱
  ```

* 当我们**要查询 B 的信息流时，只需要执行下面这条 SQL 就可以了**：

* ```sql
  select feedId from inbox where userId = "B";
  ```



###### 存在的问题和解决思路

* 问题：

* > * 首先，**就是消息延迟。**对明星来说，他们的粉丝数庞大，如果在发微博的同时还要将微博写入到上千万人的收件箱中，那么发微博的响应时间会非常慢，用户根本没办法接受。
  >
  > * 因此，我们**一般会使用消息队列来消除写入的峰值**，但即使这样，由于写入收件箱的消息实在太多，你还是有**可能在几个小时之后才能够看到明星发布的内容，这会非常影响用户的使用体验**。
  >
  > * ![img](https://static001.geekbang.org/resource/image/c2/b0/c2e64231a2b6c52082567f8422069cb0.jpg?wh=1142*474)
  >
  > * 为了尽量减少微博写入的延迟，我们可以从两方面来保障:
  >
  > * > * 一方面，在消息处理上，你可以**启动多个线程并行地处理微博写入的消息**。
  >   > * 另一方面，由于消息流在展示时可以**使用缓存来提升读取性能**，所以我们应该**尽量保证数据写入数据库的性能**，必要时可以采用写入性能更好的数据库存储引擎。
  >
  > * 其次，**存储成本很高**
  >
  > * 另外一张表是**用户的发件箱和收件箱表**，也叫做 TimeLine 表（时间线表），主要有三个字段，用户 ID、微博 ID 和创建时间。它使用用户的 ID 做哈希分库分表。
  >
  > * ![img](https://static001.geekbang.org/resource/image/71/6c/71b4b33d966a7e34a62f635a1a23646c.jpg?wh=1142*678)
  >
  > * 由于推模式需要给**每一个用户都维护一份收件箱的数据**，所以数据的存储量极大
  >
  > * 谢娜的粉丝目前已经超过 1.2 亿，那么如果**采用推模式的话**，谢娜每发送一条微博就会产生超过 1.2 亿条的数据.(数据量太大了)
  >
  > * 解决思路：
  >
  > * > * 除了**选择压缩率更高的存储引擎之外**，还可以**定期地清理数据**，因为用户更加关注最近几天发布的数据，通常不会翻阅很久之前的微博，所以你可以定期地清理用户的收件箱，比如只保留最近 1 个月的数据就可以了。

* Q:推模式究竟适合什么样的业务的场景呢？

* A： 它比较适合于一个**用户的粉丝数比较有限的场景**，比如说微信朋友圈，你可以理解为我在微信中增加一个好友是关注了他也被他关注，所以好友的上限也就是粉丝的上限（朋友圈应该是 5000）。**有限的粉丝数可以保证消息能够尽量快地被推送给所有的粉丝**，增加的存储成本也比较有限。



###### 总结

* 推模式就是在用户发送微博时，主动将微博写入到他的粉丝的收件箱中
* 推送信息**是否延迟、存储的成本、方案的可扩展性**以及针对取消关注和微博删除的特殊处理是推模式的主要问题；
* 推模式比较适合**粉丝数有限的场景**。





##### 通用信息流系统的拉模式

* 所谓拉模式，就是指**用户主动拉取他关注的所有人的微博**，将这些微博按照发布时间的倒序进行排序和聚合之后，生成信息流数据的方法。

###### 如何使用拉模式设计信息流系统

* 用户的收件箱不再有用，因为信息流数据不再出自收件箱，而是出自发件箱。

* **发件箱里是用户关注的所有人数据的聚合。**

* 因此用户在发微博的时候就只需要**写入自己的发件箱**，而不再需要推送给粉丝的收件箱了，这样在获取信息流的时候，就要查询发件箱的数据了。

* > * 这个逻辑我还用 SQL 的形式直观地表达出来，方便你理解。假设用户 **A 关注了用户 B、C、D**，那么当用户 B 发送一条微博的时候，他会执行这样的操作：
  >
  > * ```sql
  >   insert into outbox(userId, feedId, create_time) values("B", $feedId, $current_time); //写入B的发件箱
  >   ```
  >
  > * 当**用户 A 想要获取他的信息流的时候**，就要**聚合 B、C、D 三个用户收件箱**的内容了：
  >
  > * ```sql
  >   select feedId from outbox where userId in (select userId from follower where fanId = "A") order by create_time desc
  >   ```

* 拉模式的优势：

* > * 拉模式彻底解决了**推送延迟的问题**，**大 V 发微博的时候不再需要推送到粉丝的收件箱**，自然就不存在延迟的问题了。
  >
  > * **存储成本大大降低了**。在推模式下，谢娜的粉丝有 1.2 亿，那么谢娜发送一条微博就要被复制 1.2 亿条，写入到存储系统中。在**拉模式下只保留了发件箱，微博数据不再需要复制**，成本也就随之降低了。
  >
  > * **功能扩展性更好了**, 微博增加了分组的功能，而你想把**关注的 A 和 B 分成一个单独的组**，那么 A 和 B 发布的微博就形成了一个新的信息流. 你**只需要查询这个分组下所有用户（也就是 A 和 B）**，然后**查询这些用户的发件箱**，再把发件箱中的数据，按照时间倒序重新排序聚合就好了。
  >
  > * ```sql
  >   List<Long> uids = getFromGroup(groupId); //获取分组下的所有用户
  >   Long<List<Long>> ids = new ArrayList<List<Long>>();
  >   for(Long id : uids) {
  >     ids.add(getOutboxByUid(id)); //获取发件箱的内容id列表
  >   }
  >   return merge(ids); //合并排序所有的id
  >   ```

* 缺陷：

* > * 1. 在拉模式下，我们需要**对多个发件箱的数据做聚合**，这个查询和聚合的成本比较高。
  > * Q : 如何保证在毫秒级别完成这些信息的查询和聚合呢？
  > * A : 缓存。
  > * 可以把用户发布的**微博 ID 放在缓存中**，不过如果把全部用户的所有微博都缓存起来，消耗的硬件成本也是很高的。（**缓存中只放微博ID，实际内容还要去DB查**）
  > * 我们对用户的浏览行为做了分析，发现 97% 的用户都是在浏览最近 5 天之内的微博，也就是说，用户很少翻看五天之前的微博内容，所以我们只缓存了每个用户最近 5 天发布的微博 ID。
  > * 假设我们**部署 6 个缓存节点**来存储这些微博 ID，在每次聚合时并行从**这几个缓存节点中批量查询多个用户的微博 ID**，获取到之后再在**应用服务内存中排序**后就好了，这就是**对缓存的 6 次请求**，可以保证在 5 毫秒之内返回结果。
  > * 2. **缓存节点的带宽成本比较高**，
  > * **部署多个缓存副本**提升缓存可用性： 在部**署缓存副本之后，请求会先查询副本中的数据**，只有不命中的请求才会查询主缓存的数据。
  > * ![img](https://static001.geekbang.org/resource/image/67/3a/679c081c73c30ccc6dafc3f2cae0a13a.jpg?wh=1142*713)



###### 推拉结合的方案

* 当用户的粉丝量大量上涨之后，我们通过对原有系统的改造实现了一套推拉结合的方案，也能够基本解决推模式存在的问题。
* 思路： 方案的核心在于大 V 用户在发布微博的时候，**不再推送到全量用户，而是只推送给活跃的用户**。
* **不活跃用户是不推送，但是不活跃用户上线后会拉取发件箱，所以叫推拉结合**
* ![img](https://static001.geekbang.org/resource/image/4a/55/4a92721244bd0c696abbbe03dafa5955.jpg?wh=1142*674)
* 缺陷：
* 采用推拉结合的方式可以**一定程度上弥补推模式的缺陷**，不过也带来了一些维护的成本，比如说系统**需要维护用户的在线状态**，还需要多维护一套活跃的粉丝列表数据，在**存储上的成本就更高**了。



###### 总结

* 在拉模式下，我们只需要保存用户的发件箱，**用户的信息流是通过聚合关注者发件箱数据来实现的**；
* 拉模式会有比较大的聚合成本，缓存节点也会存在带宽的瓶颈，所以我们可以通过一些权衡策略尽量减少获取数据的大小，以及部署缓存副本的方式来抗并发；
* 推拉结合的模式核心是**只推送活跃的粉丝用户**，需要**维护用户的在线状态以及活跃粉丝的列表**，所以需要增加多余的空间成本来存储，这个你需要来权衡。















#### 设计一个秒杀系统，要求不能超卖并且秒杀的成功率高



![img](https://pica.zhimg.com/80/v2-d882b2f82bcb57a418e7a171c1cf58d9_720w.jpg?source=1940ef5c)

##### **第一步Scenario场景。**

* 需要确定设计哪些功能，承受多大的访问量？
* 假如一个服务的RT是20ms，则QPS为50，这里计算的是单机单线程QPS，如果计算集群的话，需要考虑集群数量和线程数量。
* 如果面试官说峰值大概量级在100万，那么按照服务单线程QPS是50，单台最大线程数按3来计算的话，单台机器最大支撑150的QPS，那么至少需要**100W/150=6667台机器**。
* 常见的组件最大QPS，**mysql单机1000QPS，Redis单机10万QPS**。

##### **第二步，Service服务**。

![img](https://picx.zhimg.com/80/v2-b8be2eb4bf35abcc77b13d7027136113_720w.jpg?source=1940ef5c)





##### **第三步，Storage存储**

* 数据是如何存储和访问的。为每个**服务选择合适的存储结构，然后细化数据表结构**。这个例子中，秒杀系统数据库设计如下。
* ![img](https://pic1.zhimg.com/80/v2-1218318a157b9a2f9ee121901e4998d5_720w.jpg?source=1940ef5c)
* 于是我们可以得到秒杀活动中，数据库之间的关系如图所示
* ![img](https://pic1.zhimg.com/80/v2-8fc3a45b93d02154010974ba764235b1_720w.jpg?source=1940ef5c)
* 接下来就是mysql扣库存了。秒杀系统一定会遇到的就是并发问题，这里说下[乐观锁](https://www.zhihu.com/search?q=乐观锁&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1352510403})和悲观锁。
* 悲观锁：
* ![img](https://pic3.zhimg.com/80/v2-0b238296db1cf727757737212e306fc6_720w.jpg?source=1940ef5c)
* 乐观锁流程如下：
* ![img](https://pica.zhimg.com/80/v2-b040e21e1f096afaf0a7df0ccf1b402b_720w.jpg?source=1940ef5c)
* 可以看到[悲观锁](https://www.zhihu.com/search?q=悲观锁&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1352510403})的问题是会**占用大量的线程资源**，可能**导致mysql的线程耗尽**。
* 在对于**数据一致性要求非常高**的场景中，一般用悲观锁；
* 而乐观锁在**version变动频繁**的情况下则不适用，比如这里的**秒杀系统就不太适合用乐观锁**，因为库存变化太快了。(因为冲突太多了吧)
* Redis 在秒杀系统下的设计
* ![img](https://pica.zhimg.com/80/v2-8c78c0bbfb1235fb8963eec101f29d4c_720w.jpg?source=1940ef5c)

##### 秒杀系统中Redis的常见问题：

* > * **1.什么时候把库存写入到Redis？**
  > * 秒杀活动创建/维护时写入Redis。
  > * **2.如何保证活动数据库和库存数据一致？**
  > * 可以使用分布式事务或消息队列。
  > * **分布式事务：保证多个数据库的操作同时成功或者同时失败**。对强一致性有要求的业务场景可以考虑使用分布式事务，比如**银行转账**
  > * ![img](https://pica.zhimg.com/80/v2-5661fb818fed9109adbf5ea730a159d6_720w.jpg?source=1940ef5c)
  > * **消息队列：**基于生产者/消费者模型的组件，一般实现异步任务（非实时处理）时会引入消息队列。
  > * 消息队列的好处是: **任务可以慢慢处理，不必同步处理等着响应结果**。使用场景除了异步任务之外，一般还用于**失败的情况下重试处理，重复消费直到消费成功**。
  > * ![img](https://pic1.zhimg.com/80/v2-06869e374aa354f2f0161b30ca06a903_720w.jpg?source=1940ef5c)
  > * **3.下单减库存/支付减库存？**
  > * 下单锁定库存，支付减库存。
  > * **4.如何防止商品被超卖？**
  > * 把库存数据放入到缓存中，利用缓存的原子特性保证同时只有一个线程操作库存。
  > * **5.库存写回数据库的时机？**
  > * 采用**定时任务同步Redis**的数据写回数据库。



##### 可能存在的问题：

###### 服务单一职责

* 单独给他建立一个数据库，现在的互联网架构部署都是**分库**的，一样的就是订单服务对应订单库，秒杀我们也给他建立自己的秒杀库。
* 至于表就看大家怎么设计了，该设置索引的地方还是要设置索引的，建完后记得用**explain**看看**SQL**的执行计划。
* 单一职责的好处就是就算秒杀没抗住，秒杀库崩了，服务挂了，也不会影响到其他的服务。



###### 秒杀链接加盐

* 链接要是提前暴露出去可能有人直接访问url就提前秒杀了，那又有小伙伴要说了我**做个时间的校验**就好了呀，那我告诉你，知道链接的地址比起页面人工点击的还是有**很大优势**。
* 把**URL动态化**，就连写代码的人都不知道，你就通过MD5之类的加密算法加密随机的字符串去做url，然后通过**前端代码获取url后台校验才能通过**。



######  Redis

* 单机的**Redis**顶不住嘛，那简单多找几个兄弟啊，秒杀本来就是读多写少，那你们是不是瞬间想起来我之前跟你们提到过的，**[Redis集群](https://www.zhihu.com/search?q=Redis集群&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A923987542})**，**主从同步**、**读写分离**，我们还搞点**哨兵**，开启**持久化**直接无敌高可用！
* ![img](https://pic1.zhimg.com/80/v2-113404a1ef54b3f3c4f0aa0e4bcc52eb_720w.jpg?source=1940ef5c)



###### Nginx

* **高性能的web服务器**，并发也随便顶几万不是梦，但是我们的**Tomcat**只能顶几百的并发呀，那简单呀**负载均衡**嘛，一台服务几百，那就多搞点，在秒杀的时候多租点**流量机**。
* **[恶意请求拦截](https://www.zhihu.com/search?q=恶意请求拦截&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A923987542})**也需要用到它，一般单个用户请求次数太夸张，不像人为的请求在**网关那一层就得拦截掉了**，不然请求多了他抢不抢得到是一回事，服务器压力上去了，可能占用网络带宽或者把**服务器打崩、缓存击穿**等等。



###### 资源静态化

* 秒杀一般都是特定的商品还有页面模板，**现在一般都是[前后端分离](https://www.zhihu.com/search?q=前后端分离&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A923987542})的**，所以页面一般都是不会经过后端的，但是**前端也要自己的服务器**啊，那就把能提前放入**[cdn服务器](https://www.zhihu.com/search?q=cdn服务器&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A923987542})**的东西都放进去，反正把所有能提升效率的步骤都做一下，减少真正秒杀时候服务器的压力。





###### 按钮控制

* 秒杀前，一般按钮都是**置灰**的，只有时间到了，才能点击。
* 这是因为怕大家在时间快到的**最后几秒秒疯狂请求服务器**，然后还没到秒杀的时候基本上服务器就挂了。
* 需要前端的配合，定时去请求你的后端服务器，获取最新的北京时间，到时间点再给按钮可用状态。



###### 限流

* **前端限流**：这个很简单，一般秒杀不会让你一直点的，**一般都是点击一下或者两下然后几秒之后才可以继续点击**，这也是保护服务器的一种手段。
* **后端限流**：秒杀的时候肯定是涉及到后续的订单生成和支付等操作，但是都只是成功的幸运儿才会走到那一步，那**一旦100个产品卖光了，return了一个false，前端直接秒杀结束**，然后你后端也关闭后续无效请求的介入了。
* 真正的限流还会有**限流组件的加入**例如：阿里的Sentinel、Hystrix等。



###### 库存预热

* **秒杀的本质，就是对库存的抢夺**，每个秒杀的用户来你都去数据库查询库存校验库存，然后扣减库存，撇开[性能因数](https://www.zhihu.com/search?q=性能因数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A923987542})，你不觉得这样好繁琐，对业务开发人员都不友好，而且数据库顶不住啊。
* 解决思路： 要开始秒杀前你通过定时任务或者运维同学**提前把商品的库存加载到Redis中**去，让整个流程都在Redis里面去做，然后等秒杀介绍了，再异步的去修改库存就好了。
* Redis + Lua 脚本解决超卖问题。
* **Lua脚本是类似Redis事务，有一定的原子性，不会被其他命令插队，可以完成一些Redis事务性的操作。**
* 写一个脚本把**判断库存扣减库存的操作**都写在一个脚本丢给Redis去做，那到**0了后面的都Return False**了是吧，一个失败了你修改一个开关，直接挡住所有的请求，然后再做后面的事情嘛。



###### 限流 & 降级 & 熔断 & 隔离

* 不怕一万就怕万一，万一你真的顶不住了，**限流**，顶不住就挡一部分出去但是不能说不行，**降级**，降级了还是被打挂了，**熔断**，至少不要影响别的系统，**隔离**，你本身就独立的，但是你会调用其他的系统嘛，你快不行了你别拖累兄弟们啊。



###### MQ

* 你买东西少了你直接100个请求改库我觉得没问题，但是万一秒杀一万个，10万个呢？服务器挂了。
* 你可以把它放消息队列，然后一点点消费去改库存就好了嘛，不过单个商品其实一次修改就够了。

![img](https://picx.zhimg.com/80/v2-8aade443815c569af8599b0683e04b69_720w.jpg?source=1940ef5c)















##### 秒杀系统架构优化思路

* 秒杀系统难做的原因：库存只有一份，所有人会在集中的时间读和写这些数据。例如小米手机每周二的秒杀，可能手机只有1万部，但瞬时进入的流量可能是几百几千万。又例如12306抢票，亦与秒杀类似，**瞬时流量更甚**。
* ![img](http://www-x-habadog-x-com.img.abc188.com/wp-content/uploads/2015/04/4layer.png)
* 流量到了亿级别，常见站点架构如上：
  1）浏览器端，最上层，会执行到一些JS代码
  2）站点层，这一层会访问后端数据，拼html页面返回给浏览器
  3）服务层，**向上游屏蔽底层数据细节**
  4）数据层，最终的库存是存在这里的，mysql是一个典型

###### 优化策略：

* > * 将**请求尽量拦截在系统上游**：传统秒杀系统之所以挂，请求都压倒了后端数据层，数据读写锁冲突严重，并发高响应慢，**几乎所有请求都超时，流量虽大，下单成功的有效流量甚小**。
  > * 充分利用缓存：这是一个典型的**读多写少的应用场景**，非常适合使用缓存.

###### 优化细节：

* **浏览器层请求拦截**

* > * 点击了“查询”按钮之后，系统那个卡呀，进度条涨的慢呀，作为用户，我会不自觉的再去点击“查询”，继续点，继续点，点点点。。。有用么？平白无故的增加了系统负载（一个用户点5次，80%的请求是这么多出来的），怎么整？
  > * 1. 产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求
  >   2. JS层面，限制用户在x秒之内只能提交一次请求
  >
  > * 如此限流，80%流量已拦。

* **站点层请求拦截与页面缓存**

* > * 浏览器层的请求拦截，只能拦住小白用户（不过这是99%的用户哟），高端的程序员根本不吃这一套，写个for循环，**直接调用你后端的http请求，怎么整**？
  > * 1. **同一个uid，限制访问频度**，做页面缓存，x秒内到达站点层的请求，均返回同一页面
  >   2. 同一个item的查询，例如手机、车次，**做页面缓存，x秒内到达站点层的请求，均返回同一页面**
  > * 如此限流，又有99%的流量会被拦截在站点层

* **服务层请求拦截与数据缓存**

* > * 站点层的请求拦截，只能拦住普通程序员，高级黑客，假设他控制了10w台肉鸡（并且假设买票不需要实名认证），这下uid的限制不行了吧？怎么整？
  > * 1. 大哥，我是服务层，我清楚的知道小米只有1万部手机，我清楚的知道一列火车只有2000张车票，我透10w个请求去数据库有什么意义呢？**对于写请求，做请求队列，每次只透有限的写请求去数据层**，如果均成功再放下一批，如果库存不够则队列里的写请求全部返回“已售完”
  >   2. **对于读请求，还要我说么？cache抗**，不管是memcached还是redis，单机抗个每秒10w应该都是没什么问题的
  >      如此限流，只有非常少的写请求，和非常少的读缓存mis的请求会透到数据层去，又有99.9%的请求被拦住了

* **数据层闲庭信步**

* > * 到了数据这一层，几乎就没有什么请求了，单机也能扛得住，还是那句话，库存是有限的，小米的产能有限，**透这么多请求来数据库没有意义。**



###### 总结

1. 尽量将请求拦截在系统上游。
2. 读多写少的常用多使用缓存。

















#### 设计一个王者荣耀的排行榜，要去按照区服排

##### 基于 mysql

* 数据表：

* > * 队伍按照历史最高分数排名
  > * 队伍名称
  > * 历史最高分数
  > * 最近一次提交得分
  > * 最近一次提交时间

* **前端每隔一分钟调用我的接口，相同分数，名次相同**，所以我在接口里面用一条比较复杂的 sql 去查询数据库，上面的这些字段就都有了。

* 也许你会给我扯什么查询慢就加索引，数据量大就分库分表的方案。

* 但是一旦数据量大起来了，这个方案其实就不是一个特别好的方案。这问题，得从根上治理。





##### redis 

* 这个场景其实就是在考察你对于 Redis 的 sorted set 数据结构的掌握。
* ![img](https://segmentfault.com/img/remote/1460000039320538)
* 前面的 sport:ranking:20210227 是 Redis 中的 key。
* value 是一个集合，且可以看出这个集合是有序的。**集合中的每一个 member 都有一个 score，然后按照这个 score 进行降序排序**。
* 所以我画图的时候，**score 在前，member 在后**。这可不是随便画的，虽然谁前谁后好像也不影响什么玩意。
* 另一个需要注意的点是，虽然我的示意图中没有体现出来，但是在有序集合中，**元素即 member 是不可以重复的，但是 score 是可以重复的。**
* 当多个元素具有**相同的分数时**，它们按照 **字典序** 进行排序。

###### 存储用户头像

* 上面我还漏了两个字段：

* > * 微信头像
  > * 朋友点赞个数

* 这个时候其实我们想要存储的是 User 对象，对象里面有这几个字段：**昵称、头像图片链接、点赞数、步数**。

* Q : 这个用 Redis 的啥数据结构来存？ A : 得用 Hash 结构

* ![img](https://segmentfault.com/img/remote/1460000039320559)

* ```shell
  hmset sport:ranking:why:20210227:jay nickName jay headPhoto xxx likeNum 520 walkNum 66079
  ```

* **执行完成之后，在 RDM 里面看起来是这样的**：

* ![img](https://segmentfault.com/img/remote/1460000039320561)

* 当后续有更多的赞的时候，需要调用更新命令更新 likeNum：

* ```shell
  hincrby sport:ranking:why:20210227:jay likeNum 500
  ```

* ![img](https://segmentfault.com/img/remote/1460000039320558)



##### 最近七天的排行榜

* 提供一个最近七天、上一周、上一月、上个季度、这一年排行榜啥的

* 其实这还是在考察你对于 Redis 有序集合 API 的掌握程度。

* ```shell
  zinterstore/zunionstore destination numkeys key [key ...] [weights weight [weight ...]] [aggregate sum|min|max] 获取交集/并集
  
  ```

* > * zinterstore/zunionstore其实就是交集/并集
  > * destination **将交集/并集的结果保存到这个键中**
  > * numkeys 需要做交集/并集的集合的个数
  > * key [key ...] 具体参与交集/并集的集合
  > * weights weight [weight ...] 每个参与计算的集合的权重。在做交集/并集计算时，每个集合中的 member 会把自己的 score 乘以这个权重，默认为 1。
  > * aggregate sum|min|max 对于各个集合中的相同元素是 sum(求和)、min(取最小值)还是max(取最大值)，默认为 sum。



##### 亿级用户排行榜

* 但是假设现在的需求就是要查询用户的全服排名，怎么查？
* 王者一共 8 个段位：所以我们可以有 8 个桶。
* 这个桶可以是一个 Redis 里面的 8 个不同的 key，甚至可以是 8 个 Redis 里面各一个 key，看面试官给你的经费是多少，钱多就可劲造。
* ![img](https://segmentfault.com/img/remote/1460000039320565)
* 首先我们用 Redis 的有序集合，那么我们就得给每个 member 一个 score。
* 所以，每个用户在桶里面都一个经过公式计算后得出的积分。
* 写程序的时候是可以知道我现在的段位是星耀，那么直接去星耀的桶里面，用 zrevrank 计算出当前桶里面的排名，假设为 n。
* 然后再**通过 zcard 这个 O(1) 的命令获取到**，前面的桶，也就是**最强王者和荣耀王者这两个桶的集合大小**，分别为 y 和 x。
* 那么why哥的全服排名就是 **n+y+x**。
* Q： 但是对于亿级用户只分 8 个桶未免太少了吧？
* A： 那就**继续分桶**呗，别忘了，**每个段位里面还有小段位**的。
* 推翻重来，直接把段位加上各种其他条件换算成积分，然后按照积分来拆分：
* ![img](https://segmentfault.com/img/remote/1460000039320566)
* 











#### 聊天系统设计

##### 确定需求

> •一对一聊天，传递延迟低
>
> •小组聊天（最多100人）
>
> •在线状态
>
> •多设备支持。同一帐户可以同时登录到多个帐户。
>
> •推送通知

* 设计规模达成一致也很重要。我们将设计一个支持5000万DAU的系统。



##### 设计

* 在聊天系统中，客户端可以是移动应用程序或web应用程序。客户机之间不直接通信。

* **每个客户端都连接到一个聊天服务，该服务支持上述所有功能**。让我们关注基本业务。聊天室服务必须支持以下功能：

  > * 接收来自其他客户端的消息。
  > * 为每条消息找到合适的收件人，并将消息转发给收件人。
  > * 如果收件人不在线，则在服务器上保留该收件人的消息，直到其在线。

* 显示了客户端（发送方和接收方）与聊天服务之间的关系：

* ![img](https://segmentfault.com/img/remote/1460000040680576)

* **当发送方通过聊天服务向接收方发送消息时**，它使用经过时间测试的HTTP协议，这是最常见的web协议。

* keep-alive在这方面很有效，因为keep-alive头允许客户端与聊天服务保持持久连接。它还减少了TCP握手的次数。

* **HTTP在发送方是一个很好的选择**，许多流行的聊天应用程序（如Facebook[1]）最初使用HTTP发送消息。

* 接收器端要复杂一些。由于**HTTP是由客户机发起的**，因此**从服务器发送消息并不简单。**多年来，许多技术被用来模拟服务器启动的连接：**轮询、长轮询和WebSocket**。



###### 轮训

* 轮询是一种**客户端定期询问服务器是否有可用消息的技术**。根据轮询频率，**轮询的成本可能会很高**。它可能会**消耗宝贵的服务器资源来回答一个在大多数情况下都没有答案的问题**。
* ![Polling](https://segmentfault.com/img/remote/1460000040680577)

###### 长轮训

* 由于轮询可能效率低下，下一步是长轮询

* ![Long polling](https://segmentfault.com/img/remote/1460000040680578)

* 在长轮询中，**客户端保持连接打开**，**直到有新消息可用或达到超时阈值**。一旦客户端接收到新消息，它会立即向服务器发送另一个请求，从而重新启动进程。

* 缺点： 

* > * 发送方和接收方可能无法连接到同一聊天服务器。基于HTTP的服务器通常是无状态的。如果使用循环法进行负载平衡，则**接收消息的服务器可能与接收消息的客户端没有长轮询连接**。
  > * **服务器无法很好地判断客户端是否已断开连接**。
  > * 效率低下。如果用户聊天不多，长轮询仍然会在超时后进行定期连接。



###### Websocket 长连接

* WebSocket是从服务器向客户端发送异步更新的最常见解决方案。
* ![Websocket](https://segmentfault.com/img/remote/1460000040680579)
* WebSocket连接由客户端启动。它是**双向和持久**的。
* 它从HTTP连接开始，可以通过一些定义良好的握手“升级”到WebSocket连接。
* **通过这种持久连接**，服务器可以向客户端发送更新。
* 即使有防火墙，WebSocket连接通常也能正常工作。这是因为**它们使用的端口80或443也被HTTP/HTTPS连接使用**。
* ![Chat service](https://segmentfault.com/img/remote/1460000040680580)
* 发送方使用HTTP是一种很好的协议，但是由于WebSocket是双向的，因此没有强有力的技术理由不将其用于发送。图12-6显示了如何将WebSocket（ws）用于发送方和接收方。
* 通过**将WebSocket用于发送和接收**，它简化了设计，并使**客户端和服务器上的实现更加简单**。由于**WebSocket连接是持久的**，因此高效的连接管理在服务器端至关重要。



##### 高级设计

* 提到WebSocket被选为客户端和服务器之间双向通信的主要通信协议。
* 需要注意的是，**其他一切都不必是WebSocket**。事实上，聊天应用程序的大多数功能**（注册、登录、用户配置文件等）都可以通过HTTP使用传统的请求/响应方法。**
* 聊天系统分为三大类：**无状态服务、有状态服务和第三方集成**。
* ![Chat service system design](https://segmentfault.com/img/remote/1460000040680581)

###### 无状态服务

* 无状态服务是传统的面向公众的请求/响应服务，用于管理登录、注册、用户配置文件等。
* 无状态服务位于负载平衡器后面，负载平衡器的任务是**根据请求路径将请求路由到正确的服务**。
* **这些服务可以是单一的或单个的微服务。**
* 它的主要工作是**为客户端提供一个可以连接到的聊天服务器的DNS主机名列表**。



###### 有状态服务

* **唯一有状态的服务是聊天服务**。该服务是有状态的，因为每个客户端都保持与聊天服务器的持久网络连接。
* 在该服务中，只要服务器仍然可用，客户端通常不会切换到另一个聊天服务器。服务发现与聊天服务密切配合，以避免服务器过载。





###### 第三方集成

* 对于聊天应用程序，**推送通知是最重要的第三方集成**。这是一种在新消息到达时通知用户的方法，即使应用程序未运行。推送通知的正确集成至关重要。



###### 可伸缩

* 理论上也可以在一台现代云服务器中容纳所有用户连接。**服务器可以处理的并发连接数很可能是限制因素**。

* 在我们的场景中，在一百万并发用户的情况下，**假设每个用户连接在服务器上需要10K的内存**，**只需要大约10GB的内存**就可以在一个框中容纳所有连接。

* **客户端保持与聊天服务器的持久WebSocket连接，以进行实时消息传递**。

* ![Messenger architecture](https://segmentfault.com/img/remote/1460000040680582)

* > * **聊天服务器**便于发送/接收消息。
  > * **状态服务器**管理在线/离线状态。
  > * **API服务器处理一切**，包括用户登录、注册、更改配置文件等。
  > * **通知服务器发送推送通知**。
  > * **键值存储用于存储聊天历史记录**。当离线用户联机时，她将看到以前的所有聊天记录。



###### 存储

* 我们必须做出的一个重要决定是选择正确的数据库类型：**关系数据库还是NoSQL数据库？**为了做出明智的决定，我们将检查数据类型和读/写模式。

* 典型的聊天系统中存在两种类型的数据。第一种是通用数据，例如用户配置文件、设置、用户好友列表。**这些数据存储在健壮可靠的关系数据库中**。复制和分片是满足可用性和可伸缩性要求的常用技术。

* 第二个是聊天系统特有的：**聊天历史数据。理解读/写模式很重要**。

* > * 聊天系统的数据量巨大。此前的一项研究[2]显示，Facebook messenger和Whatsapp每天处理600亿条消息。
  > * **只经常访问最近的聊天**。用户通常不会查找旧聊天记录。
  > * 尽管在大多数情况下都会查看最近的聊天记录，但用户可能会使用需要随机访问数据的功能，如**搜索、查看您的提及、跳转到特定消息等。数据访问层应支持这些情况**。
  > * 1对1聊天应用的读写比约为1:1。

* **推荐键值存储：**

* > * 键值存储允许轻松**水平缩放**。
  > * 键值存储提供非常低的数据访问延迟。
  > * 关系数据库**不能很好地处理长尾**[3]数据。当**索引变大时，随机访问的代价很高**。
  > * 键值存储被其他经验证的可靠聊天应用程序采用。例如，Facebook messenger和Discord都使用**键值存储**。

* 



###### 数据模型

* **主键是message_id，它有助于确定消息顺序**。我们不能依靠created_at来决定消息序列，因为可以同时创建两条消息。

* ![Data model for 1 on 1 chat](https://segmentfault.com/img/remote/1460000040680583)

* 复合主键是（通道id、消息id）。频道和组在这里表示相同的含义。**channel_id是分区键**，**因为群组聊天中的所有查询都在一个频道中运行**。

* Q： 消息 ID 如何生成

*  **消息id负责确保消息的顺序**。要确定消息的顺序，消息id必须满足以下两个要求：

* > * ID必须是唯一的。
  > * ID应按时间进行排序，这意味着新行的ID高于旧行。

* 我们如何才能实现这两个保证？首先想到的是**MySql中的“auto_increment”关键字**。然而，**NoSQL数据库通常不提供这样的功能**。

* 第二种方法是使用全局64位序列号生成器，如Snowflake.

* 最后一种方法是使用本地序列号生成器。**本地意味着ID仅在组中是唯一的**。本地ID工作的原因是在**一对一通道或组通道内维护消息序列就足够了**。与全局ID实现相比，这种方法更容易实现。





##### 深度设计

* 对于**聊天系统，服务发现、消息流和在线/离线指标**值得深入探索。

* 服务发现：

* ![img](https://pic2.zhimg.com/80/v2-c5e1d05128694eaffc043d1acf1cab41_720w.jpg)

* 服务提供者是什么，简单点说就是一个**HTTP服务器**，提供了API服务，有一个**IP端口作为服务地址**。

* 服务消费者是什么，它就是**一个简单的进程**，想要访问服务提供者提供的服务来干一些事情。

* **一个HTTP服务器既可以是服务提供者对外提供服务，也可以是消费者需要别的服务提供者提供的服务**，这就是服务依赖，没有你我就不是我自己。复杂的服务甚至有多个服务依赖。

* 服务中介就是一个字典，字典里有很多key/value键值对，key是服务名称，value是服务提供者的地址列表。**服务注册就是调用字典的Put方法**塞东西，**服务查找就是调用字典的Get方法拿东西**。

* 服务中介是联系服务提供者和服务消费者的桥梁。**服务提供者将自己提供的服务地址注册到服务中介，服务消费者从服务中介那里查找自己想要的服务的地址，然后享受这个服务**。服务中介提供多个服务，每个服务对应多个服务提供者。

* ![Service discovery](https://segmentfault.com/img/remote/1460000040680584)

* > * 用户A尝试登录应用程序。
  > * 负载平衡器将登录请求发送到API服务器。
  > * 在后端对用户进行身份验证后，服务发现会为用户A找到最佳的聊天服务器。在本例中，选择了服务器2，并将服务器信息返回给用户A。
  > * 用户A通过WebSocket连接到聊天服务器2。



###### 消息流

* 当用户A向用户B发送消息时会发生什么。

* ![1 on 1 chat flow](https://segmentfault.com/img/remote/1460000040680585)

* > * 用户A向聊天服务器1发送聊天信息
  > * 聊天服务器1从ID生成器获取消息ID
  > * 聊天服务器1将消息发送到**消息同步队列**
  > * 消息存储在键值存储中
  > * 如果用户B在线，则消息将转发到**用户B连接的聊天服务器2**
  > * 如果用户B处于脱机状态，则会从推送通知（PN）服务器发送推送通知
  > * 聊天服务器2将消息转发给用户B。用户B和聊天服务器2之间存在**持久的WebSocket连接**。



###### 跨多个设备的消息同步

* ![Message synchronization across multiple devices](https://segmentfault.com/img/remote/1460000040680586)

* 用户A有两个设备：电话和笔记本电脑。当用户A用手机登录聊天应用程序时，它会与聊天服务器1建立WebSocket连接。类似地，**笔记本电脑和聊天服务器1之间存在连接**。

* **每个设备**都维护一个名为cur_max_message_id的变量，该变量跟踪设备上的最新消息id。满足以下两个条件的消息被视为新闻消息：

* > * 收件人ID等于当前登录的用户ID。
  > * **键值存储中的消息ID大于cur_max_Message_ID**。

* 由于每个设备上都有不同的cur_max_message_id，消息同步很容易，因为每个设备都可以从**KV存储中获取新消息**。



###### 群消息流

* ![Group chat flow](https://segmentfault.com/img/remote/1460000040680587)

* 首先，将来自**用户A的邮件复制到每个组成员的邮件同步队列**：一个用于用户B，另一个用于用户C。您可以将邮件同步队列视为收件人的收件箱（推模式）。此设计选择适用于小团体聊天，因为：

* > * 它简化了邮件同步流程，因为每个客户端只需检查自己的收件箱即可获得新邮件。
  > * 当组号较小时，在每个收件人的收件箱中存储一份副本并不太昂贵。

* 微信采用了类似的方法，它**将一个群组的成员限制在500人**[8]。但是，对于**具有大量用户的组，为每个成员存储消息副本是不可接受的**。

* 在收件人端，收件人可以接收来自多个用户的消息。每个**收件人都有一个收件箱（邮件同步队列）**，其中包含来自不同发件人的邮件。

* ![Group chat message sync](https://segmentfault.com/img/remote/1460000040680588)



###### 在线状态

* 在线状态指示器是许多聊天应用程序的基本功能。通常，您可以在用户的个人资料图片或用户名旁边看到一个绿点。
* 在高级设计中，状态服务器负责管理在线状态并通过WebSocket与客户端通信。**有一些流将触发联机状态更改**。让我们逐一检查一下。
* 用户登录：
* “服务发现”部分解释了用户登录流程。在客户端和实时服务之间建立WebSocket连接后，**用户a的在线状态和时间戳处的最后一次活动将保存在KV存储中**。**状态指示器显示用户登录后处于联机状态**。
* ![Online presence when users login](https://segmentfault.com/img/remote/1460000040680589)
* 用户注销：
* 当**用户注销时，它将通过用户注销流程**，如图12-17所示。KV商店中的联机状态更改为脱机。**状态指示器显示用户处于脱机状态**。
* ![Online presence when users logout](https://segmentfault.com/img/remote/1460000040680590)
* 用户断开连接：
* 当用户断开与internet的连接时，**客户端和服务器之间的持久连接将丢失。**
* 处理用户**断开连接的一种简单方法是将用户标记为脱机，并在重新建立连接时将状态更改为联机**。
* 这种方法有一个主要缺陷。用户通常会在**短时间内频繁断开和重新连接到internet**。
* 当用户通过隧道时，可以打开和关闭网络连接。在**每次断开/重新连接时更新联机状态会使状态指示器频繁更改**，从而导致用户体验不佳。
* **引入心跳机制来解决这个问题。**
* 在线客户端定期向状态服务器发送心跳事件。如果状态服务器在特定时间内（例如x秒）从客户端接收到心跳事件，则**认为用户处于联机状态。否则，它将处于脱机状态。**
* 客户端每5秒向服务器发送一次心跳事件。发送3个心跳事件后，**客户端断开连接，并且在x=30秒内未重新连接**（任意选择此数字以演示逻辑）。**联机状态更改为脱机**。
* ![Disconnection handling for chat service](https://segmentfault.com/img/remote/1460000040680591)



* 

###### 用户在线状态传播

* 状态服务器使用**发布-订阅模型**，其中每个好友对维护一个**通道**。
* 当用户A的联机状态更改时，它将**事件发布到三个频道**，即频道A-B、A-C和A-D。这**三个频道分别由用户B、C和D订阅**。
* 朋友很容易获得在线状态更新。客户端和服务器之间的通信是通过实时WebSocket进行的。
* ![Online status fanout](https://segmentfault.com/img/remote/1460000040680592)
* 上述设计对于**小用户群是有效的**。例如，微信采用了类似的方法，因为其用户群上限为500。
* 对于较大的群体，通知所有成员在线状态既昂贵又耗时。假设一个组有100000名成员。每次状态更改将**生成100000个事件**。要解决性能瓶颈，一个可能的解决方案是**仅当用户进入组或手动刷新好友列表时获取联机状态**。



##### 技术总结

* 额外可能聊的点：

* > * 扩展聊天应用程序以支持照片和视频等媒体文件。媒体文件的大小明显大于文本。压缩、云存储和缩略图是值得讨论的有趣话题。
  > * 端到端加密。Whatsapp支持对消息进行端到端加密。**只有发件人和收件人才能阅读邮件**。感兴趣的读者应参考参考资料[9]中的文章。
  > * 在客户端缓存消息可以有效减少客户端和服务器之间的数据传输。
  > * 缩短加载时间。Slack构建了一个**地理分布的网络**，以缓存用户的数据、通道等，从而缩短加载时间
  > * 聊天服务器错误。与聊天服务器的连接可能有几十万个，甚至更多的持久连接。如果聊天服务器脱机，服**务发现（Zookeeper）将为客户端提供一个新的聊天服务器**，以建立新的连接。
  > * 消息重新发送机制。重试和排队是重新发送消息的常用技术。









#### 设计直播间

















#### 如何设计内存池









#### 设计一个调查问卷











#### 设计一个打车软件











#### 设计一个语音机器人











#### 视频流系统设计











#### 在线文档协同设计











#### 分布式文件系统设计









#### 设计一个分布式定时器













#### 地铁之间从A到B最小换乘次数？









#### 1000w个宝贝，两个特征，购买量和好评量，怎么排序









#### 查找范围区间的重复ip

* 字典树
* 









#### 为什么微信不能开多个

* **一个可执行程序通过“包装”成为一个进程，一个可执行程序通过“包装”成为多个进程也没有问题**，可执行程序和进程是一对多的关系，这就是为什么qq可以打开多个。
* 微信只能打开一个，那么说明我们需要对进程做限定，**保证进程唯一性**。
* **如何保证进程唯一性**呢？专刊里面**又讲过互斥锁**，通过互斥锁我们就可以保证了。但是是哪一个类型的互斥锁呢？
* 那就是**文件锁**
* 微信源码里面加上文件锁，那么就只有一个进程对微信读写，**即使多开几个进程也获取不了微信的可执行程序了**，开了等于没开。







#### 超大数据集里面，确定一个人的密接













#### 设计存储系统

* 大量数据，数据每一行包含很多信息，给你MySQL和分布式存储引擎，怎么设计存储系统？









#### 设计线程池

* 你会怎么设计线程池，扩容，线程池初始化大小由什么决定…







#### 秒杀系统 sql实现，从建表和建立索引开始









#### 两段很大的json如何比较这两个json是不是一样的

* 有提到从字符串比较的层面来比较，比如md5（不了解）、如何解决 json里键值对无序并且还有嵌套的情况，包括可能出现中文，有编码问题等等...









#### Redis 锁时长续费问题

* [redis](https://www.nowcoder.com/jump/super-jump/word?word=redis)加分布式锁，锁时长的续费问题，如何续费，具体实现；以及续费过程中如何保证流程正常









#### 点餐小程序

* 可支持多人同时点餐的小程序，点餐流程，如何推送，如何并发点餐











#### 发布订阅文章模型

* 网站支持作者发布文章、读者订阅作者，在首页推送读者订阅的作者们更新的最新的10篇文章，如何设计mysql表，写sql，谈优化，如何利用缓存













#### 员工绩效系统设计

* 设计一个方案，来计算和保存员工的绩效信息
* 其中，绩效是分层的，一个绩效内部可能包含了多个子绩效，并且每个绩效都有各自的权重，员工的某项绩效分数就是其子绩效对应的加权求和。



















### 网络

#### 为什么需要 HTTP 而不直接使用 Socket

* **HTTP连接 = 以HTTP协议为通信协议的TCP连接**

* TCP/IP协议可以两个进程通过三次握手建立稳定的通信信道，发送字节流；而HTTP协议建立在TCP/IP协议之上，也就是说TCP/IP协议可以让两个程序说话，而HTTP协议定义了说话的规则。
*  我们在传输数据时，可以只使用TCP/IP协议进行传输，但是这样没有应用层的参与，**会导致两端无法识别数据内容，这样传输的数据也就没有意义了。**因此如果想让传输的数据有意义，那么就必须要用到应用层的协议(HTTP、FTP、TELNET等)，也可以自己定义应用层的协议。
* socket是对TCP/IP协议的封装，Socket本身并不是协议，而是一个调用接口（API），通过Socket，我们才能使用TCP/IP协议。实际上，Socket跟TCP/IP协议没有必然的联系。
* TCP/IP只是一个协议栈，就像操作系统的运行机制一样，必须要具体实现，**同时还要提供对外的操作接口**。







#### 浏览器输入网址发生什么了？

![简单的网络模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/2.jpg)

* Mac 地址在整个传输过程中是不断的改变的，但是源 IP 和 目的 IP 地址是不变的

* HTTP：

> * **对URL 进行解析**，浏览器确认 Web 服务器，以及请求的文件名，然后根据这些消息生成 HTTP 请求报文
> * 这个报文发到什么地方呢？

* DNS
* 浏览器解析 URL 并生成 HTTP 消息后，需要委托**操作系统**将消息发送给 `Web` 服务器。
* ![域名解析的工作流程](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/6.jpg)

> * 需要查询服务器域名的 IP 地址，使用的是 DNS 服务器，优先从本地的 DNS 服务器查询，找不到的话，去根 DNS 服务器 -> 顶级域 DNS 服务器 （com）  -> 权威 DNS 服务器 (service.com) 找。
> * DNS 解析的基本过程是指路说这个可以去哪找，然后让本地的域名服务器去指定的顶级域名服务器找 IP地址。

* Q： 那是不是每次解析域名都要经过那么多的步骤呢？

* A: 当然不是了，还有缓存这个东西的嘛。

  **浏览器**会先看自身有没有对这个域名的缓存，如果有，就直接返回，如果没有，就去问**操作系统**，操作系统也会去看自己的缓存，如果有，就直接返回，如果没有，再去 **hosts 文件**看，也没有，才会去问「本地 DNS 服务器」。

* 协议栈

> * 获得 IP 之后就可以将 HTTP的传输工作交给协议栈完成了，协议栈主要包括 TCP, UDP, IP 在下面是网卡的驱动程序负责网卡硬件，在下面的网卡就是负责实际的网线中的信号的传输

* TCP 

> * TCP 头部的基本信息：
>
>   > * 源端口和目标端口，这个是为了：数据知道应该发给哪个应用.
>   > * 序号段：这个是为了解决序号乱序情况的出现
>   > * 确认号：这个是为了确认发出去的数据包是否被成功接受，为了解决丢包问题，没收到需要重传
>   > * 一些状态位： SYN 发起一个连接（三次握手）， ACK 是回复， RST ： 重新连接， FIN 结束连接（四次挥手）以上均是只有 1 bit
>   > * 窗口大小： TCP做流量控制和拥塞控制的时候使用的，控制发送速度，避免出现阻塞
>
> * 三次握手:因为 TCP是全双工的，所以需要保证发送方和接收方均有发送和接受的能力
>
> * 现在网络包的报文是： TCP头部 + 原来的 HTTP请求报文

* IP - 远程定位

> * IP 头部
>
>   > * 源 IP ： 客户端输出的 IP地址
>   > * 目标 IP ： 就是通过 DNS 域名解析得到的 WEB 服务器 IP
>   > * 协议号： 当前使用的是TCP 还是 UDP 协议
>
> * 下一站应该去哪呢？

* MAC

> * Mac 头部
>
>   > * 接受方的 MAC 地址：网卡生产时写在网络 ROM里面，只要读出来就可以了。
>   > * 发送发 MAC 地址，需要使用 ARP协议在当前路由表中广播当前的目的 IP 是谁的，为了节约时间可以使用 ARP 缓存
>   > * 协议类型： 当前如果使用的是 IP ，填写的就是 IP
>
> * 报文内容 ： MAC头部 + IP头部 + TCP头部+ HTTP请求报文

* 出口-网卡

> * IP 网络包是一个存放一串二进制数字信息，我们没法在物理信道中传输，需要将数字信息转换成电信号，才能在网线上传输。
> * 网卡主要负责这个操作，软件是网卡驱动程序，网卡驱动从 IP 模块取得包之后，将其复制到网卡的缓存中，之后将包转换成电信号，通过网线传输出去

* 送别者 - 交换机

> * 而**交换机**是基于以太网设计的，俗称**二层**网络设备，交换机的端口不具有 MAC 地址。
>
> * 交换机的 MAC 地址表主要包含以下两个信息：
>
>   > * 一个是设备的 MAC 地址
>   > * 一个是该设备连接在交换机的哪个端口上
>
> * 交换机会根据 Mac 地址表查找 Mac 地址，然后将信号发送到对应的端口。
>
> * 如果不存在，那么会发送到所有的端口中，只有相应的接受者才接受这个包，其他的设备会忽略这个包，同时交换机根据某一个端口的响应，将地址写入 MAC 地址表，下次就不需要再把包发到所有的端口

* 路由器

> * 基于 IP 设计的， **三层**网络设备，路由器的各个端口都具有 MAC 地址和 IP 地址；
> * 网络包经过交换机之后到达路由器，在此路由器被转发到下一个路由器或者目标设备。
> * Mac 头部在这里的作用就是将**包转发送达到路由器**，这个接收方的Mac地址就是路由器端口的 Mac地址，送达之后，这个Mac头部就可以丢弃了 （Mac 地址的任务完成了）
> * 接下来，**路由器会根据 MAC 头部后方的 `IP` 头部中的内容进行包的转发操作。**
> * 查询路由表，找到对方的 IP 地址，之后通过 ARP 协议将对方的 IP 地址转成 MAC 地址，然后通过对应的端口发送数据到下一个路由器，重复上面的操作
> * 当知道对方的 IP 地址之后，我们需要将 IP地址转成 MAC 地址，这个时候可以使用ARP协议。
> * 一步步重复上面的操作，经过一个又一个路由器，网络包到达了最终的目的地
> * 注意：整个过程中，IP 地址不改变，但是 Mac 地址一直在变化，因为需要 Mac 地址在以太网内进行两个设备之间的包传输， **MAC 地址用于以太网中两个设备之间的包传输**

* 服务端

> * 服务端收到消息，一步一步将头部消息拔取，获取 HTTP请求报文，最终将HTTP 相应报文封装好，返回回到客户端，同样是上面的步骤



#### HTTP 协议

* HTTP 常见状态码：

![img](https://pic2.zhimg.com/80/v2-1fde71210df0e1f5737e18bf1aab75b3_720w.jpg?source=d16d100b)

> * 2xx
>
>   > * 200 Ok 响应头都存在 Body 数据
>   > * 204 Not content ： 成功的状态码：但是响应头没有 body 数据
>   > * 206 partial content ： 服务器处理成功，但是可能是HTPP 分块下载或者断电续传，表示响应返回的数据不是资源的全部数据，而是其中的一部分
>
> * 3xx
>
>   > * 301 Move Permanently : 永久重定向，响应头里面使用字段 Location 指明需要跳转的 URL
>   > * 302 Found ： 表示临时重定向和上面一样暂时使用一个 URL 访问
>   > * 304 Not Modified 缓存重定向，表示的是不具有跳转的含义，表示资源没有修改，重定向文件在缓存中
>
> * 4xx ： 客户端请求报文出错
>
>   > * 400 Bad Request : 表示客户端请求的报文存在错误。
>   > * 403 Forbidden ： 表示服务器禁止访问这个资源，报文没错
>   > * 404 Not found : 表示请求资源在资源服务器中不存在或者没有找到，所以没法提供给客户端
>
> * 5xx : 服务端内部错误
>
>   > * 502是网关错误，504是网关超时， 502已经与后端建立了连接，但超时；504与后端连接未建立，超时
>   > * 500 : Internal Server error ： 与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。
>   > * 501 Not Implemented : 表示客户端请求功能还不支持
>   > * 502 Bad Gataway : **502是代理服务器后面的真实服务器节点配置出了问题或者已经挂掉了**
>   > * 503 Service Unavailable : 表示的是服务器正忙，暂时没法响应服务器.
>   > * 504 Gataway timeout ： 504是代理服务器后面的**真实服务器已经过载**(没有挂掉），它要处理的请求报文实在太多，忙不过来了。（超过 NGINX 配置时间）



* http 常见字段

> * Host : 客户端发送请求时用来指定服务器的域名
> * Content-Length: 表示本次回应的数据长度，服务器在返回数据的时候使用的
> * Connection ： 表示的是是否一直连接，为了兼容老版本的 http 
> * Content-type : text/html; charset=utf-8 表示的是当前我们发送的是什么类型的数据，编码格式
> * Accept : 客户端可以接受的是什么格式的返回数据，*  */*  *表示任何资源
> * Content-Encoding : 表示的是数据编码的形式，压缩之类的
> * Accept - Encoding : 表示的是自己客户端接受哪些压缩办法
> * 状态码：200，请求方式： GET，Request URL， Date 日期之类的， user-agent ：使用什么浏览器之类的。



* http 特性

> * 优点 ：
>
>   > * 简单 ： 只包含： header + body 头部信息使用的是 key-value 简单的文本形式
>   > * 支持跨平台，从而可以大面积的应用
>   > * 灵活方便扩展： HTTP 协议的各类请求方法、URL、状态码头部字段都没有固定死，允许自定义或者填充， HTTP下层可以随意的扩展。
>
> * 缺点：
>
>   > * 明文传输：不安全， 传输过程中的信息可以被看见，方便调试，但是暴露在外
>   > * 无状态：服务器不需要额外的信息记录 HTTP 状态，减轻服务器的负担，让更多的资源处理服务，服务器没有记忆，那么关联性的每一个操作都需要反复的认证，使用 cookie 解决这个问题



* http 安全问题 ： TLS/SSL：

> * 明文不加密，内容可能被窃取了
> * 不验证通信身份，可能遭受伪装，例如访问假的淘宝
> * 无法证明报文的完整性，可能报文被修改了，例如：垃圾广告

> HTTPS 使用 TLS/SSL 层达到安全通信，TLS 协议如何解决 HTTP 风险的？
>
> * **信息加密**： 交互信息没法被窃取
> * **校验机制**： 通信内容没法被篡改，对数据进行验证
> * **身份证书**： 验证网站时候是真实的



* HTTPS 如何处理不安全风险的

> * 混合加密 :实现机密性
>
>   > * 通过混合加密的方式保证信息的机密性，解决窃听的风险
>   > * 对称加密：在通讯前建立来交换会话秘钥，特点：只使用一种秘钥，运算速度快，秘钥必须保密，但是不是非常的安全。在通信的过程中全部使用对称加密的方式，通过使用对称加密的会话秘钥（非对称加密传输的）对明文进行加密传输。
>   > * 非对称加密：使用公钥和私钥，安全但是速度慢，需要对报文加密，然后再解密的过程 , 在通信建立之前采用非对称加密的方式交换会话秘钥，后续不再使用非对称加密，因为太慢了。
>
> * 摘要算法：实现完整性
>
>   > * 使用这个算法来保证数据的完整性，我们为数据生成一个独一无二的摘要指纹，我们将指纹和明文一起加密发送，之后服务器对数据解密，之后算出数据的指纹，通过比较前后的指纹，判断是否相同，说明数据是否是完整的。
>
> * 数字证书：保证公钥的正确定和不被篡改
>
>   > * **将服务器公钥放在数字证书中，只要保证证书是可靠的，那么这个公钥就是可信的。**



* HTTPS 是如何建立连接的？

> * 客户端向服务器索要并验证服务器公钥
> * 双方协商生产 会话秘钥
> * 双方采用“会话秘钥”进行加密通信
> * SSL/TLS 的握手阶段涉及四次通信：



* http & https 区别：

> * HTTPS 解决安全问题，使用 SSL 层解决的
> * HTTP 建立连接简单，三次握手就可以传输数据，HTTPS 还需要 SSL/TLS 层的握手过程，才能加密传输
> * HTTP 端口 80， HTTPS： 端口443
> * HTTPS为了安全传输，需要向 CA （证书权威机构）申请数字证书，来保证服务器的身份是可靠的

##### HTTP/1.1 优化方案

> **问题：**
>
> * HTTP 1.1 最为人诟病的是 **HTTP 头部**的传输占用了大量带宽
> * HTTP 头部使用 ASCII 编码方式，这造成它往往达到几 KB（未使用二进制编码）
> * 滥用的 Cookie 头部进一步增大了体积
> * 每一次发送 HTTP 请求，都需要发送一次头部

* 1. 首先是通过缓存避免发送 HTTP 请求；（不走网络直接从缓存中获取 HTTP 响应）
* 2. 其次，如果不得不发起请求，那么就得思考如何才能减少请求的个数；（减少重定向次数、合并请求、延迟发送请求）

* 3. 最后则是减少服务器响应的体积。

###### 通过缓存避免发送 HTTP 请求

* 缓存能够让客户端在免于发送 HTTP 请求的情况下获得服务器的资源
* 其中请求的 URL 作为关键字（部分 HTTP 头部也会加入关键字，例如确定服务器域名的 Host 头部）
* 后续发起相同的请求时，就可以先在本地磁盘上通过关键字（URL）查找，如果找到了，就直接将缓存作为服务器响应使用。
* ![img](https://static001.geekbang.org/resource/image/9d/ab/9dea133d832d8b7ab642bb74b48502ab.png?wh=1007*676)
* 存在一个过期时间，来控制资源是否过期
* ![img](https://static001.geekbang.org/resource/image/a3/62/a394b7389d0cdb5f0866223681e19b62.png?wh=1048*696)
* 当**客户端发现缓存过期后，会取出缓存的摘要**（摘要是从第 1 次请求的响应中拿到的），把**它放在请求的 Etag 头部**中再发给服务器。而服务器获取到请求后，会将本地资源的摘要与请求中的 Etag 相比较，如果不同，那么缓存没有价值，重新发送最新资源即可；
* 如果摘要与 Etag 相同，那么仅返回不含有包体的 **304 Not Modified 响应**，告知客户端缓存仍然有效即可，这就省去传递可能高达千百兆的文件资源。
* 浏览器上的缓存只能为一个用户使用，故称为私有缓存。**代理服务器上的缓存可以被许多用户使用，所以称为共享缓存**。

###### 降低 HTTP 请求的次数

* 减少重定向次数(重定向增加了请求数量)

* > * HTTP 请求会经过多个代理服务器，如果将**重定向工作交由代理服务器完成**，就能减少网络消耗
  > * ![img](https://static001.geekbang.org/resource/image/0d/2a/0d1701737956ce65f3d5ec8fa8009f2a.png?wh=724*842)
  > * 

* 合并请求

* > * 当多个访问小文件的请求被合并为一个访问大文件的请求时，这样虽然传输的总资源体积未变，但减少请求就意味着**减少了重复发送的 HTTP 头部，同时也减少了 TCP 连接的数量**，因而省去了 TCP 握手和慢启动过程消耗的时间.
  > * 缺点： 这种合并请求的方式也会带来一个新问题，即，**当其中一个资源发生变化后，客户端必须重新下载完整的大文件**，这显然会带来额外的网络消耗。
  > * 

* 延迟发送请求

* > * 当前资源还不需要使用，延迟发送请求

###### 如何重新编码以减少响应的大小？

* 减少资源体积的唯一方式是对资源重新编码压缩，其中又分为**无损压缩**与**有损压缩**两种

* 无损压缩: 这是指压缩后不会损失任何信息，可以完全恢复到压缩前的原样。因此，**文本文件、二进制可执行文件**都会使用这类压缩方法

* 支持无损压缩算法的客户端会在请求中通过 Accept-Encoding 头部明确地告诉服务器：

* ```html
  Accept-Encoding: gzip, deflate, br
  ```

* 而服务器也会在响应的头部中，告诉客户端包体中的资源使用了何种压缩算法:

* ```html
  content-encoding: gzip
  ```

* 有损压缩，它通过牺牲质量来提高压缩比，主要针对的是图片(JPEG)和音视频

* **压缩算法的原理**: 都是基于香农的信息论，将**高频出现的信息用更少的比特编码**。虽然原理是一致的，但实现上却有很大的差别，比如Huffman通过建立Huffman树来生成编码，而LZ77却是通过滑动窗口，这造成压缩比、压缩速度都很不相同。



###### HTTP/1.1 相比 HTTP/1.0 的进步

> * HTTP  组成 <header, body> **body 可以根据 header 提供的压缩方法进行压缩操作**，以节约带宽，但是 header 没有优化的方法
>
> * 优势：
>
>   > * 使用TCP 长连接，调高效率
>   > * 使用持久连接，持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。
>   > * ![短连接与长连接](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP/16-%E7%9F%AD%E8%BF%9E%E6%8E%A5%E4%B8%8E%E9%95%BF%E8%BF%9E%E6%8E%A5.png)
>   > * 支持管道传输，就是只要**传输出去就不需要等其回来，可以直接发送下一个数据**
>   > * **HTTP/1.1 管道解决了请求的队头阻塞，但是没有解决响应的队头阻塞**。
>
> * 弊端：
>
>   > * 请求和响应头部未经压缩就发送，延迟比较大，只能压缩 body 部分
>   > * **服务端请求按照顺序响应，可能出现队头阻塞的问题**
>   > * 没有请求优先级的控制
>   > * 请求只能从客户端开始，服务端只能被动的响应



##### HTTP2.0 优化方案

###### HTTP/ 2.0 相比 HTTP/1.1 进步

* HTTP/2没有改动HTTP的应用语义，仍然使用HTTP的请求方法、状态码和头字段等规则，它主要修改了HTTP的报文传输格式。 HTTP/1.1协议以换行符作为纯文本的分隔符，而HTTP/2将所有传输的信息分割为更小的消息和帧，**并采用二进制格式对它们编码**，这些帧对应着特定数据流中的消息，他们都在一个TCP连接内复用。
* 主要是消息格式的不同。 **websocket的推送是纯粹的二进制流**。 h2的推送是HTTP消息，必须含有HTTP头部(会对头部进行二进制编码）、包体。
* Q:既然h2只是对头部进行压缩，那h2+pb压缩Body是不是可以进一步减少体积；我看有的公司使用tcp+pb，这种方式比h2+pb传输效率高吗？
* A: 1、可以的，grpc协议就是h2+pb。 2、tcp+pb是非常糟糕的传输协议，应用层协议应该有头部和包体两部分，这样负载均衡、中间件才能监控系统。pb只是编码格式，tcp只是传输层协议，这个系统的监控、高可用一定很有问题。

> * HTTP 1.1 主要存在的问题：
>
> 	> * 延迟难以下降：报文过于的臃肿
> 	> * 并发连接有限：Chrome 最大并发连接数量是6 个，同时每一个连接都要经历 TCP 和 TLS 的握手过程，以及 TCP 慢启动过程给流量带来的影响
> 	> * 队头阻塞问题：同一个连接只能在完成一个 HTTP 事务（请求和响应）后，才能处理下一个事务.
> 	> * HTTP 头部巨大且重复，由于 HTTP 协议是无状态的，每一个请求都得携带 HTTP 头部，特别是对于携带 cookie 的头部的， cookie 是非常大的。(需要**压缩优化**操作),**ASCII 头部编码效率太低，浪费了大量带宽**
> 	> * 不支持服务器推送消息，只有客户端需要获取通知的时候，通过定时器不断的拉取消息。
>
> * HTTP/ 2.0 相对于 HTTP 1.0 是在语法上进行更新（基本改变了HTTP 报文的传输格式），在语义上保持不变，例如依旧采用：请求方法，状态码，头字段等规则。
>
> * header 存在的问题：
>
> 	> * 含有很多固定的字段比如 ： Cookie，User Agent, Accept 这些字段怎么加起来也几百个字节，需要压缩处理
> 	> * 大量的请求和响应报文中有重复的字段，这样带宽就大量的被这些重复字段占用了，我们需要避免这样重复性
> 	> * 字段是 ASCII 码，效率低，我们需要改成二进制编码。

> * HTTP/2 是基于 HTTPS 的所以有安全的保障
>
> * **HPACK头部压缩**(HTTP2 主要是头部压缩，没有对 body 压缩，所以传输大文件，其实效率没有 http1.1高,因为h2 增加了很多的约束)
>
>   > * 静态表、动态表以及 huffman 编码三种编码技术（合称为 HPACK）
>   >
>   > * 静态表：
>   >
>   > * > * HTTP/2 将 61 个高频出现的头部，比如描述浏览器的 User-Agent、GET 或 POST 方法、返回的 200 SUCCESS 响应等，分别**对应 1 个数字再构造出 1 张表**，并写入 **HTTP/2 客户端与服务器的代码中**(在浏览器中)。由于它不会变化，所以也称为静态表（下面这张表已经写入 HTTP/2 客户端和服务端）。
>   >   > * ![img](https://static001.geekbang.org/resource/image/5c/98/5c180e1119c1c0eb66df03a9c10c5398.png?wh=635*850)
>   >   > * 使用静态 Huffman 编码，可以将 16 个字节的 test.taohui.tech 压缩为 11 个字节: 根据信息论，高频出现的信息用较短的编码表示后，可以压缩体积
>   >   > * 哈夫曼思想非常简单，就是让出现概**率更高的字符用更短的编码表示，出现概率低一些的字符则用更长的编码表示**
>   >   > * Huffman 树： 不同的字符编码间不能彼此成为对方的前缀
>   >   > * HPACK 采用的是**静态 huffman 编码**（对URL经过 huffman 编码再传输），HTTP/2.0 协议制定者利用一个很大的 HTTP Header 的 sample，统计了所有字符出现的频率，并基于此构建了一个 huffman 编码表，需要内置在服务端和客户端里，最多能带给我们大约 37.5% 的压缩率,如下面这张表：
>   >   > * ![img](https://static001.geekbang.org/resource/image/81/de/81d2301553c825a466b1f709924ba6de.jpg?wh=1164*1022)
>   >
>   > * 动态表：
>   >
>   > * > * 所有被缓存的头部及其标识数字会构成一张表，它与已**经传输过的请求有关，是动态变化的**，因此被称为动态表
>   >   > * 对某一头部以及其值进行缓存，并由服务端生成一个对应的数字索引，供客户端下次发送时标示这个不变的头部和对应的值
>   >   > * 对**首次出现的 HTTP 头部用一个数字标识，随后再传输它时只传递数字即可**，这就可以实现几十倍的压缩率(有点类似建立了一个 hash 表)
>   >   > * ![img](https://static001.geekbang.org/resource/image/69/e0/692a5fad16d6acc9746e57b69b4f07e0.png?wh=808*829)
>   >   > * **静态表本身不需要随着报文被发送，因为双方已经达成了共识。**
>   >   > * **动态表不需要发送，只要双方对于首次出现的HTTP头部，用同样的规则去构建动态表即可。HTTP2中的动态表就是这样的，两端基于相同的规则，对首次出现的Header构建。**
>   >   > * **动态表生效得有一个前提：必须在一个会话连接上反复传输完全相同的 HTTP 头部。如果消息字段在 1 个连接上只发送了 1 次，或者反复传输时字段总是略有变动，动态表就无能为力了**
>   >   > * protobuf解决的是body的序列化空间效率，hpack解决的是header的空间效率，两者不冲突。
>   >
>   > * 如果发送多个重复的相同的头部，会消除重复的头部，提高速度，使用的是索引表，只传输索引号
>
> * 二进制格式：
>
>   > * 不是使用纯文本格式的传输了，使用二进制的格式传输，对计算机更友好，增加传输的效率
>
> * 并发传输
>
>   > * HTTP 请求和响应都被称为 Message 消息，它由 HTTP 头部和包体构成，承载这二者的叫做 Frame 帧，它是 HTTP/2 中的最小实体.
>   > * **HTTP 消息可以由多个 Frame 构成，以及 1 个 Frame 可以由多个 TCP 报文构成（TCP MSS 通常小于 1.5K）**
>   > * 解决 HTTP 1.1 的队头阻塞问题，但是同时 HTTP 2.0 自身也存在队头阻塞问题，即：HTTP 基于 TCP 连接的，TCP 是流式传输方式的，TCP 必须保证接收到的数据是连续的且是完整的，这样内核才会将缓冲区的数据交给 HTTP 应用，所以，当前一个字节的数据没有被接收到，那么后续的字节数据只能存放在内核的缓冲区中，直到这一数据被拿到，应用层才能从内核中拿到数据，这是 **TCP 层面的问题**
>   > * HTTP 1.1 的队头阻塞问题：请求-响应模型，**同一个连接过程中，HTTP 需要完成一个事务之后**(现在同一个连接可以并发传输报文），才能处理下一个问题。同理，在等待响应的过程中，是没法做其他事的，如果响应一直不来，后续的请求是不会发送的，形成队头阻塞。
>   > * HTTP 2 使用的是 **多个  Stream 复用同一条 TCP 连接，达到并发的效果**，提高 HTTP 的传输效率。
>   > * **当 HTTP/2 实现 100 个并发 Stream 时，只经历 1 次 TCP 握手、1 次 TCP 慢启动以及 1 次 TLS 握手，但 100 个 TCP 连接会把上述 3 个过程都放大 100 倍！**
>   > * ![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http2/stream.png)
>   > * HTTP 报文可以由多个 Frame 组成（基本组成单位），一个 frame 可以由多个 TCP 报文组成。
>   > * 数据包不是按序发送的，需要对数据包进行标记，客户端发送的标记奇数，服务端接受的标记偶数，表示属于某一个响应，同时可以指定数据流的优先级,设置优先发送哪些数据流
>   > * **因为当 HTTP/2 实现 100 个并发 Stream 时，只需要建立一次 TCP 连接，而 HTTP/1.1 需要建立 100 个 TCP 连接，每个 TCP 连接都要经过TCP 握手、慢启动以及 TLS 握手过程，这些都是很耗时的。**
>   > * **不同的 Stream 帧可以乱序发送**的（因此可以并发不同的 Stream), 因为每一个帧的头部都带有 Stream ID信息，所以可以**根据 ID 信息有序组装 HTTP 报文**，但是同一个 Stream 中的 frame 需要有序的发送，因为不存在 ID 值保证其有序的接受.
>   
> * HTTP2.0 最大问题：
>
> * HTTP/2 的最大问题来自于它下层的 TCP 协议。由于 TCP 是字符流协议，在前 1 字符未到达时，**后接收到的字符只能存放在内核的缓冲区里**，即使它们是并发的 Stream，应用层的 HTTP/2 协议也无法收到失序的报文，这就叫做队头阻塞问题。
>
> * **protobuf是对请求body进行了压缩，http2是对请求的header进行压缩**。**http2是应用层协议，而pb只是纯粹的消息编码工具**
>
> * **protobuf解决的是body的序列化空间效率，hpack解决的是header的空间效率，两者不冲突。**





##### HTTP 3.0 相比 HTTP 2.0 的优势

> * HTTP 2.0 的问题：
>
> 	> * 队头阻塞 （TCP 层面的问题）
> 	> * TCP 和 TLS 的握手延时（同时 TCP 拥塞控制，有一个慢启动过程，它会对TCP连接产生减速的效果）
> 	> * 网络迁移需要重新连接（因为如果 IP 或者 端口号发生改变了，那么我们的 TCP 连接个 TLS 握手需要重新进行，又需要 3 个 RTT，非常的耗时).不利于移动设备，例如将 WiFi 转成 4G
>
> * HTTP 3.0 将**传输层协议换成 UDP**
>
> * 基于传输层的 UDP，在应用层实现了 QUIC 协议，QUIC 具有 TCP 的连接管理，拥塞窗口和流量控制的网络特性，将不可靠的 UDP 协议变成“可靠”的协议了。
>
> * QUIC 协议的优点：
>
> 	> * 无队头阻塞
> 	> * 更快的连接建立
> 	> * 连接迁移
>
> * ![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http3/quic%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.png)
>
> * QUIC 采用 HTTP2 的多路复用技术，每一路流之间互不干扰，某一路流阻塞了，其他路流不影响。数据报文都**有一个序号唯一的标识，保证可靠性**（因为某一路流丢失了，那一路会阻塞）。
>
> * QUIC 协议需要握手，只不过这个握手只需要 1个 RTT ，握手的目的是确认双方的连接 ID，连接迁移就是基于连接 ID实现的。
>
> * HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是**QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果**。
>
> * QUIC 协议不是使用 四元组的方式来建立连接的，而是通过 连接 ID 标识两个端点，客户端和服务端可以选择一组 ID 来标识自己，只要断网的时候，没有出现上下文消息变化（ID 标识 和 TLS 秘钥等），没有迁移成本.
>
> * ![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http3/http3frame.png)
>
> * TCP  队头阻塞：因为 TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且有序的，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，**应用层也无法从内核中读取到这部分数据**，从 HTTP 视角看，就是请求被阻塞了。
>
> * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/http2%E9%98%BB%E5%A1%9E.jpeg)
>
> * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http3/tcp%E9%98%9F%E5%A4%B4%E9%98%BB%E5%A1%9E.gif)
>
> * 其中 packet 3 在网络中丢失了，即使 packet 4-6 被接收方收到后，**由于内核中的 TCP 数据不是连续的，于是接收方的应用层就无法从内核中读取到，只有等到 packet 3 重传后，接收方的应用层才可以从内核中读取到数据**，这就是 HTTP/2 的队头阻塞问题，是在 TCP 层面发生的。
>
> * QUIC 协议：基于 UDP
>
> * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/quic%E6%97%A0%E9%98%BB%E5%A1%9E.jpeg)
>
> *  **QUIC 协议会保证数据包的可靠性**，每个数据包都有一个序号唯一标识。当**某个流中的一个数据包丢失了**，即使该流的其他数据包到达了，数据也无法被 HTTP/3 读取，直到 QUIC 重传丢失的报文，数据才会交给 HTTP/3。（不影响其他流数据包提交到应用层）
>
> * 而其他流的数据报文只要被完整接收，HTTP/3 就可以读取到数据。这与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，**其他流也会因此受影响**。
>
> * 所以，QUIC 连接上的**多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响**。
>
> * **TCP 和 TLS 握手延迟：**
>
> * 发起 HTTP 请求时，需要经过 TCP 三次握手和 TLS 四次握手（TLS 1.2）的过程，因此共需要 3 个 RTT 的时延才能发出请求数据。
>
> * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http3/TCP%2BTLS.gif)
>
> * **TCP 由于具有「拥塞控制」的特性，所以刚建立连接的 TCP 会有个「慢启动」的过程**，它会对 TCP 连接产生**"减速"效果**
>
> * HTTP/3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程**只需要 1 RTT**，握手的目的是为确认双方的「连接 ID」，连接迁移就是**基于连接 ID 实现**的。
>
> * HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是**QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果**。
>
> * HTTP/3 当会话恢复时，有效负载数据与第一个数据包一起发送，可以做到 0-RTT：
>
> * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http3/0-rtt.gif)
>
> * **连接迁移：** 
>
> * 基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。
>
> * 移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接，而建立连接的过程包含 **TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程**，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。
>
> * **QUIC 协议没有用四元组的方式来“绑定”连接**，而是通过**连接 ID**来标记通信的两个端点，**客户端和服务器可以各自选择一组 ID 来标记自己**
>
> * 即使移动设备的网络变化后，导致 IP 地址变化了，只要**仍保有上下文信息（比如连接 ID、TLS 密钥等）**，就可以**“无缝”地复用原连接**，消除重连的成本，没有丝毫卡顿感，达到了**连接迁移**的功能



#### HTTPS 优化手段

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https%E4%BC%98%E5%8C%96/%E4%BC%98%E5%8C%96https%E6%8F%90%E7%BA%B2.png)

* 





#### RSA 握手解析

* TLS 握手过程 ：HTTPS 是应用层的协议，需要先进行 TCP 的连接建立，之后走 TLS 的连接过程，之后才可以建立安全的连接。
* RSA 握手过程（RSA 是一种非对称加密算法），可以用来进行秘钥的交换。
* 传统的 TLS 握手基本都是使用 RSA 算法来实现秘钥的交换，在将 TLS 证书部署到服务端的时候，证书文件中包含了一对公私钥，其中公钥在 TLS 的握手阶段是需要传给客户端的，但是私钥是一直会留在服务端的，一定得保证私钥不能被窃取。
* RSA 算法：客户端生成提供随机秘钥，使用服务端的公钥加密然后传给服务端。非对称加密：服务端的私钥只有服务端自己有，所以只有服务端可以对发来的使用他自己的公钥加密的文件进行解密，别人解不开，这样服务端解密之后，服务端和客户端都得到相同的秘钥。

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/https_rsa.png)



* TLS 第一次握手：

	> * 客户端发送 clienthello 消息，同时生成随机数
	> * 这个消息包含： TLS 版本号，支持的密码套件列表以及生成的随机数。
	> * 随机数会被保存在服务端中，这个是生成对称加密秘钥的材料之一。

* TLS 第二次握手

  > * 服务端发送一个 Server hello 消息，之前生成一个随机数
  >
  > * 消息包含： 服务器确认的 TLS 版本号，支持的密码套件以及生成的 随机数
  >
  > * 两边都发送了自己的随机数，这个随机数是后续作为生成 会话秘钥 的条件，所谓的**会话秘钥就是数据传输的时候，所使用的对称加密秘钥。**
  >
  > * 服务端发送自己的数字证书，这个证书包含**公钥**、**持有者的信息**、CA 机构的信息、证书有效期、CA 机构如何对这份文件数字签名以及使用的算法。
  >
  > * 客户端从发来的数字证书，通过校验证书是否合法（CA 的公钥之前已经存放在浏览器中或者 OS 中） ，之后取出数字证书的 公钥 ：这个公钥是服务端的公钥。
  >
  > * Q： 客户端拿到了服务端的数字证书后，要怎么校验该数字证书是真实有效的呢？
  >
  > * > * 说简单些，**证书就是用来告诉客户端，该服务端是否是合法的**，因为只有证书合法，才代表服务端身份是可信的
  >   > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/%E8%AF%81%E4%B9%A6%E7%9A%84%E6%A0%A1%E9%AA%8C.png)
  >   > * 首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 计算，得到一个 Hash 值；
  >   > * 然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名；
  >   > * 最后将 Certificate Signature 添加在文件证书上，形成数字证书；
  >
  > * 证书链：
  >
  > * > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/baidu%E8%AF%81%E4%B9%A6.png)
  >   > * 客户端收到 baidu.com 的证书后，发现这个证书的签发者不是根证书，就无法根据本地已有的根证书中的公钥去验证 baidu.com 证书是否可信。于是，客户端根据 baidu.com 证书中的**签发者**，找到该证书的颁发机构是 “GlobalSign Organization Validation CA - SHA256 - G2”，**然后向 CA 请求该中间证书**。
  >   > * 请求到证书后发现 “GlobalSign Organization Validation CA - SHA256 - G2” 证书是由 “GlobalSign Root CA” 签发的，由于 “GlobalSign Root CA” 没有再上级签发机构，说明它是根证书，也就是自签证书。**应用软件会检查此证书有否已预载于根证书清单上**，如果有，则可以利用根证书中的公钥去验证 “GlobalSign Organization Validation CA - SHA256 - G2” 证书，**如果发现验证通过，就认为该中间证书是可信的。**
  >   > * “GlobalSign Organization Validation CA - SHA256 - G2” 证书被信任后，可以使用 “GlobalSign Organization Validation CA - SHA256 - G2” 证书中的公钥去验证 baidu.com 证书的可信性，**如果验证通过，就可以信任 baidu.com 证书。**
  >   > * 由于**用户信任操作系统或浏览器的软件商，所以由软件商预载了根证书的 GlobalSign 都可被信任**。
  >   > * 为什么需要中间证书？
  >   > * A： 保护根证书，如果直接采用根证书签发证书，一旦发生根证书泄露，将造成极大的安全问题，所以目前根证书都需要离线保存，如果需要根证书的签名，则必须通过人手工的方式，如果用根证书在线签发证书是不允许的。
  >   > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/%E7%94%A8%E6%88%B7%E4%BF%A1%E4%BB%BB.png)
  >   > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/%E8%AF%81%E4%B9%A6%E9%93%BE.png)
  >   > * 

* TLS 第三次握手：

  > * 客户端会生成一个新的随机数，之后使用服务端提供的 RSA 公钥（之前服务器发来的证书里面的公钥）加密这个随机数，生成一个 pre-master,
  > * 会话秘钥的组成：客户端随机数 + 服务端随机数 + pre-master 通过协商的 加密算法 ： RSA 加密算法
  > * 第三次握手传输的就是这个 pre-master
  > * 会话秘钥也称为对称秘钥，用于后续的 HTTP 请求/响应 的数据加密
  > * 生成完会话秘钥之后，客户端发送 change ciper spec ,告诉服务端开始使用加密的方式传输信息。
  > * 之前发送的 TLS 握手数据都是明文传输的，之后采用的都是秘钥加密的秘文。
  > * **服务器和客户端有了这三个随机数（Client Random、Server Random、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」**。
  > * 注意是彼此双方都生成一个 会话秘钥（当然这个会话秘钥是相同的）

* TLS 第四次握手

	> * 服务端回复一个确认消息。
	> * 握手正式完成，之后就使用 会话秘钥的方式 加密和解密 HTTP 的请求和响应报文了。

* RSA 算法的问题：

	> * 服务端的私钥泄漏了，那么第三方就可以截获破解所有的 TLS 通讯秘文的具体数据，形成安全问题
	> * **使用 RSA 密钥协商算法的最大问题是不支持前向保密**。
	> * 所以一旦服务端的私钥泄漏了，**过去被第三方截获的所有 TLS 通讯密文都会被破解**。



#### TLS 优化

* **前向安全的原因**是私钥不传递而且每次都会变，所以无法破解以前的，以前的私钥在机器的内存里，早就消失或变化了
* 在 TLS1.2 的握手中，先要通过 Client Hello 和 Server Hello 消息协商出后续使用的加密算法，再互相交换公钥并计算出最终密钥。
* TLS1.3 中把 Hello 消息和公钥交换合并为一步，这就减少了一半的握手时间
* ![img](https://static001.geekbang.org/resource/image/49/20/4924f22447eaf0cc443aac9b2d483020.png?wh=1832*1290)
* Q: 那 TLS1.3 握手为什么只需要 1 个 RTT 就可以完成呢？
* A: 因为 TLS1.3 支持的密钥协商算法大幅度减少了，这样，客户端尽可以把常用 DH 算法的公钥计算出来，并与协商加密算法的 HELLO 消息一起发送给服务器，服务器也作同样处理，这样仅用 1 个 RTT 就可以协商出密钥。
* 



####  数字签名和证书

* **数字签名和数字证书只用于TSL/SSL的握手阶段**，主要是**保证服务器的公钥能够正确地传给浏览器**（不被中间人伪装发送假的公钥）
* 服务器去CA机构申请证书，证书中包含了要发给客户端的公钥、签发者、到期时间等等信息。如果这样简单地把证书发给浏览器，**中间人可以轻松地修改成自己的公钥**（在网络中被截获），之后的通信就是不安全的了。于是需要一定的加密手段，这里的做法就是使用数字签名：将证书的信息利用摘要算法计算出摘要之后，用CA的秘钥进行加密，生成数字签名。

##### 完整性

* 摘要算法保证了“数字摘要”和原文是完全等价的。所以，我们只要在**原文后附上它的摘要**，就能够保证数据的完整性。

* 不过摘要算法不具有机密性，**如果明文传输**，那么**黑客可以修改消息后把摘要也一起改了**，网站还是鉴别不出完整性。(因为原文是明文传输的)
* **真正的完整性必须要建立在机密性之上，在混合加密系统里用会话密钥加密消息和摘要**，这样黑客无法得知明文，也就没有办法动手脚了。

![img](https://static001.geekbang.org/resource/image/c2/96/c2e10e9afa1393281b5633b1648f2696.png?wh=2000*695)

##### 数字签名

* 存在的原因： 加密算法结合摘要算法，我们的**通信过程**可以说是比较安全了。但这里还有漏洞，就是**通信的两个端点**（endpoint）。
* 通过哈希算法可以确保内容不会被篡改，**但是并不能保证「内容 + 哈希值」不会被中间人替换，因为这里缺少对客户端收到的消息是否来源于服务端的证明**。
* eg : 就像一开始所说的，黑客可以伪装成网站来窃取信息。而反过来，他也可以伪装成你，向网站发送支付、转账等消息，**网站没有办法确认你的身份**，钱可能就这么被偷走了。

* 所以，**真正的完整性必须要建立在机密性之上**，在混合加密系统里用会话密钥**加密消息和摘要**，这样黑客无法得知明文，也就没有办法动手脚了。

* 在 TLS 里有什么东西和现实中的签名、印章很像，只能由本人持有，而其他任何人都不会有呢？只要用这个东西，就能够在数字世界里证明你的身份?

* > * 这个东西就是非对称加密里的“私钥”，使用**私钥再加上摘要算法**，就能够实现“数字签名”，同时实现“身份认证”和“不可否认”。
  > * 数字签名的原理其实很简单，就是把公钥私钥的用法反过来，之前是公钥加密、私钥解密，现在是**私钥加密、公钥解密**。
  > * 这两个密钥可以**双向加解密**的，比如可以用公钥加密内容，然后用私钥解密，也可以用私钥加密内容，公钥解密内容。
  > * 流程的不同，意味着目的也不相同：
  >   - **公钥加密，私钥解密**。这个目的是为了**保证内容传输的安全**，因为被公钥加密的内容，其他人是无法解密的，只有持有私钥的人，才能解密出实际的内容；
  >   - **私钥加密，公钥解密**。这个目的是为了**保证消息不会被冒充**，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的。
  > * 所以非对称加密的用途主要在于**通过「私钥加密，公钥解密」的方式，来确认消息的身份**，我们常说的**数字签名算法**，就是用的是这种方式，不过私钥加密内容不是内容本身，而是**对内容的哈希值加密**。
  > * ![img](https://static001.geekbang.org/resource/image/84/d2/84a79826588ca35bf6ddcade027597d2.png?wh=1375*1252)
  > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP/%E6%95%B0%E5%AD%97%E7%AD%BE%E5%90%8D.png)

##### 数字证书和 CA

* 上述的公钥交换存在什么问题？

* > * “公钥的信任”问题。因为谁**都可以发布公钥**，我们还缺少**防止黑客伪造公钥**的手段，也就是说，怎么来判断这个公钥就是你或者某宝的公钥呢？
  >
  > * 找一个公认的可信第三方，让它作为“信任的起点，递归的终点”，**构建起公钥的信任链**。
  >
  > * 这个“第三方”就是我们常说的 CA（Certificate Authority，证书认证机构）。它就像网络世界里的公安局、教育部、公证中心，具有极高的可信度，由它来**给各个公钥签名，用自身的信誉来保证公钥无法伪造，是可信的**。
  >
  > * CA 怎么证明自己呢？
  >
  > * > * 这还是信任链的问题。小一点的 CA 可以让大 CA 签名认证，但链条的最后，也就是 Root CA，就只能自己证明自己了，这个就叫“自签名证书”（Self-Signed Certificate）或者“根证书”（Root Certificate）
  >   > * <img src="https://static001.geekbang.org/resource/image/8f/9c/8f0813e9555ba1a40bd2170734aced9c.png?wh=1300*1292" alt="img" style="zoom:50%;" />
  >   > * 有了这个证书体系，**操作系统和浏览器都内置了各大 CA 的根证书**，上网的时候只要服务器发过来它的证书，就可以验证证书里的签名，顺着证书链（Certificate Chain）**一层层地验证**，直到找到根证书，就能够确定证书是可信的，从而里面的公钥也是可信的。

  
  
* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP/22-%E6%95%B0%E5%AD%97%E8%AF%81%E4%B9%A6%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" alt="数子证书工作流程" style="zoom:80%;" />

##### 总结

1. 摘要算法用来实现完整性，能够为数据生成独一无二的“指纹”，常用的算法是 SHA-2；
2. 数字签名是私钥对摘要的加密，可以由公钥解密后验证，实现身份认证和不可否认；
3. 公钥的分发需要使用数字证书，必须由 CA 的信任链来验证，否则就是不可信的；
4. 作为信任链的源头 CA 有时也会不可信，解决办法有 CRL、OCSP，还有终止信任。









#### TCP 协议

![TCP 头格式](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzYuanBn?x-oss-process=image/format,png)

> 基本知识：

* IP 层是不可靠的，所以需要保障网络数据包的可靠性，那么就需要 TCP 协议来负责。
* TCP 可以确保接收端的网络包是：**无损坏的、无间隔的、非冗余的以及按序的**
* TCP ： **可靠的、面向连接的（一对一主机）、字节流**的协议
* *ACK*：该位为 `1` 时，**「确认应答」**的字段变为有效，TCP 规定除了最初建立连接时的 `SYN` 包之外该位必须设置为 `1`
* *RST*：该位为 `1` 时，表示 TCP 连接中**出现异常必须强制断开连接**。
* *SYN*：该位为 `1` 时，表示**希望建立连接**，并在其「序列号」的字段进行序列号初始值的设定。
* *FIN*：该位为 `1` 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 `FIN` 位为 1 的 TCP 段。



* 什么是 TCP 连接？

> * 用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 **socket 、序列号和窗口大小** 称为连接。
> * Socket ： 由 IP 地址和端口号组成
> * 序列号： 用来解决乱序问题
> * 窗口大小：用来做流量控制



* 如何确定一个 TCP 连接

> * 源端口 + 目的端口 + 源 IP + 目的 IP



* 最大的 TCP 连接数量：

> * 客户端的数量 ： 最大的 TCP 连接数量 ： 客户端 IP个数 * 客户端的端口数
> * 理论上 IP个数 2 ^32 端口数 ： 2 ^16 所以最大是 ： 2^48个
> * 服务端是不设上线的，**但是受到内存的限制以及每一个TCP 需要一个文件描述符，所以也受文件描述符个数的限制**





##### 三次握手



* TCP 三次握手和状态变迁

![img](https://pic1.zhimg.com/80/v2-07b64eaf858949d47eb4f849ef4c1da6_720w.jpg?source=d16d100b)



> * 上面的序列号服务端和客户端发送的序列号都是随机数，初始化的， 同时均需要将 SYN 设置成 1.
> * 注意服务端和客户端当接收到消息均需要改变自己的状态
> * 服务端和客户端在一开始均需要发送 SYN 表示希望建立连接
> * 只有收到对方的报文才需要发送 ACK 报文，表示 **确认应答**，这一位在之后一直有效
> * 第三次握手可以携带数据，前两次握手是不可以携带数据的。





###### TCP 为什么三次握手不是两次和四次

> * 传统回答： 因为三次握手才能保证双方具有接收和发送的能力。
>
> * 以后回答 ： 
>
>   > * 三次握手的原因： 
>   >
>   > * **三次握手才可以阻止重复的历史连接的初始化 （主要原因）**
>   >
>   >   > * 当一个旧的 SYN 报文比 最新的 SYN 报文早到达了服务端
>   >   > * 此时服务端就会回一个 SYN + ACK 报文给客户端
>   >   > * 客户端根据自身上下文，判断当前的 seq 是否是合理的，不合理那么发送一个 RST 报文给服务端，终止连接，因为通常第三次连接发送的是 ACK 报文
>   >
>   > * 三次握手才可以同步双方的初始序列号
>   >
>   >   > * 序列号非常重要，有以下作用：
>   >   > * 1. 接收方可以去除重复的数据
>   >   > * 2. 接收方可以根据数据包的序列号按序接收
>   >   > * 3. 可以标识发送出去的数据包中，哪些是已经被对方收到的
>   >   > * 两次连接只保证一方的初始序列号被对方成功接收，没法保证双方的初始化序列号都被顺利接受了
>   >
>   > * 三次握手才可以避免资源浪费
>   >
>   >   > * 两次握手可能导致：第一次连接请求网络阻塞了，之后又连接上了，但是客户端会忽略这个超时的请求，但是服务端不知道，就一直等，浪费服务器资源



###### 第一次握手丢失，第二次握手丢失，第三次握手丢失

> * 总结一下都是： **重传** ， 只不过重传的对象和时机不同
>
> * 第一次丢失：
>
> * > * 如果客户端迟迟收不到服务端的 SYN-ACK 报文（第二次握手），就会触发「超时重传」机制，重传 SYN 报文。
>   > * 重传间隔时间是：倍数递增的
>
> * 第二次丢失：
>
> * > * 因为第二次握手报文里是包含对客户端的第一次握手的 ACK 确认报文，所以，如果客户端迟迟没有收到第二次握手，那么客户端就觉得可能自己的 SYN 报文（第一次握手）丢失了，于是**客户端就会触发超时重传机制，重传 SYN 报文**。
>   > * 服务端就收不到第三次握手，于是**服务端这边会触发超时重传机制，重传 SYN-ACK 报文**。
>   > * * 客户端会重传 SYN 报文，也就是第一次握手，最大重传次数由 `tcp_syn_retries`内核参数决定；
>   > * * 服务端会重传 SYN-ACK 报文，也就是第二次握手，最大重传次数由 `tcp_synack_retries` 内核参数决定。
>
> * 第三次丢失：
>
> * > * **服务端**那一方迟迟收不到这个确认报文，就会触发超时重传机制，重传 SYN-ACK 报文，直到收到第三次握手，或者达到最大重传次数。





* TCP 为什么不4次握手

> 三次握手就已经理论上最少的可靠连接，所以不需要使用更多的通信次数



###### 为什么 C 和 S 的初始化序列号不一样

* 序列号的作用？
* A: 因为 TCP 是面向字节流的可靠协议，为了保证消息的**顺序性和可靠性**，TCP 为每个传输方向上的每个报文都赋予了一个编号，以便于**传输成功后确认、丢失后重传以及在接收端保证不会乱序**。



* A: **主要原因是为了防止历史报文被下一个相同四元组的连接接收。**
* 如果一个已经失效的连接被重用了，但是旧连接的历史报文还是在网络中，**如果序列号相同，那么无法判断这个报文是不是历史报文了，可能造成数据的错乱**。
* **序列号和初始化序列号并不是无限递增的，会发生回绕为初始值的情况，这意味着无法根据序列号来判断新老数据**。
* Q： TCP 四次挥手中的 TIME_WAIT 状态不是会持续 2 MSL 时长，历史报文不是早就在网络中消失了吗？
* A:  是的，如果能正常四次挥手，**由于 TIME_WAIT 状态会持续 2 MSL 时长，历史报文会在下一个连接之前就会自然消失。**
* **但是我们并不能保证每次连接都能通过四次挥手来正常关闭连接。**
* 假设每次建立连接，**客户端和服务端的初始化序列号都是从 0 开始**：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/isn%E7%9B%B8%E5%90%8C.png)



* > * 客户端和服务端建立一个 TCP 连接，在客户端发送数据包被网络阻塞了，然后超时重传了这个数据包，而此时服务端设备断电重启了，之前与客户端建立的连接就消失了，于是在收到客户端的数据包的时候就会**发送 RST 报文**。(重启连接都不存在了)
  > * 紧接着，客户端又与服务端**建立了与上一个连接相同四元组的连接**；
  > * 在新连接建立完成后，**上一个连接中被网络阻塞的数据包正好抵达了服务端**，刚好该数据包的序列号正好是在服务端的接收窗口内，所以该数据包会被服务端正常接收，就会造成数据错乱。

* 如果每次建立连接客户端和服务端的初始化序列号都「不一样」，就有**大概率因为历史报文的序列号「不在」对方接收窗口**，从而很大程度上避免了历史报文。

* 如果每次建立连接客户端和服务端的初始化序列号都「一样」，就有**大概率遇到历史报文的序列号刚「好在」对方的接收窗口内**，从而导致历史报文被新连接成功接收。

* 所以，每次初始化序列号不一样能够很大程度上避免历史报文被下一个相同四元组的连接接收，注意是很大程度上，并不是完全避免了。

* Q : 如何完全避免了历史报文被接收的问题？

* A: 客户端和服务端的初始化序列号都是随机生成，能很大程度上避免历史报文被下一个相同四元组的连接接收，然后又引入时间戳的机制，从而完全避免了历史报文被接收的问题。









###### 为什么存在 IP分片还需要 TCP 分片

> * IP 是不可靠的，所以一个 IP 分片丢失，整个 IP 报文的所有分片都需要重传。
> * IP 没有超时重传机制，所以需要TCP来完成超时和重传，TCP 需要重传整个 报文， 效率比较低
> * 使用 TCP 分片，如果丢了，只需要重传这个分片就可以了,不用重传整个报文了。
> * 当 IP 层有一个超过 `MTU` 大小的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进行分片，把数据分片成若干片，保证每一个分片都小于 MTU。把一份 IP 数据报进行分片以后，**由目标主机的 IP 层来进行重新组装后，再交给上一层 TCP 传输层。**
> * 为了达到最佳的传输效能 TCP 协议在**建立连接的时候通常要协商双方的 MSS 值**，当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，**自然也就不用 IP 分片了**。(直接进行 TCP 的分片， IP 层可以忽略不用分片)





###### 什么是 SYN 攻击

> * SYN 攻击：攻击者伪造不同的 IP地址发送连接 SYN报文， 服务器收到连接请求进入到 SYN_RECV 状态，但是客户端不做 ACK应答，久而久之占满服务端的 SYN 接收队列，使得服务器没法提供正常服务
>
> * 解决办法：
>
>   > * 修改 linux 参数控制队列的大小以及当队列满的时候需要咋处理，一般就是丢弃
>   >
>   > * Linux 内核的SYN（未建立连接）队列 和 accept （已建立连接队列）如何工作？
>   >
>   >   > * 当SYN 对列满了之后，还有SYN发送过来，发送 syn cookie 数值返回到客户端。
>   >   > * 服务器收到 客户端的应答报文判断这个 ACK 包是否合法，合法放入到 accpet 队列中，之后调用 accept() socket 接口，从 accept 队列中取出数据.



###### TCP 两次握手没法阻止历史连接

> * 两次握手之后，客户端是可以根据返回的确认报文来判断是不是当前发的报文，还是历史连接报文。
> * 但是第一次连接之后，服务端的状态就变成 Established 这样服务端就可以发送消息了，这样如果客户端收到的是历史连接会返回一个 带有 RST 的报文，阻止本次连接，但是服务端在这之前已近发送数据了，这样就导致没法阻止历史连接了。
> * 服务端白白浪费资源，发送了一些数据。
> * 解决办法，三次握手

##### 四次挥手

* TCP 断开连接

![img](https://pic1.zhimg.com/80/v2-2085b747725cd09fb01334cd29774b6d_720w.jpg?source=d16d100b)

> * 每一个方向都有一个 FIN 和一个 ACK 
> * TCP 主动关闭连接的，才有 TIME_WAIT 状态



###### 为什么是 四次

> * 因为发送了 FIN之后才表示当前端不需要再发送数据了，但是服务端可能在ACK 之后还需要发送数据，所以需要单独自己发送 FIN 表示连接结束。总的来说就是服务端可能还需要发送数据
> * 之所以绝大数时候我们看到的都是四次挥手，是因为收到fin后，知道对方要关闭了，然后OS通知应用层要关闭，**这里应用层可能需要做些准备工作，可能还有数据没发送完，所以内核先回ack**，等应用准备好了主动调close时再发fin 。 握手过程没有这个准备过程所以可以立即发送syn+ack（把这里的两步合成一步了）。 **内核收到对方的fin后，只能ack，不能主动替应用来fin，因为他不清楚应用能不能关闭**。





###### 为什么需要 time_wait 这个参数

* 如果第四次挥手的 ACK 报文没有到达服务端，服务端就会重发 FIN 报文，**重发次数仍然由前面介绍过的 `tcp_orphan_retries` 参数控制。**
* 主动发起关闭连接的一方才会有 TIME_WAIT 状态
* 一般情况下，都是「客户端」所处的状态；「服务器端」一般设置「不主动关闭连接」。
* 「服务器」一般设置为「不主动关闭连接」，服务器通常执行「被动关闭」；
* 但 HTTP 请求中，http 头部 connection 参数，可能设置为 close，则，服务端处理完请求会主动关闭 TCP 连接，

**关于 HTTP 请求中，设置的主动关闭 TCP 连接的机制：TIME_WAIT的是主动断开方才会出现的，所以主动断开方是服务端？**

*  A: 1.答案是是的。在HTTP1.1协议中，有个 Connection 头，Connection有两个值，close和keep-alive，这个头就相当于客户端告诉服务端，服务端你执行完成请求之后，是关闭连接还是保持连接，**保持连接就意味着在保持连接期间，只能由客户端主动断开连接**。还有一个keep-alive的头，设置的值就代表了服务端保持连接保持多久。
* 2.HTTP默认的**Connection值为close**，那么就意味着**关闭请求的一方几乎都会是由服务端这边发起的**。那么这个**服务端产生TIME_WAIT过多**的情况就很正常了。
* 3.虽然HTTP默认Connection值为close，但是，现在的浏览器发送请求的时候一般都会设置Connection为keep-alive了。所以，也有人说，现在没有必要通过调整参数来使TIME_WAIT降低了。





* 原因一：**防止历史连接中的数据，被后面相同四元组的连接错误的接收**（处理延迟到达的报文）

* > * ![TIME-WAIT 时间过短，收到旧连接的数据报文](https://img-blog.csdnimg.cn/img_convert/6385cc99500b01ba2ef288c27523c1e7.png)
  >
  > * > - 服务端在关闭连接之前发送的 `SEQ = 301` 报文，被网络延迟了。
  >   > - 接着，服务端以**相同的四元组重新打开了新连接**，前面被延迟的 `SEQ = 301` 这时抵达了客户端，而且该数据报文的序列号刚好在客户端接收窗口内，因此客户端会正常接收这个数据报文，但是这个数据报文是上一个连接残留下来的，这样就产生数据错乱等严重的问题。
  >
  > * 为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了 TIME_WAIT 状态，状态会持续 `2MSL` 时长，这个时间**足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。**

* 原因二： **保证「被动关闭连接」的一方，能被正确的关闭**

* > * TIME-WAIT 作用是**等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。**
  > * 假设**客户端没有 TIME_WAIT 状态，而是在发完最后一次回 ACK 报文就直接进入 CLOSED 状态**，如果该 ACK 报文丢失了，服务端则重传的 FIN 报文，而这时客户端已经进入到关闭状态了，在收到服务端重传的 FIN 报文后，就会回 RST 报文。
  > * ![TIME-WAIT 时间过短，没有确保连接正常关闭](https://img-blog.csdnimg.cn/img_convert/3a81c23ce57c27cf63fc2b77e34de0ab.png)
  > * **服务端收到这个 RST 并将其解释为一个错误**（Connection reset by peer），这对于一个可靠的协议来说不是一个**优雅的终止方式**。
  > * 但是你可能会说**重新发送的 ACK 还是有可能丢失啊，没错，但 TCP 已经等待了那么长的时间了，已经算仁至义尽了**。



* TIME_WAIT 为什么是 2MSL (linux 60s)

> * MSL : 报文的最大生存时间，TLL 是 IP报文头部的字段，表示的是 IP报文进过的路由表数量，每经过一个路由器数量减一，值为 0 丢弃报文， MSL大于 TLL 以确保报文自然死亡
> * `MSL` 是 Maximum Segment Lifetime，**报文最大生存时间**，它是任何报文在网络上存在的最长时间，**超过这个时间报文将被丢弃**。
> * 为什么是 2MSL， 因为服务端超时未接收到 ACK，会触发超时重发 FIN 报文，一来一回正好 2 个 MSL
> * 如果客户端之后收到了服务端发送的 FIN报文，2MSL会重新计时。
> * Linux 默认一个 MSL是 30s 。
> * 使用 2MSL 可以保证双方的连接都可以正常的关闭



> * 保证**被动连接的一方可以正确的关闭连接**，即保证最后的 ACK 能够被被动关闭方接受，确保可以正常的关闭
>
> * **可能出现服务端因为没有接受到 ACK 报文，从而一直处于 LAST_ACK 状态，浪费服务端资源**
>
> * 防止具有四元祖的旧数据包被接受
>
>   > * 就是当这个报文在关闭连接之后，还是在网络中存活在，到了下一次使用相同的端口号的一个相同的 TCP 连接出现，这样就会导致之前阻塞的报文会被接收端接受，这样就会导致数据错乱的问题
>   > * **使用 2MSL 足够让两个端口的数据包都被丢弃，使得原来的数据包在网络中自然消失**，再次出现的数据包一定是新建立产生的。



* TIME_WAIT 过多的危害

> * 对内存的资源的占用
> * 对端口资源的占用，**一个 TCP 连接至少消耗一个端口，当占着一个端口的时候， tcp 没法创建新的连接，因为 TCP 的端口数量是有限的.**
> * 客户端受端口资源的限制：因为当客户端的 TIME_WAIT 过多，端口资源被占用，客户端端口是65536 个，无法创建新的连接
> * 服务端的系统资源限制： 理论上服务端可以建立非常多的连接，但是服务端监听的端口只有一个，会把连接请求扔给线程处理，但是多了，线程也处理不过来了，所以当出现大量的 TIME_WAIT 会导致系统资源占满，导致处理不过来新的连接.



* TCP 重传机制

> * 在复杂的网络，数据在网络中传输的过程中， TCP 针对数据包的丢失情况，会用重传机制解决.
> * 超时重传、快速重传、SACK、D-SACK
>
> * 超时重传：
>
>   > * 数据包丢失
>   > * 确认应答丢失



* 超时时间 RT0 应该设置多少？

> * 超时时间 RTO 的值需要略大于报文往返时间 RTT
> * 每当发生一次超时重传的时候，我们需要将下一次超时时间的间隔设置成先前值的两倍，两次超时说明网络的环境太差，不宜反复发送



* 快速重传机制：

> * 不是以时间作为驱动，而是以数据作为驱动，当某个数据包丢了，服务端会发送三个 Ack ，确认上一次收到的数据，然后告诉客户端发送丢失的报文
> * 重传可能只重传丢失的 seq2 也可能需要将 seq2 后面的所有报文都给重传一下.





###### 服务端没收到四次挥手会如何？









##### 流量控制

* 滑动窗口

> * 窗口大小表示的是无需等待确认应答，而可以继续的发送数据的最大值
> * 这个窗口是操作系统开的一个缓冲区，在没有收到应答之前这个缓冲区中的内容不能清除，收到了应答才可以将内容从缓冲区中清除
> * 滑动窗口采用的是累计应答的模式，就是即使某些中间数据丢失，但是最终确认报文应答确认好更大，表示的是我们收到了之前丢失的报文。



* 窗口大小如何确定？

> * TCP  头部存在一个字段 Window 表示的是窗口大小
> * 窗口的大小通常是由接收方的窗口大小决定的，发送端根据接收端发来的窗口大小来处理发送数据，而不会导致接收端处理不过来。



* 流量控制

> * TCP 提供一种方法可以让发送方根据接收方的实际接受能力**控制发送的数据量**。

##### 拥塞控制

* 拥塞控制

> * 拥塞控制是为了控制发送数据的大小，避免网络出现拥堵的情况
> * 控制的目的就是避免 发送方 的数据填满整个网络
> * 我们使用 拥塞窗口 的大小调整所需要发送的数据量的大小。



* 什么是拥塞窗口其和发送窗口的关系？

> * 拥塞窗口是发送方控制维护的一个状态量，它会根据网络的拥塞程度动态变化
>
> * 客户端发送窗口 ： swnd = min(cwnd, rwnd)
>
>   > cwnd 拥塞窗口的变化
>   >
>   > * 只要网络没有出现阻塞，cwnd 就会增大
>   > * 当网络出现阻塞的时候 cwnd 就会减小



* 如何判断网络是否出现阻塞

> * 发送方在规定的时间内没有收到确认报文，即发生了超时重传，就可以认为网络出现了阻塞





* 拥塞控制的四个算法

> * 慢启动
>
>   > * 慢启动指数增加拥塞窗口的值，这样可以发送的数据大小也是指数增加
>   > * 当超出了慢启动门限 ssthresh 时，使用拥塞避免算法
>
> * 拥塞避免
>
>   > * 拥塞窗口大小线性增长
>   > * 但是当网络出现拥塞的时候，需要发生数据包的重传，进入拥塞恢复阶段
>
> * 拥塞发生
>
>   > * ssthresh = cwnd / 2         同时将 cwnd = 1
>   > * 执行慢恢复算法，但是方式过于激进，会造成网络卡顿情况
>
> * 快速恢复
>
>   > * 连续发送三个 ACK 表示的是执行快速恢复
>   > * 之后从 cwnd = ssthresh /2 的位置开始重新传输,然后拥塞窗口一个一个增加



![快速重传和快速恢复](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/%E6%8B%A5%E5%A1%9E%E5%8F%91%E7%94%9F-%E5%BF%AB%E9%80%9F%E9%87%8D%E4%BC%A0.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

* 而**快速重传后没有使用慢启动算法**，而是拥塞避免算法，所以这又叫做快速恢复算法。

* ![img](https://pic3.zhimg.com/80/v2-de79bf2c38bddb0c1caf5768577648e2_720w.jpg)

* 快速恢复流程：

* 快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，**你还能收到 3 个重复 ACK 说明网络也不那么糟糕，**所以没有必要像 `RTO` 超时那么强烈。

* > - 拥塞窗口 `cwnd = ssthresh + 3` （ 3 的意思是确认有 3 个数据包被收到了）；
  > - 重传丢失的数据包；
  > - 如果再收到重复的 ACK，那么 cwnd 增加 1；
  > - 如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，**说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束**，可以回到**恢复之前的状态**了，也即再次进入拥塞避免状态；









* ISN 的作用是什么？

ISN 是可靠传输的重要因素

> * 接收方可以根据接收的序号按序接收
> * 接收方可以根据接收的需要，去除重复的序号
> * 发送方可以可以根据发送的数据，标识哪些数据是已经被接收到的（根据 ACK 报文的序列号）



* ISN 为什么需要随机？

> * 为了防止历史报文被下一个相同的四元组连接接收（主要方面）
>
>   > * ![img](https://img-blog.csdnimg.cn/img_convert/9cbee7c645dad8ee2a1ad5fd31008d3c.png)
>   > * 客户端和服务端建立好了一个连接但是在数据传输过程中数据被网络阻塞了，于是服务端发送 RST 报文终止连接了。
>   > * 接着客户端和服务端又建立了一个了连接，这个连接四元组和上一次的相同
>   > * 但是这个时候之前的阻塞报文到达了服务端，这个时候如果 序列号 相同，这个数据包就会被接收，这样会导致数据的错乱。
>   > * 上面会接受的原因： 因为序列号在接收窗口的范围内，就会出现大概率遇到历史报文的序列号正好出现在接收窗口的范围内。
>   > * 但是ISN 随机并不能完全的解决历史报文被下一个四元组接收，（需要使用时间戳解决历史报文的问题）。
>
> * 为了安全性，防止黑客伪造相同的 IP 发送 TCP连接
>
>   > * 因为当 ISN 是随机的，所以伪造的主机发送的 序列号 服务端可以收到，但是服务端发送的随机 ISN 伪造主机收不到，因为会发送到符合条件的非伪造主机，同时正常主机收到这个确认号，不是自己发的，发送 RST 报文。
>   > * ISN 非唯一，伪造主机可以伪造一个确认信号发给这个服务端，总而服务端通过三次握手建立起这个伪造的连接。



#### **TCP 和 UDP 区别：**

*1. 连接*

- TCP 是面向连接的传输层协议，传输数据前先要建立连接。
- UDP 是不需要连接，即刻传输数据。

*2. 服务对象*

- TCP 是一对一的两点服务，即一条连接只有两个端点。
- UDP 支持一对一、一对多、多对多的交互通信

*3. 可靠性*

- TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。
- UDP 是尽最大努力交付，不保证可靠交付数据。

*4. 拥塞控制、流量控制*

- TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。
- UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。

*5. 首部开销*

- TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 `20` 个字节，如果使用了「选项」字段则会变长的。
- UDP 首部只有 8 个字节，并且是固定不变的，开销较小。

*6. 传输方式*

- TCP 是流式传输，没有边界，但保证顺序和可靠。
- UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。

*7. 分片不同*

- TCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。
- UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。

**TCP 和 UDP 应用场景：**

由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于：

- `FTP` 文件传输；
- HTTP / HTTPS；

由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于：

- 包总量较少的通信，如 `DNS` 、`SNMP` 等；
- 视频、音频等多媒体通信；
- 广播通信；



#### TCP 粘包和拆包

* 粘包：
* 拆包：
* [TCP 如何解决粘包问题](#TCP 如何解决粘包问题)











#### CDN 

* 内容分发网络：是指一种透过[互联网](https://zh.wikipedia.org/wiki/互聯網)互相连接的电脑网络系统，利用最靠近每位用户的服务器，更快、更可靠地将音乐、图片、视频、应用程序及其他文件发送给用户，来提供高性能、可扩展性及低成本的网络内容传递给用户。

* CDN 存储源服务器提供的静态内容，下次在访问的时候速度会有所提升。

* CDN 服务器可以隐藏原始的 IP 服务器，从而起到保护 源服务器的目的

* 边缘服务器如果没有内容就会回到源站点。

* CDN 的基本工作原理：

* ![img](https://pic2.zhimg.com/80/v2-5ba76e77f05b030b5879177bd336928f_1440w.jpg?source=1940ef5c)

	> * 基本的首先 输入一个 IP 地址，之后需要进行域名解析操作，
	> * 阿里云的 DNS 调度系统会请求分配最佳结点的 IP 地址。
	> * LDNS 获取 DNS 返回的解析 IP 地址
	> * 用户向获取的 IP 地址发起对该资源的访问请求
	> * 如果 IP 地址对应的结点存在这个资源，直接返回，如果不存在那么向源站发起对资源的请求，获取资源之后，结合用户自定义配置策略，将资源存在该节点，并返回用户的请求。
	> * 通过域名访问资源，首先是通过 DNS 查找离用户最近的 CDN 结点（边缘服务器的）IP

	

	 

#### cookie、session、token、jwt区别

* 认证： 验证当前的用户的身份，证明“你是你自己”

* 授权：用户授权第三方应用访问该用户的某些资源的权限就是授权

* 实现授权的三种方式：cookie,session, token

* 如何实现认证和授权的：使用的是一种证书来标记访问者的身份，例如当我们在登录模式下，服务器会给该用户使用的浏览器发送一个 token,表明你的身份，在下次浏览器再发送请求带上这个 token，服务器收到识别出用户的身份。

* cookie ： 存在的原因： HTTP 是无状态的，通过将请求和响应的报文中写入 Cookie 信息来控制客户端的状态，就是当客户端在第一次向服务端发送请求之后，服务端会发送一个装有客户信息的小贴纸，后续客户端请求服务器的时候，客户端将报文中带上小贴纸，服务器就认识了。

* ![Cookie 技术](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP/14-cookie%E6%8A%80%E6%9C%AF.png)

* Session: 记录服务器和客户端会话状态的机制。

	> * session 是基于 cookie 实现的， session 是存储在服务端， sessionId 会被存储到客户端的 cookie 中。
	> * 用户在第二次访问服务器的时候，会自动判断该域名下是否存在 Cookie 信息，如果存在将自动将 Cookie 信息发送给服务端，服务端会从 Cookie 中获取 SessionID，在根据 SessionId 查找 Session 信息，如果**没有找到说明用户没有登录或者登录失败，如果找到了说明用户登陆成功，可以执行接下来的工作**
	> * SessionID 是连接 **Session 和 Cookie 之间的桥梁**，大部分都是使用这个原理来验证用户的登录状态。
	> * 通过 session 机制来验证用户端的身份。
	> * 

* Cookie 和 Session 的区别：

	> * session 存在服务器端，cookie存储在 客户端，session更加的安全
	> * 存储类型的部分同：cookie 存储的是只支持字符串数据（其他的数据需要转成字符串），session 可以支持任意的数据格式
	> * 有效期不同：cookie 一直存在，所以一直保持默认的登录状态，session 一般失效的时间比较短，客户端关闭session 关闭
	> * 存储大小不同：session:可存储的数据高于 cookie ，cookie 保存的数据不能超过 4KB 

* Token 

	> * token只是一种加密的文本字符串，用来快速验证用户状态
	> * cookie和session都是存储（非永久），客户端和服务端
	> * 

* jwt:

	> * JWT 组成：**头部. 载荷. 签名**
	> 	头部和载荷用 **base64 编码**
	> * 出现的原因：session 存在服务端中，但是现在服务端多了，我们需要**将 session 保存到每一个服务器**，但是服务器共享也不是方法，可以将 sessionID 放到数据库中，但是数据库可能崩溃，这个时候使用 JWT 技术
	> * **多服务器不共享 session，因此多服务器无法使用session机制。**
	> * 服务端不存储 session了，只需要对客户端发来的 token 进行解密操作。很好解决了负载均衡的多服务器问题。
	> * token 存储在用户端。不保存在服务器端，服务器只保留秘钥，
	> * 解决防篡改的情况。
	> * 应用：身份认证：当用户登录系统之后，后台会返回一个 token 给用户，用户只用本地保存这个 token 即可（保存在 cookie 或者 local storage)，用户在请求资源的时候，每次都要带上这个 token，后台会对这个 token 进行验证.
	> * 信息交换：因为信息是签名的，所以可以确保发送的信息的真实性。（使用比较少）

* 鉴权 : 身份验证，用户是否有访问系统的权限。

	> * Session - cookie
	>
	> 	> * 利用服务端的 session 和客户端的 cookie 来实现前后端的通信
	> 	> * 
	>
	> * Jwt : token

	

#### 多播通信

* 为什么 IP 协议具有广播功能，但实际不用 IP 协议发送广播？

* 因为它很难与进程关联起来：与进程关联起来的是传输层。 端口号决定了这个网络包该被哪个进程处理。

* 根据网络分层模型，上层协议可以使用下层协议的功能，所以传输层协议拥有 IP 协议的广播能力。

* 传输层的 TCP 协议为了保证可靠性，建立了逻辑上的连接概念，由于一个连接上只能有两方（所以貌似还是一对一的概念），所以 TCP 无法进行一对多通讯。而传输层的 UDP 协议无需建立连接，所以我们常用 UDP 协议发送广播。

* 当交换机收到目标 MAC 地址是 ff:ff:ff:ff:ff:ff 的报文时，便知道这是一个广播报文，

* 我们写代码时无法控制底层的 MAC 地址，只能填写目标 IP 地址。什么样的目标 IP 地址，会生成广播 MAC 地址呢？

* > * 如果只是对所在子网进行广播，那么使用受限广播地址 255.255.255.255 就可以了；如果局域网划分了多个子网，主机需要向其他子网广播，则需要正确地设置直接广播地址.
  > * 就是在对应的子网号和主机号中的主机号全部置成 1，就代表向指定网络中的所有主机发送广播报文.

* 我们设置好 IP 地址后，再把 socket 句柄设置 SO_BROADCAST 属性，就可以发送广播了。





![img](https://static001.geekbang.org/resource/image/1c/b3/1c8b6032474debdd2a4d4569a1752ab3.jpg?wh=1070*976)

* 组播是一种“定向广播”，它设定了一个虚拟组，用组播 IP 来标识。这个虚拟组中可以包含多个主机的 IP，当向对应的组播 IP 发送消息时，仅在这个组内的主机才能收到消息。(D 类地址) 虚拟组要通过 **IGMP 协议管理**
* 广播和组播的应用： 服务发现或者 PUB/SUB
* 转换 IP 地址与 MAC 地址的 ARP 协议就是用广播实现的。
* 广播不是双工，因为广播是由网络设备实现的，所以服务器无法感知到每个客户端的响应，因此客户端对服务器的响应，与本次广播消息链路无关，它必须是另一个通道



#### 事件驱动和并发连接

* 事件是什么？？？

* > * 简单来说，从网络中接收到一个报文，就可能产生一个事件。如上一讲介绍过的 UDP 请求就是最简单的例子，一个 UDP 请求通常仅由一个网络报文组成，所以，当收到一个 UDP 报文，就意味着收到一个请求，它会生成一个事件，进而触发回调函数执行。
  > * HTTP 请求的大小并不受限制,所以并不是每个到达的 TCP 报文都能生成事件的（例子： 如果报文1未收到，不连续的报文2的到达不会产生epollin事件。之后当报文1到达后，这两个报文只会触发1个epoll in事件）。需要在接收端的缓冲区中重组、排序
  > * 一个请求若是10KB，那么MSS在1KB左右的话，意味着一个请求会有10个左右的报文，若是报文不是连续到达的话，是可以产生10个事件的

* 只有两种类型：读事件与写事件，其中，读事件表示有到达的消息需要处理，而写事件表示可以发送消息（TCP 连接的写缓冲区中有可用空间）。

* 三次握手建立连接说起，这一过程会产生一读、一写两个事件

* 

![img](https://static001.geekbang.org/resource/image/73/98/73b9d890c7087531b51180ada6e65f98.png?wh=909*628)

* Q: 读事件表示有到达的消息需要处理，而写事件表示可以发送消息。在三次握手里的SYN+ACK，客户端产生写事件，为什么不是读事件呢？先读到SYN+ACK然后触发写事件恢复ACK到服务端。
* A：这个读、写面对的对象，是应用代码。SYN+ACK对于系统层来说，是到达了1个报文，需要内核发送ACK去处理，但对于应用层来说，是需要发送消息了，因此是写事件

* 当它们到达客户端时，双向连接中由客户端到服务器的通道就建立好了，此时客户端就已经可以发送请求了，因此客户端会产生写事件。接着，客户端发送 ACK 报文，到达服务器后，服务器上会产生读事件，因为进程原本在监听 80 等端口，此时有新连接建立成功，应当调用 accept 函数读取这个连接，所以这是一个读事件。

* 无论是服务端还是客户端发送**ACK报文的动作都是内核自动发送**，不会产生事件给用户态的程序

* 在建立好的 TCP 连接上收发消息时，读事件对应着接收到对方的消息。

* 当调用 write 函数发送时，会先把内存中的数据拷贝到写缓冲区中后，再发送到网卡上。为什么需要将数据拷贝到写缓冲区，直接发给网卡不可以吗？

* > * 为何要多此一举呢？这是因为在对方没有明确表示收到前，TCP 会通过定时器重发写缓冲区中的数据，保证消息能够到达对方。
  > * Write函数调用过程: 内存数据—>写缓冲区—>网卡
  > * ![img](https://static001.geekbang.org/resource/image/c5/7a/c524965bee6407bd716c7dc33bdd437a.png?wh=903*477)

* 关闭连接：

* > * 主动关闭的一方发送 FIN 报文，到达被动方后，内核自动回复 ACK 报文，这表示从主动方到被动方的通道已经关闭。但被动方到主动方的通道也需要关闭，所以此时被动方会产生读事件，提醒被动方调用 close 函数关闭连接。
  > * <img src="https://static001.geekbang.org/resource/image/b7/96/b73164fd504cc2574066f526ebee7596.png?wh=893*627" alt="img" style="zoom:100%;" />
  > * 下面的fin报文为什么会产生读事件呢？ 
  > * 因为此时客户端到服务端的通道是已建立成的，所以收到报文，会产生读事件，需要反应给进程，让进程调用close，关闭服务端到客户端方向的通道。

* 网络不只速度慢，而且波动很大，既受制于连接对端的性能，也受制于网络传输路径。把操作网络的同步 API 改为事件驱动的异步 API 收益最大。

* 而且目前磁盘异步 IO 技术还不成熟，它绕过了 PageCache 性能损失很大(无内存的缓存命中，时间局部性原理）。所以当下的事件驱动，主要就是指网络事件。

* **网络事件是由内核产生的**，进程该怎样获取到它们呢？

* 如 epoll 这样的多路复用技术可以帮我们做到.什么是多路复用呢？

* **多个请求复用了一个进程**，也叫做多路复用

* epoll:就是内核提供给用户态的多路复用接口，进程可以通过它从内核中获取事件。

* epoll 是如何获取网络事件的呢？

* 最简单的方法，就是在获取事件时，把所有并发连接传给内核，再由内核返回产生了事件的连接，再处理这些连接对应的请求即可。

* 网络报文到达后，内核就产生了读、写事件，而 **epoll 函数使得进程可以高效地收集到这些事件**。接下来，要确保在进程中处理每个事件的时间足够短，才能及时地处理所有请求，这个过程中既要**避免阻塞 socket** 的使用，也要把**耗时过长的操作拆成多份执行**。最终，通过**快速、及时、均等地执行所有事件**，异步 Server 实现了高并发。



#### TCP 优化

##### 三次握手优化

* 长连接， 减少time_wait时长，复用time_wait连接

###### 客户端优化

* 简单操作与实现,主动发起连接的客户端优化相对简单一些，而服务器需要在监听端口上被动等待连接，并保存许多握手的中间状态，优化方法更为复杂一些
* 三次握手建立连接的首要目的是同步序列号。只有同步了序列号才有可靠的传输，TCP 协议的许多特性都是依赖序列号实现的，比如流量控制、消息丢失后的重发等等,(seq)
* 客户端在等待服务器回复的 ACK 报文。正常情况下，服务器会在几毫秒内返回 ACK，但如果客户端迟迟没有收到 ACK 会怎么样呢？客户端会重发 SYN，重试的次数由 tcp_syn_retries 参数控制，默认是 6 次
* 如果这是一台有明确任务的服务器，你可以根据网络的稳定性和目标服务器的繁忙程度修改重试次数，调整客户端的三次握手时间上限。比如内网中通讯时，就可以适当调低重试次数，尽快把错误暴露给应用程序。



###### 服务端优化

* 当服务器收到 SYN 报文后，服务器会立刻回复 SYN+ACK 报文，既确认了客户端的序列号，也把自己的序列号发给了对方。
* 服务器端出现了新连接，状态是 SYN_RCV（RCV 是 received 的缩写）。这个状态下，服务器必须建立一个 SYN 半连接队列来维护未完成的握手信息，当这个队列溢出后，服务器将无法再建立新连接。
* ![img](https://static001.geekbang.org/resource/image/c3/82/c361e672526ee5bb87d5f6b7ad169982.png?wh=690*304)
* Q: 如果 SYN 半连接队列已满，只能丢弃连接吗？
* A： 并不是这样，开启 syncookies 功能就可以在不使用 SYN 队列的情况下成功建立连接。
* syncookies 是这么做的：服务器根据当前状态计算出一个值，放在己方发出的 SYN+ACK 报文中发出（一起发送给客户端），当客户端返回 ACK 报文时，取出该值验证，如果合法，就认为连接建立成功
* syncookies这种方式建立的连接，许多 TCP 特性都无法使用。所以，应当把 tcp_syncookies 设置为 1，仅在队列满时再启用。
* **服务器**收到客户端返回的 ACK 后，会把连接移入 accept 队列，等待进程调用 accept 函数取出连接。
* accept 队列也可能溢出：**调整 backlog 的大小，设置 accept 队列的大小**，持续不断地有连接因为 accept 队列溢出被丢弃，就应该调大 backlog 以及 somaxconn 参数。
* tcp_abort_on_overflow 设为 0 可以提高连接建立的成功率，只有你非常肯定 accept 队列会长期溢出时，才能设置为 1 以尽快通知客户端
* ![img](https://static001.geekbang.org/resource/image/0d/c0/0d963557347c149a6270d8102d83e0c0.png?wh=690*319)
* Q:什么是 SYN泛洪攻击？
* A:攻击者恶意构造大量的 SYN 报文发送给服务器，**造成 SYN 半连接队列溢出**，导致正常客户端的连接无法建立

###### 半连接队列 & 全连接队列 （accept）

* **TCP 三次握手建立连接不需要 accept 过程**，因为 accept 是在三次握手之后做的事情
* 内核会为每一个处于`LISTEN`状态的`socket` 分配两个队列,分别叫**半连接队列和全连接队列**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1YAnibQeDqpw5Q4DgXX0d1D2DnLD5YYI9DvxlZEj0wy6wv0MDGOZIsEUQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* **半连接队列（SYN队列）**，服务端收到**第一次握手**后(**内核会把该连接存储到半连接队列**)，会将`sock`加入到这个队列中，队列内的`sock`都处于`SYN_RECV` 状态。

* **全连接队列（ACCEPT队列）**，在服务端收到**第三次握手**后，会将半连接队列的`sock`取出，放到全连接队列中。队列里的`sock`都处于 `ESTABLISHED`状态。这里面的连接，就**等着服务端执行accept()后被取出了。**

* **建立连接的过程中根本不需要`accept()` 参与**， **执行accept()只是为了从全连接队列里取出一条连接。**

* 虽然都叫**队列**，但其实**全连接队列（icsk_accept_queue）是个链表**，而**半连接队列（syn_table）是个哈希表**。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1YzgqRF1t0O2iae5bmy8xHSAIGz1KXuwickPyFHs7lFvyzLSBpYd6QdKag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* Q : 为什么全连接队列设置成链表，半连接队列设置成 hash 表

* > * **全连接队列里面存放的是已近建立好的连接，这些连接正在等待取走，而服务端取走连接的过程中，并不关心具体是哪个连接**，只要是个连接就行，所以直接从队列头取就行了。这个过程算法复杂度为`O(1)`。
  > * 而**半连接队列**却不太一样，因为队列里的都是不完整的连接，嗷嗷等待着第三次握手的到来。那么现在有一个第三次握手来了，则需要从**队列里把相应IP端口的连接取出**，
  > * **如果半连接队列还是个链表，那我们就需要依次遍历，才能拿到我们想要的那个连接，算法复杂度就是O(n)。**

* Q: 全连接队列满了如何处理？

* > * 如果队列满了，服务端还收到客户端的第三次握手ACK，**默认当然会丢弃这个ACK**。
  > * 但除了丢弃之外，还有一些附带行为，这会受 `tcp_abort_on_overflow` 参数的影响
  > * `tcp_abort_on_overflow`设置为 0，全连接队列满了之后，会丢弃这个第三次握手ACK包，并且开启定时器，重传第二次握手的SYN+ACK，如果重传超过一定限制次数，还会把对应的**半连接队列里的连接**给删掉。
  > * ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1YPJU6QMwN1NevnsKlMQYCrPzEM9Z1VDnxCiaiaB3zPibiaKVVXk22iaG0IqQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
  > * `tcp_abort_on_overflow`设置为 1，全连接队列满了之后，就直接发**RST给客户端，效果上看就是连接断了**。
  > * 这个现象是不是很熟悉，服务端**端口未监听**时，客户端尝试去连接，服务端也会回一个RST。
  > * tcp_abort_on_overflow = 1 时：
  > * ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1Ya0KGVP0ADmx4ZMjOJDsYSVkiboZy8UYgqn6j6bKcYBjMp8ZxwdcexGw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* Q： 半连接队列满了如何处理？

* > * **一般是丢弃**，但这个行为可以通过 `tcp_syncookies` 参数去控制。
  > * 一般情况下，半连接的"生存"时间其实很短，只有在**第一次和第三次握手间**，如果半连接都满了，说明服务端疯狂收到第一次握手请求 (SYN 攻击)
  > * ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1YhOouYDD7nIrXr9ugJ2rQGdBpCaBTfHKwO5JWcQUts18nuh8fW6bm6A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
  > * **解决方法**：找到一种途径绕过半连接队列.
  > * tcp_syncookies == 1 客户端发来**第一次握手**SYN时，服务端**不会将其放入半连接队列中**，而是直接生成一个`cookies`，这个`cookies`会跟着**第二次握手**，发回客户端。
  > * 客户端在发**第三次握手**的时候带上这个`cookies`，服务端**验证到它就是当初发出去的那个**，就会建立连接并放入到全连接队列中。可以看出整个过程不再需要半连接队列的参与。
  > * ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1Y48l9ngvPzSBLdr9hzDSV2wLrVFbITGE5gPiaLcK4LBZNXflA4GhylWA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
  > * **这个 cookie 保存在 TCP 报文头的 seq 里面**, 不再是一个队列。

* Q： syncookie 的短板？

* > * 因为**服务端并不会保存连接信息**，所以如果传输过程中数据包丢了，也**不会重发第二次握手的信息。**
  > * 编码解码`cookies`，都是比较**耗CPU**的，利用这一点，如果此时攻击者构造大量的**第三次握手包（ACK包）**，同时带上各种瞎编的`cookies`信息，服务端收到`ACK包`后**以为是正经cookies**，憨憨地跑去解码（**耗CPU**），最后**发现不是正经数据包后才丢弃**。
  > * 这种通过构造大量`ACK包`去消耗服务端资源的攻击，叫**ACK攻击**，受到攻击的服务器可能会因为**CPU资源耗尽**导致没能响应正经请求。

* 我们知道执行`listen`方法时，会**创建半连接队列和全连接队列**。**内核**自动创建的。
* **每一个**`socket`执行`listen`时，内核都会自动创建一个半连接队列和全连接队列。(服务器端)
* 三次握手的过程中会在这两个队列中暂存连接信息。
* 所以形成连接，前提是你得**有个地方存放着**，方便握手的时候能根据**IP端口**等信息找到socket信息。
* `accept方法`只是为了从全连接队列中拿出一条连接，本身跟三次握手几乎**毫无关系**。

###### listen







###### TFO 连接方式优化

* 一个客户端可能需要多次和服务端建立三次握手连接，可以第一次正常三次握手连接，之后的连接客户端第一次均可以发送数据了，不用在按部就班的三次握手了。
* 第一阶段为首次建立连接，这时走正常的三次握手，但在客户端的 SYN 报文会明确地告诉服务器它想使用 TFO 功能，这样服务器会把客户端 IP 地址用只有自己知道的密钥加密（比如 AES 加密算法），作为 Cookie 携带在返回的 SYN+ACK 报文中，客户端收到后会将 Cookie 缓存在本地。
* 之后，如果客户端再次向服务器建立连接，就可以在第一个 SYN 报文中携带请求数据，同时还要附带缓存的 Cookie。
* 服务器收到后，会用自己的密钥验证 Cookie 是否合法，验证通过后连接才算建立成功，再把请求交给进程处理，同时给客户端返回 SYN+ACK。虽然客户端收到后还会返回 ACK，但服务器不等收到 ACK 就可以发送 HTTP 响应了，这就减少了握手带来的 1 个 RTT 的时间消耗。
* ![img](https://static001.geekbang.org/resource/image/7a/c3/7ac29766ba8515eea5bb331fce6dc2c3.png?wh=961*806)



###### TCP 优化 tricks

* 当网络丢包严重的时候，可以使用快速重传机制。重传时间间隔是指数级退避，直到达到 120s 为止，总时间将近 15 分钟，重传次数默认是 15次 ，重传次数默认值由 /proc/sys/net/ipv4/tcp_retries2 决定。
* 客户端在回复ack时带上数据是比较通用的优化方式
* 降低服务端的synretries，有助于尽快释放半连接队列
* Q： 都是减少tcp握手的代价，那tfo和长链接分别是什么使用场景，或者说，什么场景下，tfo会比长链接性价比更高？
* A:如果**应用层允许使用长连接**（比如应用层的超时可能有上限），或者**空闲连接占用的内存资源不在乎（长连接不只在内核，在应用层在每连接也要消耗KB级别以上的内存）**，就使用长连接。 TFO的开启是有前提的，至少两端都要支持。
* 访量很大时，time_wait 会非常多，一般都用**长连接**来规避这个问题。但是长连接也有问题，比如保活，比如当服务端挂了，但是因为网络隔离客户端还没感知道，这时请求就会有大量超时。
* 





##### 四次挥手优化

* 你需要根据主动方与被动方的连接状态变化来调整系统参数，使它在特定网络条件下更及时地释放资源。

###### 主动方优化

* read返回0（EOF），说明对方已经关闭发送通道了，FIN只有内核才能知道，应用层不知道，可以通过read()的返回值来判断
* close & shutdown 的区别：
* 安全关闭连接的方式必须通过四次挥手，它由进程调用 close 或者 shutdown 函数发起，这二者都会向对方发送 FIN 报文（shutdown 参数须传入 SHUT_WR 或者 SHUT_RDWR 才会发送 FIN），**区别在于 close 调用后，哪怕对方在半关闭状态下发送的数据到达主动方，进程也无法接收。** shutdown 函数调用，处于半连接状态下仍然还可以接受数据.
* 而 shutdown 函数调用后，即使连接进入了 FIN_WAIT1 或者 FIN_WAIT2 状态，它也不是孤儿连接，进程仍然可以继续接收数据。
* 对于正常情况来说，调低 tcp_orphan_retries 已经够用，但如果遇到恶意攻击，FIN 报文根本无法发送出去。这是由 TCP 的 2 个特性导致的：
* 1. 首先，TCP 必须保证报文是有序发送的，FIN 报文也不例外，当发送缓冲区还有数据没发送时（恶意发送数据），FIN 报文也不能提前发送。
  2. 其次，TCP 有流控功能，当接收方将接收窗口设为 0 时，发送方就不能再发送数据。所以，当攻击者下载大文件时，就可以通过将接收窗口设为 0，导致 FIN 报文无法发送，进而导致连接一直处于 FIN_WAIT1 状态。
* **Q**: 接收窗口变成0后，也会影响到发送缓冲区吗？
* **A**: 这里说的是服务端主动close的情况吧。攻击者模拟的是被动关闭的一方，通过通告接收窗口为0，进而影响服务端的发送缓冲区一直无法腾出空间，进而也就导致了FIN包无法发出。
* **Q**:虽然 TIME_WAIT 状态的存在是有必要的，但它毕竟在消耗系统资源，比如 TIME_WAIT 状态的端口就无法供新连接使用。怎样解决这个问题呢？（如何做到 time_wait 端口复用）
* **A** : Linux 提供了 tcp_max_tw_buckets 参数，当 TIME_WAIT 的连接数量超过该参数时，新关闭的连接就不再经历 TIME_WAIT 而直接关闭。
* **Q** : 有没有办法让新连接复用 TIME_WAIT 状态的端口呢？
* **A** : 如果服务器会主动向上游服务器发起连接的话，就可以把 tcp_tw_reuse 参数设置为 1，它允许作为客户端的新连接，在安全条件下使用 TIME_WAIT 状态下的端口。
* **A**: 要想使 tcp_tw_reuse 生效，还得把 timestamps 参数设置为 1，它满足安全复用的先决条件（对方也要打开 tcp_timestamps ）
* 

###### 被动方优化

* 大多数应用程序并不使用 shutdown 函数关闭连接，所以，当你用 netstat 命令发现大量 CLOSE_WAIT 状态时，要么是程序出现了 Bug，read 函数返回 0 时忘记调用 close 函数关闭连接，要么就是程序负载太高，close 函数所在的回调函数被延迟执行了。此时，我们应当在应用代码层面解决问题
* 它在回复 ACK 后就进入了 CLOSE_WAIT 状态，等待进程调用 close 函数关闭连接。因此，出现大量 CLOSE_WAIT 状态的连接时，应当从应用程序中找问题。
* 当被动方发送 FIN 报文后，连接就进入 LAST_ACK 状态，在未等来 ACK 时，会在 tcp_orphan_retries 参数的控制下重发 FIN 报文。
* Q： 主动方调用的是close还是shutdown：
*  1、调用的是close，此时连接将成为orphan（孤儿），一般在 tcp_fin_timeout秒左右连接会被系统关闭； 
*  2、调用的是shutdown，此时只有TIME_WAIT才有2MSL限制，FIN_WAIT2可以长时间处于此状态。





#### TCP 缓冲优化

###### 滑动窗口

**主要处理接受主机处理速度能力不足的时候，如何通过控制发送窗口来减缓对方的发送速度**

* 可见，TCP 报文发出去后，并不能立刻从内存中删除，因为重发时还需要用到它。由于 TCP 是由内核实现的，所以报文存放在**内核缓冲区**中，这也是高并发下 buff/cache 内存增加很多的原因。
* 接收方把它的处理能力告诉发送方，使其限制发送速度即可，这就是滑动窗口的由来。
* 只要进程能及时地调用 read 函数读取数据，并且接收缓冲区配置得足够大，那么接收窗口就可以无限地放大，发送方也就无限地提升发送速度。
* 因为网络的传输能力是有限的，当发送方依据发送窗口，发送超过网络处理能力的报文时，路由器会直接丢弃这些报文。
* 这里提到了MSS，这是TCP协议才有的概念，**TCP协议是由操作系统实现的**，即使应用层调用了2次write函数，TCP层有可能将它合并为1个报文；同时，应用层调用了1次write函数，TCP可能将其切割为10个报文
* Q： 一个TCP连接在客户端和服务端是都会有两个内存缓冲区吗，一个接收缓冲区，一个发送缓冲区？还是说各只有一个缓冲区，接收和发送共用？
* A： 因为 TCP 的数据可以双向传输数据的， 所以缓冲区是有2个的，如果只有一个就会混乱，在设计上也是不行的
* Q:带宽时延积是用带宽乘以时延，这里的带宽是指客户端的还是服务端的？因为客户端和服务端的带宽不可能都一样，所以算出来的时延积是不同的。
* A： 这里是服务端的带宽，客户端的带宽你是没办法知道的。我只需要控制服务端的最大处理能力就可以了



###### 拥塞控制

**主要解决网络处理能力不足的时候，如何优化 TCP 性能**

* 网络的传输速度是有限的，它会直接丢弃超过其处理能力的报文。而发送方只有在重传定时器超时后，才能发现超发的报文被网络丢弃了，发送速度提不上去
* 如果网络中的每个连接都按照接收窗口尽可能地发送更多的报文时，就会形成恶性循环，最终超高的网络丢包率会使得每个连接都无法发送数据。
* 发送窗口则应当是拥塞窗口与对方接收窗口的最小值： 发送速度就综合考虑了**接收方**和**网络的处理能力**。
* 2013 年 TCP 的初始拥塞窗口调整到了 10 个 MSS（参见RFC6928），这样 1 个 RTT 内就可以传输 10KB 的请求。（这个初始拥塞窗口可以动态调整的）
* 预测网络的情况：网络刚出现拥塞时并不会丢包，而真的出现丢包时，拥塞已经非常严重了。所以以丢包作为网络拥塞的信号往往为时已晚
* 进行拥塞控制的最佳时间点，是缓冲队列刚出现积压的时刻，此时，**网络时延会增高，但带宽维持不变**，这两个数值的变化可以给出明确的拥塞信号。



#### TCP 百万连接

* 知道了理论层面的东西，接下来，我们还需要做两件事情。

  - 突破单进程最大文件句柄数限制
  - 突破单机最大文件句柄数限制

* 由于**TCP对应于Linux系统来说就是一个文件描述符**。可以使用ulimit -n来查看默认的最大句柄数。

* **Q: 如何设计更快的宕机判断算法？**

* 链表 + hash 表方式判断过时的（类似 LRU），只要看队列首部最老的心跳包，距现在是否超过 5 秒，如果超过 5 秒就认定宕机，同时把它取出队列，否则证明队列中不存在宕机服务，维持队列不变。

* **Q: 如何设计高并发架构？**

* 我们必须选择多线程或者多进程开发模式。**多进程之间干扰更小，但内存不是共享的，数据同步较为困难，因此案例中我们还是选择多线程开发模式。**

* > * 第一是负载均衡，我们应当把心跳包尽量**均匀分配到不同的工作线程**上处理。比如，接收网络报文的线程基于主机名或者 IP 地址，**用哈希算法将心跳包分发给工作线程处理**，这样每个工作线程只处理特定主机的心跳，相互间不会互相干扰，从而可以无锁编程。
  >
  > * 第二是多线程同步。分发线程与工作线程间可以采用生产者 - 消费者模型传递心跳包，然而多线程间传递数据要加锁，为了减少争抢锁对系统资源的消耗，需要做到以下两点：
  >
  > * > * 由于工作线程多过分发线程（接收心跳包消耗的资源更少），所以每个工作线程都配发独立的缓冲队列及操作队列的互斥锁；
  >   > * 为避免线程执行主动切换，必须使用自旋锁
  >   > * ![img](https://static001.geekbang.org/resource/image/27/76/2726b6c9b73325583f8491a822a22476.png?wh=1008*785)
  >
  > * 第三要解决 CPU 亲和性问题。从[第 1 讲] 我们可以看到，CPU 缓存对计算速度的影响很大，如果线程频繁地切换 CPU 会导致缓存命中率下降，降低性能，此时将**线程绑定到特定的 CPU 就是一个解决方案**
  >
  > * 第四：内存使用效率,TCMalloc 相比 Linux 默认的 PtMalloc2 内存池，**在多线程下分配小块内存的速度要快得多**，所以对于心跳服务应当改用 TCMalloc 申请内存.

* **Q:如何选择心跳包网络协议？**

* 如果心跳包长度小于 MTU，那么 UDP 协议是最佳选择。如果心跳包长度大于 MTU，那么最好选择 TCP 协议





#### TCP 最大连接数



一台主机上的 TCP 连接数并不会受端口号 65535 的限制，我们有很多的办法绕开。**最终限制最大 TCP 连接数的资源是机器上的内存**。



##### 客户端

```shell
sysctl -a | grep ip_local_port_range
net.ipv4.ip_local_port_range = 32768	60999
```

* 通过上述内核参数的输出看到内核开放了30000个端口可以供TCP连接使用。

* **当Linux作为客户端建立连接的时候，最大连接数量是受内核参数net.ipv4.ip_local_port_range限制** 而ip_local_port_range是可配置的，最大理论范围是0-65535

* ```shell
  $ php client1.php 10.143.x.x 某台服务器IP 80
  $ php client2.php 10.153.x.x 某台服务器IP 80
  // 两个网卡，两个 IP
  ```

* 这个时候通过ss命令监控本机的ESTABLISH连接，发现已经突破5万，并向10万逼近了。

* ```shell
  $ ss -n | grep ESTAB | wc –l
  90005 
  ```

* **对于有1个Ip的客户端来说，受限于ip_local_port_range参数，也受限于65535。但单Linux可以配置多个ip，有几个ip，最大理论值就翻几倍**

* 在 linux 中一个网卡可以配置多个 IP 地址（主机会根据路由或者其他规则选择一条合适的网卡将其发出。）

* 一台 Linux 客户端最多只能发起 64 k 条 TCP 连接。因为TCP 协议规定的端口数量有 65535 个，但是一般的系统里 1024 以下的端口都是保留的，所以没法用。可用的大约就是 64 k 个。

* 真实情况：

* > * **情况1：** 这个 64 k 的端口号实际上说的是一个 ip 下的可用端口号数量。而一台 Linux 机器上是可以配置多个 IP 的。假如配置了 20 个 IP，那这样一台客户端机就可以发起 120 万多个 TCP 连接了。
  >
  > * **情况2：** 再退一步讲，假定一台 Linux 上确实只有一个 IP，那它就只能发起 64 k 条连接了吗？ 其实也不是的
  >
  > * > * 根据四元组的理论，**只要服务器的 IP 或者端口不一样，即使客户端的 IP 和端口是一样的。这个四元组也是属于一条完全不同的新连接。**
  >   > * 连接1：客户端IP 10000 服务器IP 10000
  >   > * 连接2：客户端IP 10000 服务器IP 20000
  >   > * 以上不是同一个 TCP 连接

* **所以一台客户端机器理论并发最大数是一个比服务器的两百万亿更大的一个天文数字**（因为四元组里每一个元素都能变）



* Q : **客户端上如果两条连接使用一个端口，那数据包来了不会错乱吗**？

* A : 不会乱，保证客户端使用了相同端口的两条连接不串线的原因就在 INET_MATCH 这里。在这个函数里不仅仅是比较了客户端的端口号，包括四元组在内的标识都比较了一遍。 （注意下，每个 socket 上都保存了连接四元组等信息，所以才能比较）

* ```cpp
  #define INET_MATCH(__sk, __net, __cookie, __saddr, __daddr, __ports, __dif) \
   ((inet_sk(__sk)->inet_portpair == (__ports)) &&  \
    (inet_sk(__sk)->inet_daddr == (__saddr)) &&  \
    (inet_sk(__sk)->inet_rcv_saddr == (__daddr)) &&  \
    (!(__sk)->sk_bound_dev_if ||    \
      ((__sk)->sk_bound_dev_if == (__dif)))  &&  \
    net_eq(sock_net(__sk), (__net)))
  
  ```

* **因为服务器端口不同，所以当服务器发送数据过来的时候 INET_MATCH 可以精确定位到连接，而不会串线**。







##### 服务端

* NGINX 为例
* Nginx服务器只监听了一个80端口。那Nginx只能接受一个TCP连接吗？
* A： 不是的
* 一条TCP连接是由一个四元组组成的。不考虑地址重用（unix的SO_REUSEADDR选项）的情况下，对于我们这台Nginx Server来说，它的**IP和端口是固定的**。tcp连接4元组中**只有remote ip（也就是client ip）和remote port（客户端port）是可变的**。它可能建立的**最大的连接数是2的32次方（ip数）×2的16次方（port数）**。这是2.8*10的14次方的一个大数字，两百万亿！！
* **Linux上除了监听80以外，还可以监听其它的端口**，例如**Mysql的3306, Redis的6339**，当然所有65535个端口你都可以用来监听一遍。这样理论上线就到了2的32次方（ip数）×2的16次方（port数）×2的16次方（服务器port数）个。
* 上述只是理想情况下，真实小很多
* 因为Linux每维护一条TCP连接都要花费资源。处理连接请求，保活，数据的收发时需要消耗一些CPU，**维持TCP连接主要消耗内存**。
* 那么**TCP在静止的状态下**，就不怎么消耗CPU了，主要消耗内存。而Linux上内存是有限的。
* 一条TCP连接如果不发送数据的话，**消耗内存是3.3K左右**。
* 但是受到内存的限制以及**每一个TCP 需要一个文件描述符，所以也受文件描述符个数的限制**
* 如果有数据发送，**需要为每条TCP分配发送缓存区**，大小受你的参数net.ipv4.tcp_wmem配置影响，默认情况下最小是4K。
* **假设你只保持连接不发送数据，那么你服务器可以建立的连接最大数量 = 你的内存/3.3K。** 假如是4GB的内存，那么大约可接受的TCP连接数量是100万左右。
* **TCP连接的服务器机：**每一个监听的端口虽然理论值很大，但这个数字没有实际意义。**最大并发数取决你的内存大小**，每一条静止状态的TCP连接大约需要吃3.3K的内存。



######  **Linux 最大文件描述符限制**

* linux 下一切皆文件，包括 socket。**所以每当进程打开一个 socket 时候，内核实际上都会创建包括 file 在内的几个内核对象**。该进程如果打开了两个 socket，那么它的内核对象结构如下图。
* ![img](https://pica.zhimg.com/80/v2-3072dc9abe9347a98bf947c9ed2feb8b_720w.jpg?source=1940ef5c)
* 进程打开文件时消耗内核对象，换一句直白的话就是**打开文件对象吃内存**。所以linux系统出于安全角度的考虑，在多个位置都限制了可打开的文件描述符的数量
* 在 Linux 3.10.0 版本中，创建一个socket 需要消耗 densty、flip、sock_inode_cache、TCP 四个内核对象。这些对象加起来总共需要消耗大约 **3 KB 多一点的内存**。
* 如果连接上有数据收发的话，还需要**消耗发送、接收缓存区**。这两个缓存区占用内存影响因素比较多，既受收发数据的大小，也受 tcp_rmem、tcp_wmem 等内核参数，还取决于服务器进程能否及时接收（及时接收的话缓存区就能回收）。
* 









#### TCP 基础知识

##### 拔掉网线 TCP 还存在吗

* Q : 拔掉网线这个动作会影响传输层?
* A : 不会的，只会影响传输层，所以拔了网线 TCP 连接还在。

* TCP 连接在 Linux 内核中是一个名为 `struct socket` 的结构体，该结构体的内容包含 TCP 连接的状态等信息。当拔掉网线的时候，操作系统并不会变更该结构体的任何内容，所以 TCP 连接的状态也不会发生改变。

![图片](https://img-blog.csdnimg.cn/img_convert/fff358407ee92aeea1e17386191a5d18.png)



###### 拔掉网线还有数据传输

* **如果在服务端重传报文的过程中，客户端刚好把网线插回去了**，由于拔掉网线并不会改变客户端的 TCP 连接状态，并且还是处于 ESTABLISHED 状态，所以这时客户端是可以正常接收服务端发来的数据报文的，然后客户端就会回 ACK 响应报文。

* **客户端和服务端的 TCP 连接依然存在的，就感觉什么事情都没有发生**。

* **如果如果在服务端重传报文的过程中，客户端一直没有将网线插回去**，服务端超时重传报文的次数达到一定阈值后，内核就会判定出该 TCP 有问题，然后通过 Socket 接口告诉应用程序该 TCP 连接出问题了，于是服务端的 TCP 连接就会断开。

* 而等客户端插回网线后，如果客户端向服务端发送了数据，由于**服务端已经没有与客户端相同四元祖的 TCP 连接了**，因此服务端内核就会回复 **RST 报文**，**客户端收到后就会释放该 TCP 连接**。

* Q： 那 TCP 的数据报文具体重传几次呢？

* ```shell
  cat /proc/sys/net/ipv4/tcp_retries2
  15
  ```

* 不过 tcp_retries2 设置了 15 次，并不代表 TCP 超时重传了 15 次才会通知应用程序终止该 TCP 连接，**内核还会基于「最大超时时间」**来判定。

* 在重传报文且一直没有收到对方响应的情况时，**先达到「最大重传次数」或者「最大超时时间」这两个的其中一个条件后**，就会停止重传，然后就会**断开 TCP 连接**



###### 拔掉数据线没有数据传输

* 看是否开启了 TCP keepalive 机制(保证 TCP 连接始终存在) 

* Keepalive ： TCP 保活机制可以在双方没有数据交互的情况，通过探测报文（**该探测报文包含的数据非常少**），来确定对方的 TCP 连接是否存活（如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡）。

* > * 如果双方都**没有开启** TCP keepalive 机制，在客户端拔掉网线后，并且双方都没有进行数据传输，那么客户端和服务端的 TCP 连接将会一直保持存在。
  >
  > * 而如果双方都**开启**了 TCP keepalive 机制，在客户端拔掉网线后，即使双方都没有进行数据传输，在持续一段时间后，TCP 就会发送探测报文：
  >
  > * > * 如果**对端是正常工作**的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。
  >   > * 如果**对端主机崩溃，或对端由于其他原因导致报文不可达**。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。

* Q： TCP keepalive 机制探测的时间也太长了吧？

* A: TCP keepalive 是 **TCP 层（内核态）** 实现的，它是给所有基于 TCP 传输协议的程序一个兜底的方案。

* 实际上，我们**应用层可以自己实现一套探测机制**，可以在较短的时间内，探测到对方是否存活。

* **web 服务软件**一般都会提供 `keepalive_timeout` 参数，用来指定 HTTP 长连接的超时时间。如果设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会**启动一个定时器**，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，**定时器的时间一到，就会触发回调函数来释放该连接。**

* 







##### TCP 和 UDP 可以同一个端口吗

**多个 IP+ PORT 相同的时候 执行 Bind（）函数的时候会失败**，无论是在服务端或者是在客户端都是一个道理（服务端可能处于 **TIME_WAIT 状态**, 可以设置参数避免这个问题 。)

* Q:TCP 和 UDP 可以**同时**绑定相同的端口吗？
* A： 可以的
* 传输层的「端口号」的作用，是为了区分**同一个主机上不同应用程序的数据包**。
* 传输层有两个传输协议分别是 TCP 和 UDP，在内核中是**两个完全独立的软件模块**。
* **TCP/UDP 各自的端口号也相互独立，互不影响。**
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/tcp%E5%92%8Cudp%E6%A8%A1%E5%9D%97.jpeg)



* Q: 多个 TCP 可以绑定到同一个端口吗?
* A : 如果两个 TCP 服务进程绑定的 **IP 地址不同，而端口相同**的话，也是可以绑定成功的.
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/4.png)
* **如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”**。



* Q: 重启 TCP 服务进程时，为什么会有“Address in use”的报错信息？
* A : 当我们**重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作**，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。
* **当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误**。
* 必须等待 TIME_WAIT 状态结束之后，重启 TCP 服务进程才能成功



* Q : 重启 TCP 服务进程时，如何避免“Address in use”的报错信息？

* A : 我们可以在**调用 bind 前**，对 socket 设置 **SO_REUSEADDR** 属性，可以解决这个问题。

* ```cpp
  int on = 1;
  setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));
  ```

* SO_REUSEADDR:  如果当前启动进程绑定的 IP+PORT 与**处于TIME_WAIT 状态的连接占用的 IP+PORT 存在冲突**，但是新启动的进程使用了 SO_REUSEADDR 选项，那么该进程就**可以绑定成功**。

* 0.0.0.0:8888 和192.168.1.100:8888，虽然逻辑意义上前者包含了后者，但是 0.0.0.0 泛指所有本地 IP，而 192.168.1.100 特指某一IP，**两者并不是完全相同**，所以在**对 socket 设置 SO_REUSEADDR 属性后，那么执行 bind() 时候就会绑定成功**.



* Q: 客户端的端口可以重复使用吗？
* 可以的，只要目的主机 IP 或者端口不同就可以，因为**TCP 四元祖是唯一标识一个 TCP 连接的**，**并不会因为客户端的端口号相同，而导致连接冲突的问题。**
* 客户端在执行 connect 函数的时候，会在**内核里随机选择一个端口**，然后向服务端发起 SYN 报文，然后与服务端进行三次握手。
* 客户端的端口选择的发生在 connect 函数，内核在选择端口的时候，会从 `net.ipv4.ip_local_port_range` 这个**内核参数指定的范围来选取一个端口作为客户端端口**。（随机选择一个端口）



* Q： 多个客户端可以 bind 同一个端口吗？
* A: 可以
* 客户端是在调用 connect 函数的时候，由内核随机选取一个端口作为连接的端口。
* 如果我们想自己指定连接的端口，就可以用 bind 函数来实现：客户端先通过 bind 函数绑定一个端口，然后**调用 connect 函数就会跳过端口选择的过程了，转而使用 bind 时确定的端口**。



* Q: 多个客户端可以 bind 同一个端口吗？
* A: 多个客户端绑定的 **IP + PORT 是否都相同，如果都是相同的，那么在执行 bind() 时候就会出错**，错误是“Address already in use”。
* 一般而言，客户端不建议使用 bind 函数，应该交由 connect 函数来选择端口会比较好，因为**客户端的端口通常都没什么意义**。



* Q： 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？

* A： 针对这个问题要看，客户端**是否都是与同一个服务器**（目标地址和目标端口一样）建立连接？

* > * 如果**客户端都是与同一个服务器**（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就**无法与这个服务器再建立连接了**。
  > * **因为只要客户端连接的服务器不同，端口资源可以重复使用的**。
  > * 如果**客户端都是与不同的服务器建立连接**，即使客户端端口资源只有几万个， **客户端发起百万级连接也是没问题的**（当然这个过程还会受限于其他资源，比如文件描述符、内存、CPU 等）。





* Q: 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与**同一个服务器建立连接**的问题？

* A : 参数设置： **打开 `net.ipv4.tcp_tw_reuse` 这个内核参数**

* **因为开启了这个内核参数后，客户端调用 connect 函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态，如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。**

* ```shell
  客户端地址:端口           服务端地址:端口         TCP 连接状态
  192.168.1.100:2222      172.19.11.21:8888     TIME_WAIT
  ```

* 然后客户端又与该服务器（172.19.11.21:8888）发起了连接，**在调用 connect 函数时，内核刚好选择了 2222 端口，接着发现已经被相同四元组的连接占用了：**

* > - 如果**没有开启** net.ipv4.tcp_tw_reuse 内核参数，那么内核就会选择下一个端口，然后继续判断，直到找到一个没有被相同四元组的连接使用的端口， **如果端口资源耗尽还是没找到，那么 connect 函数就会返回错误。**
  > - 如果**开启**了 net.ipv4.tcp_tw_reuse 内核参数，就会判断该四元组的连接状态是否处于 TIME_WAIT 状态，**如果连接处于 TIME_WAIT 状态并且该状态持续的时间超过了 1 秒，那么就会重用该连接**，于是就可以使用 2222 端口了，这时 connect 就会返回成功。

* 开启了 net.ipv4.tcp_tw_reuse 内核参数，是**客户端（连接发起方） 在调用 connect() 函数时才起作用**，所以在**服务端开启这个参数是没有效果的**。

* 客户端执行 connect 流程

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/%E7%AB%AF%E5%8F%A3%E9%80%89%E6%8B%A9.jpg)

* 













##### TCP 面向字节流如何理解

* Q： 如果收到了两个 UDP 报文，操作系统是怎么区分开的？

* A： 操作系统在收到 UDP 报文后，会将其插入到**队列**里，**队列里的每一个元素就是一个 UDP 报文**，这样当用户调用 recvfrom() 系统调用读数据的时候，就会从队列里取出一个数据，然后从内核里拷贝给用户缓冲区。

* ![图片](https://img-blog.csdnimg.cn/img_convert/a9116c5b375d356048df033dcb53582e.png)

* Q： 为什么 TCP 是面向字节流的协议？

* > * 当用户消息通过 TCP 协议传输时，**消息可能会被操作系统分组成多个的 TCP 报文**，也就是一个完整的用户消息被拆分成多个 TCP 报文进行传输。
  >
  > * **接收方的程序如果不知道发送方发送的消息的长度，也就是不知道消息的边界时，是无法读出一个有效的用户消息的**，因为用户消息被拆分成多个 TCP 报文后，并不能像 UDP 那样，一个 UDP 报文就能代表一个完整的用户消息。
  >
  > * > * 发送方准备发送 「Hi.」和「I am Xiaolin」这两个消息。
  >   > * 当我们调用 send 函数完成数据“发送”以后，数据并没有被真正从网络上发送出去，只是从**应用程序拷贝到了操作系统内核协议栈**中。
  >   > * 至于什么时候真正被发送，**取决于发送窗口、拥塞窗口以及当前发送缓冲区的大小等条件**。
  >   > * 不能认为每次 send 调用发送的数据，都会作为一个整体完整地消息被发送出去。
  >   > * 可能以以下三种情况发送：
  >   > * ![图片](https://img-blog.csdnimg.cn/img_convert/02dce678f870c8c70482b6e37dbb5574.png)
  >   > * ![图片](https://img-blog.csdnimg.cn/img_convert/f58b70cde860188b8f95a433e2f5293b.png)
  >   > * ![图片](https://img-blog.csdnimg.cn/img_convert/68080e783d7acc842fa254e4f9ec5630.png)
  >   > * **我们不能认为一个用户消息对应一个 TCP 报文，正因为这样，所以 TCP 是面向字节流的协议**。
  >   > * 



###### TCP 为什么需要分段， IP 已经存在分片了





##### TCP 如何解决粘包问题

* 出现原因： 因为不知道一个用户消息的边界在哪，如果知道了边界在哪，接收方就可以通过边界来划分出有效的用户消息。

* 解决方式：应用层解决

* http 的 **head 和 body 部分一样都要解决粘包的问题的，因为他们都属于应用层数据。**

* > - 固定长度的消息；
  >
  > - > * 即每个用户消息都是固定长度的，比如规定一个消息的长度是 64 个字节，**当接收方接满 64 个字节，就认为这个内容是一个完整且有效的消息**。
  >
  > - 特殊字符作为边界；
  >
  > - > * 两个用户消息之间插入一个特殊的字符串，这样接收方在接收数据时，读到了这个特殊字符，就把认为已经读完一个完整的消息。
  >   > * HTTP 通过设置回车符、换行符作为 HTTP 报文协议的边界。
  >   > * ![图片](https://img-blog.csdnimg.cn/img_convert/a49a6bb8cd38ae1738d9c00aec68b444.png)
  >   > * **如果刚好消息内容里有这个特殊字符，我们要对这个字符转义**，避免被接收方当作消息的边界点而解析到无效的数据。
  >   > * **HTTP 头部解决粘包的方式**
  >
  > - 自定义消息结构。
  >
  > - > * 自定义一个消息结构，由包头和数据组成，其中包头包是固定大小的，而且包头里有一个字段来说明**紧随其后的数据有多大**。
  >   >
  >   > * ```cpp
  >   >   struct { 
  >   >       u_int32_t message_length;  // 紧随其后的数据大小
  >   >       char message_data[]; 
  >   >   } message;
  >   >   ```
  >   >
  >   > * 接收方接收到包头的大小（比如 4 个字节）后，就解析**包头的内容**，于是就可以知道**数据的长度**
  >   >
  >   > * 接下来就继续读取数据，**直到读满数据的长度**，就可以组装成一个完整到用户消息来处理了。





##### TCP keepalive 和 HTTP keepalive 区别

###### HTTP keepalive

**这两个完全是两样不同东西**，实现的层面也不同：

- HTTP 的 Keep-Alive，是由**应用层（用户态）** 实现的，称为 HTTP 长连接；（不需要反复 TCP 连接）
- TCP 的 Keepalive，是由 **TCP 层（内核态）** 实现的，称为 TCP 保活机制；（确定当前 TCP 连接依旧存在）
- 如果每次请求都要经历这样的过程：建立 TCP -> 请求资源 -> 响应资源 -> 释放连接，那么此方式就是 **HTTP 短连接**，如下图：
- ![HTTP 短连接](https://img-blog.csdnimg.cn/img_convert/d6f6757c02e3afbf113d1048c937f8ee.png)
- 一次连接只能请求一次资源。（不好）
- Q: 能不能在第一个 HTTP 请求完后，先不断开 TCP 连接，让后续的 HTTP 请求继续使用此连接？
- A: HTTP 的 Keep-Alive 就是实现了这个功能，可以使用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，避免了连接建立和释放的开销，这个方法称为 **HTTP 长连接**。
- HTTP 长连接的特点是，**只要任意一端没有明确提出断开连接**，则保持 TCP 连接状态。
- **从 HTTP 1.1 开始， 就默认是开启了 Keep-Alive**，如果要关闭 Keep-Alive，需要在 HTTP 请求的包头里添加：Connection:close
- 给 **HTTP 流水线**技术提供了可实现的基础： 所谓的 HTTP 流水线，是**客户端可以先一次性发送多个请求，而在发送过程中不需先等待服务器的回应**，可以减少整体的响应时间。（HTTP1.1实现的技术，但是服务端响应可能出现队头阻塞，因为服务器依然需要按照顺序响应）
- 为了避免一个连接建立长期不发送新的数据请求：web 服务软件一般都会提供 `keepalive_timeout` 参数，用来指定 HTTP 长连接的超时时间。避免TCP 连接一直建立浪费资源。
- 比如设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会**启动一个定时器**，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，**定时器的时间一到，就会触发回调函数来释放该连接。**

###### TCP  Keepalive

* TCP 的 Keepalive 这东西其实就是 **TCP 的保活机制**(判断当前 TCP 连接是否还存在)
* 该功能是由**「内核」**实现的
* 如果对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。
* 如果对端主机崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。
* ![TCP 保活机制](https://img-blog.csdnimg.cn/img_convert/87e138ae9f2438c8f4e2c9c46ec40b95.png)
* 应用程序**若想使用 TCP 保活机制**需要通过 socket 接口设置 `SO_KEEPALIVE` 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。



 











##### TCP连接，断电和进程崩溃有啥区别









#### TCP 丢包

* 数据包发送过程：
* A: 一个数据包，从聊天框里发出，消息会从**聊天软件**所在的**用户空间**拷贝到**内核空间**的**发送缓冲区（send buffer）**，数据包就这样顺着**传输层、网络层，进入到数据链路层，在这里数据包会经过流控（qdisc），再通过RingBuffer发到物理层的网卡**。数据就这样顺着**网卡**发到了**纷繁复杂**的网络世界里。这里头数据会经过n多个**路由器和交换机**之间的跳转，最后到达**目的机器的网卡**处。
* 此时目的机器的网卡会通知**DMA**将数据包信息放到`RingBuffer`中，再触发一个**硬中断**给`CPU`，`CPU`触发**软中断**让`ksoftirqd`去`RingBuffer`收包，于是一个数据包就这样顺着**物理层，数据链路层，网络层，传输层**，最后从内核空间拷贝到用户空间里的**聊天软件**里。
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialY8ABtqnTxKrnxXxj3TEXy2IhQILqlQBtsW1II3XcvG7cLCrkwA8RKCTjyBia4kNpqclGuGVdKY4w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



##### 可能丢包的场景

###### 建立连接是丢包

* 在服务端，第一次握手之后，会先建立个**半连接**，然后再发出第二次握手。这时候需要有个地方可以**暂存**这些半连接。这个地方就叫**半连接队列**。

  如果之后**第三次握手来了**，**半连接就会升级为全连接**，然后暂存到另外一个叫**全连接队列**的地方，坐等程序**执行`accept()`方法将其取走**使用。

* 是队列就有长度，有长度就有可能会满，如果它们**满了**，那新来的包就会被**丢弃**。

* 从现象来看就是连接建立失败。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialY8ABtqnTxKrnxXxj3TEXyuU9ESQR7WxCSqY8wxHGhiabNGy91rcWht7iaC7ZPsYvNQU5heCQQEurA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 



###### 流量控制丢包

* 应用层能发网络数据包的软件有那么多，如果所有数据不加控制一股脑冲入到网卡，网卡会吃不消，那怎么办？让数据按一定的规则排个队依次处理，也就是所谓的**qdisc**(**Q**ueueing **Disc**iplines，排队规则)，这也是我们常说的**流量控制**机制。
* 当发送数据过快，流控队列长度`txqueuelen`又不够大时，就容易出现**丢包**现象。
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialY8ABtqnTxKrnxXxj3TEXyjGxCPcWdicXHmz3GZShCa0SKPPH2fZgTons2EYxujbmbBR7YWpjStKw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 解决方法：**修改下流控队列的长度**

* ```cpp
  # ifconfig eth0 txqueuelen 1500
  ```





###### 网卡丢包

* 网卡和它的驱动导致丢包的场景也比较常见，原因很多，比如**网线质量差，接触不良**。

* 在接收数据时，会将数据暂存到`RingBuffer`接收缓冲区中，然后等着内核触发软中断慢慢收走。如果这个**缓冲区过小**，而这时候发送的数据又过快，就有可能发生溢出，此时也会产生**丢包**。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialY8ABtqnTxKrnxXxj3TEXy2RJykwKiaaQSpYudr1dPvSKoUCib0VAibNUBvI65hicZyaiau0vSvj9OaxQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* ```cpp
  eno1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
          inet 10.19.126.255  netmask 255.255.252.0  broadcast 10.19.127.255
          inet6 fe80::f9b6:5a75:25b3:74ec  prefixlen 64  scopeid 0x20<link>
          ether b0:7b:25:18:6c:8b  txqueuelen 1000  (以太网)
          RX packets 7620222  bytes 3848596856 (3.8 GB)
          RX errors 0  dropped 142098  overruns 0  frame 0
          TX packets 6909188  bytes 1334287860 (1.3 GB)
          TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
          device interrupt 16  memory 0xa2300000-a2320000 
  ```

* 查看上面的`overruns`指标，它记录了由于`RingBuffer`长度不足导致的溢出次数。



###### 网卡性能不足

* 网卡作为硬件，**传输速度是有上限的**。当网络传输速度过大，达到网卡上限时，就会发生丢包。这种情况一般常见于压测场景。

* ```cpp
  Speed: 1000Mb/s // 千兆网卡
  ```

* ```cpp
  # sar -n DEV 1
  Linux 3.10.0-1127.19.1.el7.x86_64      2022年07月27日     _x86_64_    (1 CPU)
  
  08时35分39秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s    rxcmp/s   txcmp/s  rxmcst/s
  08时35分40秒      eth0      6.06      4.04      0.35    121682.33   0.00    0.00     0.00
  ```

* 其中 **txkB/s是指当前每秒发送的字节（byte）总数，rxkB/s是指每秒接收的字节（byte）总数**。

  当两者加起来的值约等于`12~13w字节`的时候，也就对应大概`125MB/s`的传输速度。此时**达到网卡性能极限，就会开始丢包**。

* 



###### 接受缓冲区丢包

* 当接受缓冲区满了，事情就不一样了，它的TCP接收窗口会变为0，也就是所谓的**零窗口**，并且会通过数据包里的`win=0`，告诉发送端，"球球了，顶不住了，别发了"。一般这种情况下，**发送端就该停止发消息了**，但如果这时候确实还有数据发来，就会发生**丢包**。
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialY8ABtqnTxKrnxXxj3TEXy5U4N3yYAWPCzA9quEztb9oCwagD3XxaoTYTlcOUg5ibWInjYgicRbywg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)





###### 两端直接丢包

* 两端之间那么长的一条链路都属于外部网络，这中间有各种路由器和交换机还有光缆啥的，丢包也是很经常发生的。
* 这些丢包行为发生在中间链路的某些个机器上，我们当然是没权限去登录这些机器。但我们可以通过一些命令观察整个链路的连通情况。
* ping 命令查看丢包



##### 丢包如何解决

* TCP 重发（**保证传输层一定不会丢包**，上层应用层 TCP 管不了）
* 建立了TCP连接的两端，发送端在发出数据后会等待接收端回复`ack包`，`ack包`的目的是为了告诉对方自己确实收到了数据，但如果中间链路发生了丢包，那发送端会迟迟收不到**确认ack**，于是就会进行**重传**。
* 假设现在网断了，我们还用聊天软件发消息，聊天软件会使用TCP**不断尝试重传**数据，**如果重传期间网络恢复了**，那数据就能正常发过去。但如果**多次重试直到超时都还是失败**，这时候你将收获一个**红色感叹号**。
* TCP保证的可靠性，是**传输层的可靠性**。也就是说，**TCP只保证数据从A机器的传输层可靠地发到B机器的传输层。**
* 至于数据到了接收端的传输层之后，**能不能保证到应用层，TCP并不管**。
* 假设现在，我们输入一条消息，从聊天框发出，走到**传输层TCP协议的发送缓冲区**，不管中间有没有丢包，最后通过重传都保证发到了对方的**传输层TCP接收缓冲区**，此时接收端回复了一个`ack`，发送端收到这个`ack`后就会将自己**发送缓冲区**里的消息给扔掉。到这里TCP的任务就结束了。
* TCP 任务结束，数据发送还没有结束！
* **聊天软件还需要将数据从TCP的接收缓冲区里读出来，如果在读出来这一刻，手机由于内存不足或其他各种原因，导致软件崩溃闪退了。**
* **消息依旧丢失了，概率低**
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialY8ABtqnTxKrnxXxj3TEXyDZvuGEwcKPg9sgWuGea3DaKbJHML1Sdiaa7YjlqfsehkvDuOL64jkeg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



###### 应用层丢包如何解决

* 中间的服务器解决！！！

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialY8ABtqnTxKrnxXxj3TEXyzOAficqjKzMzn4g0qtf8nxlgaWAygCo5ic1UyTuWGsHUQmtCETcJib3kQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 也就是说服务器**可能**记录了我们最近发过什么数据，假设**每条消息都有个id**，服务器和聊天软件每次都拿**最新消息的id**进行对比，就能知道两端消息是否一致，就像**对账**一样。

* > * 对于**发送方**，只要定时跟服务端的内容对账一下，就知道哪条消息没发送成功，直接重发就好了。
  > * 如果**接收方**的聊天软件崩溃了，重启后跟服务器稍微通信一下就知道少了哪条数据，同步上来就是了，所以也不存在上面提到的丢包情况。

* 可以看出，**TCP只保证传输层的消息可靠性，并不保证应用层的消息可靠性。如果我们还想保证应用层的消息可靠性，就需要应用层自己去实现逻辑做保证。**

###### 为什么引入第三方服务器

* 第一，如果是两端通信，你聊天软件里有`1000个`好友，你就得建立`1000个`连接。但如果引入服务端，你只需要跟服务器建立`1个`连接就够了，**聊天软件消耗的资源越少，手机就越省电**。
* 第二，就是**安全问题**，如果还是两端通信，随便一个人找你对账一下，你就把聊天记录给同步过去了，这并不合适吧。如果对方别有用心，信息就泄露了。引入第三方服务端就可以很方便的做各种**鉴权**校验。
* 第三，是**软件版本问题**。软件装到用户手机之后，软件更不更新就是由用户说了算了。如果还是两端通信，且两端的**软件版本跨度太大**，很容易产生各种兼容性问题，但引入第三端服务器，就可以强制部分过低版本升级，否则不能使用软件。但对于大部分兼容性问题，**给服务端加兼容逻辑就好了**，不需要强制用户更新软件。



##### TCP 重传机制

* RTO：英文全称是Retransmission TimeOut，即重传超时时间；
* RTO是根据给定连接的往返时间（RTT，全称：Round Trip Time）的测量值而计算出来的。
* 具体的TCP协议实现中，**并不是每一个包都有一个定时器**；如果每一个包都有一个定时器，那系统中定时器数据就太多了，会消耗很多资源，性能会比较差；
* TCP的实现中，都是**一个连接对应一个超时定时器**；
* 那一个连接对应一个定时器，那一个数据包的超时时间严格等于RTO时间吗？答案是：一个连接只有一个超时定时器，那么对于每个数据包，没办法做到超时时间严格等于RTO；但是可以保证超时时间不大于2*RTO。**其实这就是性能和准确性的权衡考虑**。



###### 快速重传：

* 因为RTO超时重传的代价是比较大，会**导致拥塞控制机制进行慢启动过程**。对于因为网络毛刺或者随机因素导致的偶尔单个丢包，如果也进行RTO超时重传，会影响网络传输的性能。
* ![img](https://ask.qcloudimg.com/draft/2992766/p1p4wu83v1.jpg?imageView2/2/w/1620)
* 发送方连续收到3次相同的ack，这个时候即使**超时定时器还没有超时**，也开始启动重传。





###### 选择性重传（SACK）

* TCP通信时，如果发送序列中间某个数据包丢失，TCP会通过**重传最后确认的包开始的后续包**，这样原**先已经正确传输的包也可能重复发送**，急剧降低了TCP性能。
* SACK(Selective Acknowledgment, 选择性确认)技术，使TCP只重新发送丢失的包，不用发送后续所有的包，而且提供相应机制使接收方能告诉发送方哪些数据丢失，哪些数据重发了，哪些数据已经提前收到等。
* ![img](https://ask.qcloudimg.com/draft/2992766/z1drvbmf06.jpg?imageView2/2/w/1620)
* SACK会消费发送方的资源，试想，如果一个攻击者给数据发送方发一堆SACK的选项，这会导致**发送方开始要重传甚至遍历已经发出的数据**，这会**消耗很多发送端的资源**。



#### 断网了， ping 127.0.0.1 可以成功吗？

* **ping 是应用层命令**，可以理解为它跟游戏或者聊天软件属于同一层。只不过**聊天软件可以收发消息，还能点个赞什么的，有很多复杂的功能**。而 ping 作为一个小软件，它的**功能比较简单**，就是**尝试**发送一个小小的消息到目标机器上，判断目的机器是否**可达**，其实也就是判断目标机器网络是否能连通。
* ping应用的底层，用的是网络层的**ICMP协议**。
* ICMP 协议： 主要用于在**IP主机和路由器之间传递控制消息(ping)**，用于报告主机是否可达、路由是否可用等。这些**控制消息虽然并不传输用户数据**，但是对于**收集各种网络信息、诊断和排除各种网络故障以及用户数据的传递**具有至关重要的作用。
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3abT1NHNLtfz0KV8jBOVDFtaEb7ZqpQ48XEzZ1eAnbXrDXmXPuMelJ6A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
* 虽然ICMP协议和IP协议**都属于网络层协议**，但其实**ICMP也是利用了IP协议进行消息的传输**。
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3anicOgzO4NDtrmN7NTibbR3Ze5DUzjDLDEDStBX0WL4A9iaL0h5gq28Xqw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
* 完全可以简单的理解为 **ping 某个IP 就是往某个IP地址发个消息。**



##### TCP & Ping

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3aTZVia58LjFhfprOVOg8y4WclVdG4Y7tD8IGhPQia65nPey5TDWARgGNA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* ping和其他应用层软件都属于**应用层**。

* TCP 发送消息的方式：

* > * linux里万物皆文件，那你要**发消息的目的地，也是个文件**，这里就引出了socket 的概念。
  > * 创建好了 `socket` 之后，就可以愉快的把要传输的数据写到这个文件里。
  > * 调用 socket 的`sendto`接口的过程中进程会从**用户态进入到内核态**
  > * 最后会调用到 `sock_sendmsg` 方法。

* Ping 发送消息的方式：

* > * 创建 `socket` 的时候用的是  `socket(AF_INET,SOCK_RAW,IPPROTO_ICMP)`
  > * `SOCK_RAW` 是原始套接字 ，**工作在网络层**， 所以构建`ICMP`（网络层协议）的数据，是再合适不过了。
  > * ping 在**进入内核态后最后也是调用的  `sock_sendmsg` 方法**，进入到网络层后加上**ICMP和IP头**后，数据链路层加上**MAC头**，也是顺着网卡发出。

* 可以简单理解为**ping就是自己组了个数据包，让系统按着其他软件发送数据的路径往外发一遍**，能通的话说明其他软件发的数据也能通。

##### 为什么断网还能 ping 通

* 有网的情况下，ping 最后是**通过网卡**将数据发送出去的。

* 那么断网的情况下，**网卡已经不工作了**，ping 回环地址却一切正常，我们可以看下这种情况下的工作原理。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3a3njA3Ysh4YWH7owWreIy81oOqpAr5vOfdp5QkPjPIiarZ5Tkg7MZUYQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 到了**网络层**，系统会根据目的IP，在路由表中获取对应的**路由信息**，而这其中就包含选择**哪个网卡**把消息发出。

* > * 当发现**目标IP是外网IP**时，会从"真网卡"发出。
  > * 当发现**目标IP是回环地址**时，就会选择**本地网卡**。

* 本地网卡，其实就是个**"假网卡"**，它不像"真网卡"那样有个`ring buffer`什么的，"假网卡"会把数据推到一个叫 `input_pkt_queue` 的 **链表** 中。

* 这个链表，其实是所有网卡共享的，上面挂着发给本机的各种消息。消息被发送到这个链表后，会再触发一个**软中断**。

* 专门处理软中断的工具人**"ksoftirqd"** （这是个**内核线程**），它在收到软中断后就会立马去链表里把消息取出，然后顺着数据链路层、网络层等层层往上传递最后给到应用程序。

* ping 回环地址和**通过TCP等各种协议发送数据到回环地址**都是走这条路径。

* **之所以127.0.0.1叫本地回环地址，可以理解为，消息发出到这个地址上的话，就不会出网络，在本机打个转就又回来了。**所以断网，依然能 `ping` 通 `127.0.0.1`。

* 只要走了本地回环接口，那数据都不会发送到网络中，在本机网络协议栈中兜一圈，就发回来了。因此 **ping回环地址和ping本机地址没有区别**。



##### Ping  0.0.0.0 和 localhost 如何？

* `localhost` 就不叫 `IP`，它是一个域名，就跟 `"baidu.com"`,是一个形式的东西，只不过默认会把它解析为 `127.0.0.1` ，当然这可以在 `/etc/hosts` 文件下进行修改。
* 默认情况下，使用 `localhost` 跟使用  `127.0.0.1` 确实是没区别的。
* 执行 ping 0.0.0.0  ，是会失败的，因为它在`IPV4`中表示的是无效的**目标地址**。
* 启动服务器的时候，**一般会 `listen` 一个 IP 和端口，等待客户端的连接**。
* 如果此时 `listen` 的是本机的 `0.0.0.0` , 那么它表示本机上的**所有IPV4地址**。
* 当然， 客户端 `connect` 时，不能使用 `0.0.0.0` 。**必须指明要连接哪个服务器IP。**



##### 总结

* `127.0.0.1` 是**回环地址**。`localhost`是**域名**，但默认等于 `127.0.0.1`。
* `ping` 回环地址和 `ping` 本机地址，是一样的，走的是**lo0 "假网卡"**，都会经过网络层和数据链路层等逻辑，最后在快要出网卡前**狠狠拐了个弯**， 将数据插入到一个**链表**后就**软中断**通知 **ksoftirqd** 来进行**收数据**的逻辑，**压根就不出网络**。所以断网了也能 `ping` 通回环地址。
* 如果服务器 `listen` 的是 `0.0.0.0`，那么此时用`127.0.0.1`和本机地址**都可以**访问到服务。
* 











#### 收到 RST， TCP 一定断开吗？









#### 连接 IP 不存在主机握手过程

* 而此时客户端主动调用 `connect(IP地址)` ，就会向某个IP地址发起**第一次握手**，发送`SYN` 到目的服务器。

###### 局域网 IP

* A类地址：10.0.0.0 - 10.**255.255.255** 
* B类地址：172.16.0.0 - 172.**31.255.255** 
* C类地址：192.168.0.0 -192.168.**255.255** 

##### **IP 不存在**

###### 目的 IP 在局域网内

* 那不存在的IP，分两种，**局域网内和局域网外**的。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnnibRGLpTcC2LZkBCBuXJBeZZ7YF8KaewubIl1Mib72V77tLRVFo3wmKPZwXZ6ibiaasfsjln7dJ3pdKQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 家里有一台**家用路由器**。本质上它的功能已经集成了我们常说的**路由器，交换机和无线接入点 **   (**无线接入点**基本可以认为就是个放出 wifi 信号的组件)  的功能了。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnnibRGLpTcC2LZkBCBuXJBeZtZYu6Hc4tIoEA53C020Jaf07TRExol2icOX8pBqiafc8c3tHZlgy2IFw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 应用层执行`connect`过后，会通过socket层，操作系统接口，进程会从**用户态进入到内核态**，此时进入 **传输层**，因为是**TCP第一次握手**，会加入**TCP头**，且置**SYN**标志。

* 然后进入**网络层**，我想要连的是 `192.168.31.7` ，虽然它是我瞎编的，但**IP头**还是得老老实实把它加进去。

* 此时需要重点介绍的是**邻居子系统**，它在**网络层和数据链路层之间**。可以通过**ARP协议将目的IP转为对应的MAC地址**，然后**数据链路层**就可以用这个MAC地址组装**帧头**。

* **ARP协议的流程**是 :

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnnibRGLpTcC2LZkBCBuXJBeZkH8npDpbbzb3D9sLJ3QqicOPN6RgFFQDs53Lo7bEl8ibjq2n0CdbObYA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* > * 1.先到本地ARP表查一下有没有 `192.168.31.7` 对应的 **mac地址**，有的话就返回，这里显然是不可能会有的。
  > * 2.看下 `192.168.31.7` 跟本机IP  `192.168.31.6`**在不在一个局域网下**。如果**在**的话，就在局域网内**发一个 arp 广播**，内容就是 前面提到的 “谁是 `192.168.31.7`，告诉一下 `192.168.31.6`”。
  > * 3.**如果目的IP跟本机IP不在同一个局域网下**，那么会去获取**默认网关的MAC地址**，这里就是指获取**家用路由器的MAC地址**。然后把消息发给家用路由器，让**路由器发到互联网**，找到下一跳路由器，一跳一跳的发送数据，**直到把消息发到目的IP上，又或者找不到目的地最终被丢弃。**
  > * 4.第2和第3点都是本地没有查到 ARP 缓存记录的情况，这时候会**把SYN报文放进一个队列（叫unresolved_queue）里暂存起来**，然后发起ARP请求；**等ARP层收到ARP回应报文之后**，会再从缓存中**取出 SYN 报文**，组装 MAC 帧头，完成刚刚没完成的发送流程。

* 因为现在这个IP是瞎编的，因此**不可能得到目的地址 MAC ，所以消息也一直没法到数据链路层**。**整个流程卡在了ARP流程中。**

* 而**抓包是在数据链路层之后进行的**，因此 **TCP 第一次握手的包**一直没能抓到，只能抓到为了获得  `192.168.31.7` 的MAC地址的ARP请求。

* TCP  可靠传输会反复的法送第一次握手， 而每一次重发，都会因为同样的原因（没有目的 MAC 地址）而尬在了 ARP 那个流程里。因此，**才看到好几次重复的 ARP 消息**。

* **因为没能获得目的 MAC 地址，这些 TCP 握手请求最终都发不出去**，



###### 目的 IP 在局域网外

* TCP 超时重传







##### **IP 地址存在但端口号不存在**

###### IP 地址是 127.0.0.1

* 已经IP地址是存在的，也就是在互联网中这个机器是存在的。
* 可以正常发消息到目的IP，因为对应的MAC地址和IP都是正确的，所以，数据从**数据链路层到网络层**都很OK。
* 直到传输层，**TCP协议在识别到这个端口号对应的进程根本不存在时**，就会把数据丢弃，**响应一个RST消息**给发送端。
* <img src="https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnnibRGLpTcC2LZkBCBuXJBeZb7X5R0hw3mhQnd06OgIv3QokmYuz1nkoH14pMvwq4rJ1tSIicolLm5w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:80%;" />



###### RST

* TCP正常情况下断开连接是用四次挥手，那是**正常时候**的优雅做法。
* 但**异常情况**下，收发双方都不一定正常，连挥手这件事本身都可能做不到，所以就需要一个机制去**强行关闭连接**。
* **RST** 就是用于这种情况，一般用来**异常地**关闭一个连接。一般会看到一个 `connection reset` 或  `connection refused` 的报错。
* 很多发到 8080端口的消息都**在防火墙这一层就被拒绝掉了**，根本到不了目的主机里，而**RST是在目的主机的TCP/IP协议栈里发出**的，都还没到这一层，就更不可能发RST了。
* 因此发送端发现消息没有回应（因为被防火墙丢了），就会重传。
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnnibRGLpTcC2LZkBCBuXJBeZVcNpca6Wv99bgCt3wnh7VYblLs2OTM7BcTnTaNVmPP6Oiaav6OViaEvg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
* 



###### IP 在局域网内

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnnibRGLpTcC2LZkBCBuXJBeZnuykh0zZHJ8py0B4iaicdRibDff6sib5BQKibxSG4qeI5MNryiaqcicEfNvbw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
* 唯一不同的是，前者是回环地址，**RST数据是从本机的传输层返回的**。而这次的情况，RST数据是从**目的机器**（需要通过外部网络票）的传输层返回的。



##### 总结

连一个 IP 不存在的主机时： 

- 如果IP在局域网内，会发送N次ARP请求获得目的主机的MAC地址，同时**不能发出TCP握手消息**。
- 如果IP在局域网外，会将消息通过路由器发出，但因为最终找不到目的地，**触发TCP重试流程**。

连IP 地址存在但端口号不存在的主机时： 

- 不管目的IP是回环地址还是局域网内外的IP地址，目的主机的传输层都会在收到握手消息后，发现端口不正确，**发出RST消息断开连接。**
- 当然如果目的机器设置了防火墙策略，限制他人将消息发到不对外暴露的端口，那么这种情况，发送端就会不断重试第一次握手。









#### Send 成功，数据发送出去了吗？

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjXicIIHoCtx2KrEJLHASA3CRkD5t7X0mYlOm3XSXpnibbjiaqAD7ZRMmew/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



##### socket 缓冲区

* **socket** 在操作系统层面，可以理解为一个**文件**。

* 编程的时候，如果要跟某个IP建立连接，我们需要调用操作系统提供的 `socket API`。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjvRd2M0FiaaT9nWzLibKUflPASdGv9ZLtxEQlBu8zKrUmPbibnC6CGFcVA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 如果我们想给远端服务发点什么东西，那就只需要对这个文件执行写操作就行了。

* **写到了这个文件之后**，剩下的发送工作自然就是由操作系统**内核**来完成了。

* 既然是写给操作系统，那操作系统就需要**提供一个地方给用户写**。同理，接收消息也是一样。（这个地方就是 socket 缓冲区）

* > * 用户**发送**消息的时候写给 send buffer（发送缓冲区）
  > * 用户**接收**消息的时候写给 recv buffer（接收缓冲区）

* **一个socket ，会带有两个缓冲区**，一个用于发送，一个用于接收。因为这是个先进先出的结构，有时候也叫它们**发送、接收队列**。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjicVh0e15ib4rx1N0V9ic5eWtXKp8GZAefOYONT4KKfUhcAkqJvph7CNfQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* ```shell
  # netstat -nt
  Active Internet connections (w/o servers)
  Proto Recv-Q Send-Q Local Address           Foreign Address         State      
  tcp        0     60 172.22.66.69:22         122.14.220.252:59889    ESTABLISHED
  ```

* 还有**Send-Q 是发送缓冲区**，下面的数字60是指，当前还有60 Byte在发送缓冲区中未发送。而 **Recv-Q 代表接收缓冲区**， 此时是空的，数据都被应用进程接收干净了。



##### 执行 send 发送字节，会立刻发送吗？

* 答案： **不确定**， 执行 send 之后，**数据只是拷贝到了socket 缓冲区**。至 什么时候会发数据，发多少数据，全听操作系统安排。

```cpp
// 执行 send 发送消息
send(sockfd,str,sizeof(str),0)) 
```

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjqxb2RZpP3PEu9PpjaALP2ic37KzVtytibmQ9uRHRxdaseKHHkxxtWDLA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 在用户进程中，程序通过操作 socket 会从用户态进入内核态，而 **send方法会将数据一路传到传输层**。**在识别到是 TCP协议后，会调用 tcp_sendmsg 方法**。

* ```cpp
  // net/ipv4/tcp.c
  // 以下省略了大量逻辑
  int tcp_sendmsg()
  {  
    // 如果还有可以放数据的空间
    if (skb_availroom(skb) > 0) {
      // 尝试拷贝待发送数据到发送缓冲区
      err = skb_add_data_nocache(sk, skb, from, copy);
    }  
    // 下面是尝试发送的逻辑代码,先省略     
  }
  ```

* 在 tcp_sendmsg 中， **核心工作就是将待发送的数据组织按照先后顺序放入到发送缓冲区中**， 然后**根据实际情况（比如拥塞窗口等）判断是否要发数据**。如果不发送数据，那么此时直接返回。



##### 发送缓冲区满了

* socket在创建的时候，是可以设置是**阻塞**的还是**非阻塞**的。

* ```cpp
  int s = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, IPPROTO_TCP);
  ```

* > * 阻塞：
  >
  > * > * 如果此时 socket 是阻塞的，那么程序会在那**干等、死等**，直到释放出新的缓存空间，就继续把数据拷进去，然后**返回**。
  >   > * ![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjeg5UOCtVEU0TC9eNeSybzlfmKzZDmo3sBBOsOt8ibydIEBQMt3W4QCw/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)
  >
  > * 非阻塞：
  >
  > * > * 如果此时 socket 是非阻塞的，程序就会**立刻返回**一个 `EAGAIN` 错误信息，意思是  `Try again` , 现在缓冲区满了，**你也别等了，待会再试一次。**
  >   > * ![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjWL1ptVibTnKOsMc7lCQEgWLUhZX3VBzMYHPPvmcrZyAt1RheRhcZCFQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)



##### 接收缓冲区为空

* 当接收缓冲区**为空**，如果还向socket执行 recv

* 阻塞 & 非阻塞

* 阻塞： 

* > * 如果此时 socket 是阻塞的，那么程序会在那**干等**，直到接收缓冲区有数据，就会把数据从接收缓冲区拷贝到用户缓冲区，然后**返回**。
  > * ![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjSJukYkMGeUviaGDLIaCk6oZ8J3VqPcu61Cq2tq6xiaWxWgtbQwrm7XDg/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

* 非阻塞：

* > * 如果此时 socket 是非阻塞的，程序就会**立刻返回**一个 `EAGAIN` 错误信息。
  > * ![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjNLGjIFwxJuPicnESic7QGicYc7y4Zib4YANHNl7icNzbtyLK9Ja6ElydPwA/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsj9hSlrmT2jGibWZW7blcOgJxZQX7BznAAGxU6AX6nn6vDto9rsDvLpEA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)





##### socket 缓冲区还有数据，执行 close

* 首先我们要知道，**一般正常情况下，发送缓冲区和接收缓冲区 都应该是空的。**
* 如果发送、接收缓冲区长时间非空，说明有数据堆积，这往往是由于一些网络问题或用户应用层问题，导致数据没有正常处理。
* 那么正常情况下，如果 **`socket` 缓冲区**为空，执行 `close`。就会触发四次挥手。
* 这个也是面试老八股文内容了，**这里我们只需要关注第一次挥手，发的是 `FIN` 就够了**。



##### 缓冲区有数据，执行 close

* 接收缓冲区存在数据：

* > * `socket close` 时，主要的逻辑在 `tcp_close()` 里实现。
  >
  > * > - 如果接收缓冲区**还有数据未读**，会先把接收缓冲区的数据清空，**然后给对端发一个RST**。
  >   >
  >   > - 如果接收缓冲区是空的，那么就调用 `tcp_send_fin()` 开始进行四次挥手过程的第一次挥手。
  >   >
  >   > - ```cpp
  >   >   void tcp_close(struct sock *sk, long timeout)
  >   >   {
  >   >     // 如果接收缓冲区有数据，那么清空数据
  >   >       while ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {
  >   >           u32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq -
  >   >                 tcp_hdr(skb)->fin;
  >   >           data_was_unread += len;
  >   >           __kfree_skb(skb);
  >   >       }
  >   >                   
  >   >      if (data_was_unread) {
  >   >       // 如果接收缓冲区的数据被清空了，发 RST
  >   >           tcp_send_active_reset(sk, sk->sk_allocation);
  >   >        } else if (tcp_close_state(sk)) {
  >   >       // 正常四次挥手， 发 FIN
  >   >           tcp_send_fin(sk);
  >   >       }
  >   >       // 等待关闭
  >   >       sk_stream_wait_close(sk, timeout);
  >   >   }
  >   >   ```

* 发送缓存有数据：

* > * ```cpp
  >   void tcp_send_fin(struct sock *sk)
  >   {
  >     // 获得发送缓冲区的最后一块数据
  >       struct sk_buff *skb, *tskb = tcp_write_queue_tail(sk);
  >       struct tcp_sock *tp = tcp_sk(sk);
  >                   
  >     // 如果发送缓冲区还有数据
  >       if (tskb && (tcp_send_head(sk) || sk_under_memory_pressure(sk))) {
  >           TCP_SKB_CB(tskb)->tcp_flags |= TCPHDR_FIN; // 把最后一块数据值为 FIN 
  >           TCP_SKB_CB(tskb)->end_seq++;
  >           tp->write_seq++;
  >       }  else {
  >       // 发送缓冲区没有数据，就造一个FIN包
  >     }
  >     // 发送数据
  >       __tcp_push_pending_frames(sk, tcp_current_mss(sk), TCP_NAGLE_OFF);
  >   }
  >   ```
  >
  > * 还有些数据没发出去，**内核会把发送缓冲区最后一个数据块拿出来**。然后置为 FIN。
  >
  > * `socket` 缓冲区是个**先进先出**的队列，这种情况是指内核会等待TCP层安静把发送缓冲区数据都发完，最后再执行 四次挥手的第一次挥手（FIN包）。
  >
  > * 有一点需要注意的是，只有在**接收缓冲区为空的前提下**，我们才有可能走到 `tcp_send_fin()` 。
  >
  > * ![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjWnK0M8ibXAAxOl5mudeZcVDYZU0icOfn4WcCME5iaHBdXib3oqZH7kyic1g/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)



##### UDP 有缓冲吗？

* **每个UDP socket都有一个接收缓冲区**，**没有发送缓冲区**，从概念上来说就是只要有数据就发，不管对方是否可以正确接收，所以不缓冲，不需要发送缓冲区。（**错误的想法**）

* UDP socket 也是 socket，一个socket 就是会有收和发两个缓冲区。跟用什么协议关系不大。

* **UDP不仅有发送缓冲区，也用发送缓冲区。**

* 一般正常情况下，**会把数据直接拷到发送缓冲区后直接发送**。

* ```cpp
  ssize_t send(int sock, const void *buf, size_t len, int flags); // flag 置为 MSG_MORE
  ```

* `MSG_MORE `: 大概的意思是告诉内核，待会还有其他**更多消息**要一起发，先别着急发出去。此时内核就会把这份数据先用**发送缓冲区**缓存起来，待会应用层说ok了，再一起发。

* 而我们大部分情况下，都不会用  `MSG_MORE`，也就是来一个数据包就直接发一个数据包。从这个行为上来说，**虽然UDP用上了发送缓冲区，但实际上并没有起到"缓冲"的作用。**

* 









#### TCP 的 11 中状态

##### TCP 三次握手 5 中状态

* ![img](https://ask.qcloudimg.com/http-save/yehe-9954695/4ovi96z2lx.jpeg?imageView2/2/w/1620)
* CLOSED：初始状态，表示TCP连接是”关闭着的”或”未打开的”
* LISTEN：表示服务器端的某个SOCKET处于监听状态，可以接受客户端的连接
* SYN_RCVD：表示服务器接收到了来自客户端请求连接的SYN报文。这个状态是在服务端的，但是它是一个中间状态，很短暂，平常我们用netstat或ss的时候，不太容易看到这种状态，但是遇到SYN flood之类的SYN攻击时，会出现大量的这种状态，即收不到三次握手最后一个客户端发来的ACK，所以一直是这个状态，不会转换到ESTABLISHED
* SYN_SENT：这个状态与SYN_RCVD状态相呼应，，它是TCP连接客户端的状态，当客户端SOCKET执行connect()进行连接时，它首先发送SYN报文，然后随机进入到SYN_SENT状态，并等待服务端的SYN和ACK，该状态表示客户端的SYN已发送
* ESTABLISHED：表示TCP连接已经成功建立，开始传输数据





##### TCP 四次挥手的 6 中状态

* ![img](https://ask.qcloudimg.com/http-save/yehe-9954695/3ah2jcpp82.jpeg?imageView2/2/w/1620)
* FIN_WAIT_1：这个状态在实际工作中很少能看到，当客户端想要主动关闭连接时，它会向服务端发送FIN报文，此时TCP状态就进入到FIN_WAIT_1的状态，而当服务端回复ACK，确认关闭后，则客户端进入到FIN_WAIT_2的状态，也就是只有在没有收到服务端ACK的情况下，FIN_WAIT_1状态才能看到，然后长时间收不到ACK，通常会在默认超时时间60s(由内核参数tcp_fin_timeout控制)后，直接进入CLOSED状态
* FIN_WAIT_2：这个状态相比较常见，也是需要注意的一个状态，FIN_WAIT_1在接收到服务端ACK之后就进入到FIN_WAIT_2的状态，然后等待服务端发送FIN，所以在收到对端FIN之前，TCP都会处于FIN_WAIT_2的状态，也就是，在主动断开的一端发现大量的FIN_WAIT_2状态时，需要注意，可能时网络不稳定或程序中忘记调用连接关闭，FIN_WAIT_2也有超时时间，也是由内核参数tcp_fin_timeout控制，当FIN_WAIT_2状态超时后，连接直接销毁
* CLOSE_WAIT：表示正在等待关闭，该状态只在被动端出现，即当主动断开的一端调用close()后发送FIN报文给被动端，被动段必然会回应一个ACK(这是由TCP协议层决定的)，这个时候，TCP连接状态就进入到CLOSE_WAIT
* LAST_ACK：当被动关闭的一方在发送FIN报文后，等待对方的ACK报文的时候，就处于LAST_ACK的状态，当收到对方的ACK之后，就进入到CLOSED状态了
* TIME_WAIT：该状态是最常见的状态，主动方在收到对方FIN后，就由FIN_WAIT_2状态进入到TIME_WAIT状态
* CLOSING：这个状态是一个比较特殊的状态，也比较少见，正常情况下不会出现，**但是当双方同时都作为主动的一方，调用 close() 关闭连接的时候，两边都进入FIN_WAIT_1 的状态**，此时期望收到的是ACK包，进入 FIN_WAIT_2 的状态，但是**却先收到了对方的FIN包**，这个时候，就**会进入到 CLOSING 的状态**，然后**给对方一个ACK**，**接收到 ACK 后直接进入到 CLOSED 状态**。







#### RPC

##### 编码方式

* 系统的核心模块之一就是传输内容的序列化反序列化模块，它能让我们可以像调用本地方法一样，调用远程服务器上的方法。
* 们会将过程调用里的参数对象转化成网络中可传输的二进制流，从客户端发送给服务端，**然后在服务端按照同样的协议规范**，从**二进制流中反序列化并组装出调用方法中的入参对象**，进行本地方法调用。
* ![img](https://static001.geekbang.org/resource/image/01/a2/01188a87d34f54f3e5f8fdefd61cf6a2.jpg?wh=1920x829)
* Q： 为什么要经过序列化和反序列化过？
* A： 本质上是因为**网络传输的是二进制**，而方法调用的参数和返回值是编程语言中定义的对象，要实现远程过程调用，序列化和反序列化过程是不可避免的环节。
* Q: 那 JDK 序列化具体是怎么实现的呢？
* A: 本质就是要把 Java 中的类型以一种特定的协议翻译成二进制流，然后就可以依据协议再次从这个流中恢复出原始的类型。
* Q:为什么 RPC 不采用 JSON 作为网络传输格式?
* A : 1. JSON 缺点也很明显，它本质上是**纯文本的编码方式**，编码空间利用率很低，导致一次 RPC 调用在网络上传输的二进制流长度比 JDK 的实现要高很多.   2. 编解码需要对 **JSON 文本进行嵌套的解析**，整体上性能比较差。
* 我们其实完全没有必要将成员变量名等信息一起放到传输的数据中，取而代之，如果为每个成员变量设置一个编号，在网络中传输数据的时候，**只是传输编号和对应的内容，这样整体的传输数据量不就大大减少了嘛。**(protbuf & thrift 编码方式都是基于此)
* 由于**去掉了冗长的类型名称**，并**采用二进制而非文本的方式进行元素存储**，Thrift 的空间效率和性能都得到了极大的提升.
* 而 Thrift 或者 Protobuf 的协议，采用二进制编码，并**引入了 schema 文件**(Thrift 为了让**服务器和客户端都能反序列化或序列化方法参数，需要在服务端和客户端都保存一份同样的 schema 文件**），去掉了许多冗余的成员变量信息，**直接采用字段编号进行成员标识**，效率很高，得到了广泛的应用.
* 



##### 有 HTTP 为什么需要 RPC







#### 为什么存在 HTTP 还需要 websocket 协议

* 这种由客户端主动请求，服务器响应的方式也满足大部分网页的功能场景。
* 服务器从来就不会**主动**给客户端发一次消息。
* **你全程没点任何一次鼠标**。服务器就自动将怪物的移动数据和攻击数据源源不断发给你了。
* 像这种**看起来服务器主动发消息给客户端的场景**，是怎么做到的？



##### 服务器如何主动给客户端发消息

###### 使用HTTP不断轮询

* 其实问题的痛点在于，**怎么样才能在用户不做任何操作的情况下，网页能收到消息并发生变更。**

* **网页的前端代码里不断定时发HTTP请求到服务器，服务器收到请求后给客户端响应消息。**

* 这其实时一种**伪**服务器推的形式。

* 它其实并不是服务器主动发消息到客户端，而是客户端自己不断偷偷请求服务器，只是用户无感知而已。

* 比如某信公众号平台，登录页面二维码出现之后，**前端**网页根本不知道用户扫没扫，于是不断去向**后端**服务器询问，看有没有人扫过这个码。而且是以大概1到2秒的间隔去不断发出请求，这样可以保证用户在扫码后能在1到2s内得到及时的反馈，不至于**等太久**。

* ![使用HTTP定时轮询](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/36e68b2e16484b20aa66db0a784c53c5~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

* 缺点：

* > * 当你打开F12页面时，你会发现满屏的HTTP请求。虽然很小，但这其实也消耗带宽，同时也会增加下游服务器的负担。
  > * 最坏情况下，用户在扫码后，需要等个1~2s，**正好才触发下一次http请求**，然后才跳转页面，用户会感到**明显的卡顿**。

###### 长轮训

* HTTP请求发出后，一般会给服务器留一定的时间做响应，比如3s，规定时间内没返回，就认为是超时。
* 如果我们的HTTP请求**将超时设置的很大**，比如30s，**在这30s内只要服务器收到了扫码请求，就立马返回给客户端网页。如果超时，那就立马发起下一次请求。**
* 这样就减少了HTTP请求的个数，并且由于大部分情况下，用户都会在某个30s的区间内做扫码操作，所以响应也是及时的。
* ![长轮询](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/376a95f1a8bc4ea7b1d926b7e4601476~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)
* 比如，某度云网盘就是这么干的。所以你会发现一扫码，手机上点个确认，电脑端网页就**秒跳转**，体验很好。
* 像这种发起一个请求，在较长时间内等待服务器响应的机制，就是所谓的**长训轮机制**。我们常用的消息队列RocketMQ中，消费者去取数据时，也用到了这种方式。
* ![RocketMQ的消费者通过长轮询获取数据](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/709bd609a68c41bcbeea1cec39732de5~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)
* 像这种，在用户不感知的情况下，服务器将数据推送给浏览器的技术，就是所谓的**服务器推送**技术
* 上面提到的两种解决方案，本质上，其实还是**客户端主动去取数据**。
* 如果是**网页游戏**呢，**游戏一般会有大量的数据需要从服务器主动推送到客户端**。-> websocket



###### websocket

* 我们知道TCP连接的两端，**同一时间里**，**双方**都可以**主动**向对方发送数据。这就是所谓的**全双工**。
* 现在使用最广泛的`HTTP1.1`，也是基于TCP协议的，**同一时间里**，客户端和服务器**只能有一方主动**发数据，这就是所谓的**半双工**。
* 由于HTTP协议设计之初，考虑的是看看网页文本的场景，能做到**客户端发起请求再由服务器响应**，就够了，根本就没考虑网页游戏这种，客户端和服务器之间都要互相主动发大量数据的场景。
* 为了更好的支持这样的场景，我们需要另外一个**基于TCP的新协议**。
* ![websocket在四层网络协议中的位置](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/52ac01da172d491fa156dbdcea82d94d~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)





###### 如何建立 websocket 连接

* 平时刷网页，一般都是在浏览器上刷的，一会刷刷图文，这时候用的是**HTTP协议**，一会打开网页游戏，这时候就得切换成我们新介绍的**websocket协议**。

* 为了兼容这些使用场景。浏览器在**TCP三次握手**建立连接之后，都**统一使用HTTP协议**先进行一次通信。

* 如果这时候是**想建立websocket连接**，就会在HTTP请求里带上一些**特殊的header头**。

* ```shell
  Connection: Upgrade
  Upgrade: websocket
  Sec-WebSocket-Key: T2a6wZlAwhgQNqruZ2YUyg==\r\n
  ```

* 这些header头的意思是，浏览器想**升级协议（Connection: Upgrade）**，并且**想升级成websocket协议（Upgrade: websocket）**。

* 如果服务器正好支持升级成websocket协议。就会走websocket握手流程，同时根据客户端生成的base64码，用某个**公开的**算法变成另一段字符串，放在HTTP响应的 `Sec-WebSocket-Accept` 头里，同时带上`101状态码`，发回给浏览器。

* ```shell
  HTTP/1.1 101 Switching Protocols\r\n
  Sec-WebSocket-Accept: iBJKv/ALIW2DobfoA4dmr3JHBCY=\r\n
  Upgrade: websocket\r\n
  Connection: Upgrade\r\n
  ```

* http状态码=200（正常响应）的情况，大家见得多了。**101确实不常见，它其实是指协议切换**。

* ![对比客户端和服务端生成的字符串](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b4883177655e4fa4a908fe5cae2b51a1~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

* 之后，浏览器也用同样的**公开算法**将`base64码`转成另一段字符串，如果这段字符串跟服务器传回来的**字符串一致**，那验证通过。

* 就这样经历了一来一回两次HTTP握手，websocket就建立完成了，后续双方就可以使用webscoket的数据格式进行通信了。

* ![建立websocket连接.drawio](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/90cf4e27558d4373b8f9ef678b696eb9~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

* ![两次HTTP请求之后正式使用websocket通信](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e87e1d4665554e1392ed71c5295397b4~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

* **websocket和HTTP一样都是基于TCP的协议。**

* "websocket是基于HTTP的新协议"，**其实这并不对**，因为websocket只有在建立连接时才用到了HTTP，**升级完成之后就跟HTTP没有任何关系了**。



###### websocket 消息格式

* 上面提到在完成协议升级之后，两端就会用webscoket的数据格式进行通信。
* 数据包在websocket中被叫做**帧**。
* ![websocket报文格式](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ad3688a03a114bccbc75cb01ae9f7e6e~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)
* TCP协议本身就是全双工，但直接使用**纯裸TCP**去传输数据，会有**粘包**的"问题"。为了解决这个问题，上层协议一般会用**消息头+消息体**的格式去重新包装要发的数据。
* 而**消息头**里一般含有**消息体的长度**，通过这个长度可以去截取真正的消息体。
* **HTTP协议和大部分RPC协议，以及我们今天介绍的websocket协议，都是这样设计的**。
* ![消息边界长度标志](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5da32fca58e24daf88a0b66e55094a7f~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)



###### 使用场景

* websocket完美继承了TCP协议的**全双工**能力，并且还贴心的提供了解决粘包的方案。
* 它适用于**需要服务器和客户端（浏览器）频繁交互**的大部分场景。比如网页/小程序游戏，网页聊天室，以及一些类似飞书这样的网页协同办公软件。
* 在使用websocket协议的网页游戏里，怪物移动以及攻击玩家的行为是**服务器逻辑**产生的，对玩家产生的伤害等数据，都需要由**服务器主动发送给客户端**，客户端获得数据后展示对应的效果。



###### 总结

* TCP协议本身是**全双工**的，但我们最常用的HTTP1.1，虽然是基于TCP的协议，但它是**半双工**的，对于大部分需要服务器主动推送数据到客户端的场景，都不太友好，因此我们需要使用支持全双工的websocket协议。
* 在HTTP1.1里。只要客户端不问，服务端就不答。基于这样的特点，对于登录页面这样的简单场景，可以使用**定时轮询或者长轮询**的方式实现**服务器推送**(comet)的效果。
* 对于客户端和服务端之间需要频繁交互的复杂场景，比如网页游戏，都可以考虑使用websocket协议。
* websocket和socket几乎没有任何关系，只是叫法相似。
* 正因为各个浏览器都支持HTTP协议，所以websocket会先利用HTTP协议加上一些特殊的header头进行握手升级操作，升级成功后就跟HTTP没有任何关系了，之后就用websocket的数据格式进行收发数据。



### 性能调优

#### Hash ：管理亿级数据对象

* 哈希表基于数组实现，而数组可以根据**下标随机访问任意元素**。数组之所以可以随机访问，是因为它由**连续内存承载，且每个数组元素的大小都相等**。

* 在生产环境用哈希表管理如此多的数据，必然面临以下问题：

* > * 首先，面对上亿条数据，为了保证可靠性，需要做灾备恢复，我们可以结合**快照 +oplog 方式恢复数据**，但内存中的哈希表如何**快速地序列化为快照文件**？
  >
  > * > * 生产级存放大量对象的哈希表是需要容灾的，比如每隔一天把哈希表数据定期备份到另一台服务器上。当服务器宕机而启动备用服务器时，首先可以用备份数据把哈希表恢复到 1 天前的状态，再通过操作日志 oplog 把 1 天内的数据载入哈希表，这样就可以最快速的恢复哈希表。所以，为了能够传输，首先**必须把哈希表序列化**。
  >   > * **这样可以把整块内存复制到磁盘上**，或者发送到网卡上，而不用重新耗费CPU进行一次编码，先将**分散的内存信息转换到一块连续的内存**。
  >   > * 当数组外的还有链表中的元素时，**序列化就必须遍历所有元素**，比如，至少要做1次循环，把每1个遍历到的元素的值，**序列化写入至另一段内存中**。而使用闭散列时，**直接将这个数组占用的内存，直接作为序列化后的数据即可。**
  >
  > * 其次，简单的使用标准库提供的哈希表处理如此规模的数据，会导致内存消耗过大，因为每多使用一个 8 字节的指针（或者叫引用）都会被放大亿万倍，此时该如何实现更节约内存的个性化哈希表？
  >
  > * > * 可以通过降低装载因子来减少哈希冲突，提升速度。
  >
  > * 再次，哈希表频繁发生冲突时，速度会急剧降低，我们该通过哪些手段减少冲突概率？
  >
  > * > * **链接法**: 允许存放元素个数大于数组的大小（也叫装载因子大于 1）, 但链接法序列化数据的代价很大，因为使用了指针后，内存是不连续的.
  >   > * **开放寻址法**: 所有对象都在数组里，就可以把数组用到的这段连续内存原地映射到文件中。**再通过备份文件的方式备份哈希表**.

##### 降低哈希表的冲突概率

* 方法一： **调优哈希函数**

* > * 什么是好的哈希函数呢？
  >
  > * > * 首先它的计算量不能大
  >   > * 其次应尽量降低冲突概率
  >   > * 通过 hash 函数映射到数组下标一定会丢失一定的信息，我们希望丢失的这些信息是无关紧要的。
  >   > * 当找一个合适的**素数作为基数**(必须的)

* 方法二： **扩容**

* > * 扩容需要新老哈希表同时存在，通过遍历全部数据，用新的哈希函数把关键字放到合适的新哈希桶中。可见，扩容是一个极其耗时的操作，尤其在元素以亿计的情况下。
  >
  > * 在耗时以小时计的扩容过程中，如何持续提供正常服务呢？
  >
  > * > * 只要把一次性的迁移过程，**分为多次后台迁移**，且提供服务时能够根据迁移情况选择新老哈希表即可。







### 分布式理论

#### 分布式系统基础知识：

##### 理想的分布式系统

* 高性能： 可拓展、低时延、高吞吐
* 正确：一致性、易于理解
* 可靠：容错、高可用



* 共识协议不等于一致性

* > * 应用层面的不同的一致性，都可以使用共识协议来实现。
  > * 比如都是使用相同的共识算法，但是在应用层的实现的时候，我们可以故意返回旧的值（这个是应用层实现的）
  > * 简单的复制协议也可以实现线性一致性（例如只有一台节点，没法做到容错，但是是一个线性一致性的系统）



* 一般讨论共识协议提到的一致性都是线性一致性，因为弱一致性往往可以使用相对简单的复制算法实现





##### Raft

* 共识算法是一种算法，一致性是一个模型，两者有点差距

* 易于理解作为算法设计目标：

* > * 使用了 RSM 、log、RPC 的概念
  > * 直接使用 RPC 对算法进行描述
  > * Strong Leader-based 
  > * 使用了随机的方法减少约束

* 正确性

* > * 形式化验证
  > * 拥有大量的成熟系统



###### 复制状态机（RSM）

* RSM：（可以理解成 DB Engine 数据库的引擎）

* > * Raft 中的所有的 consensus 都是直接使用 Log 作为载体的
  > * 状态机是和服务端交互的机器引擎，可以理解成是一个 B+ 树形状的引擎，客户端也是从我们的引擎中取数据的。
  > * ![image-20220813225914609](/home/zhoubing/.config/Typora/typora-user-images/image-20220813225914609.png)

* Commited index

* > * 一旦 Raft 更新 Commited Index,意味着这个 Index 前的所有 Log 都可以提交给状态机
  > * Commited Index 是不持久化的，状态机也是 Volatile 的，重启之后从第一条 Log 开始
  > * 但是提交到状态机之后，就不会出现被丢弃，和被新的日志覆盖的情况



* 在任何的 <term, index> 位置中，至多存在一条 log
* 如果一个 log 被标记 commited（commit 可以理解就是持久化了）, 那么这个 log 一定会在未来所有的 Leader 中出现。



##### 一致性读写的定义

* 总的来说： **在读日志之前，需要确保之前的写日志已经提交到状态机了**

* 方案一：

* > * 写 log 被 commit 了，返回客户端成功（这个时候写 log 可能没有被提交到状态机）
  > * 读操作也写入一条 log, **状态机 apply 时返回 client** (这个时候读 log 都被提交了，之前的写操作日志一定也已经被提交了)
  > * 增加 Log 量

* **方案二：**

* > * 写 log 被 commit 了，返回客户端成功。
  > * 读操作先**等待所有 commited log apply**（有一批写日志需要提交才可以提供给读操作）,再读取状态机 （读操作可能需要之前的所有写日志提交到状态机，这样读操作才可以读取数据）
  > * 优化写时延（可能读时延较高）

* **方案三：**

* > * 写 log 被状态机 apply，返回给 client.（这样状态机一直是最新的，读操作可以直接读取操作）
  > * 读操作直接读取状态机
  > * 优化读时延（可能写时延较高）



##### 脑裂

![image-20220814011442248](/home/zhoubing/.config/Typora/typora-user-images/image-20220814011442248.png)

* client2 和 kv3 服务器读操作，**不涉及写操作**，但是现在网络断开了，**会造成 client2 一直读取的是一个旧的数据**

* 所以读操作，在脑裂的情况下不能直接读取操作（**本质是这个 leader 不是一个合法的 leade**r）

* 解决方案：(确认真的 leader)

* > * 方案一：
  >
  > * > * **读取数据之前通过一轮 heartbeat 确认 leadership** (获取多数派的响应)
  >
  > * 方案二： 
  >
  > * > * 通过上一轮 HeartBeat 时间来保证接下来的有段时间内 follower 不会 timeout
  >   > * 同时 follower 在这段时间不会投票操作
  >   > * 如果多数 follower 满足条件，那么在这段时间内保证不会出现新的 leader 产生
  >   > * ![image-20220814012455085](/home/zhoubing/.config/Typora/typora-user-images/image-20220814012455085.png)





#### CAP

1. C 一致性：分布式环境中，**一致性是指多个副本之间，在同一时刻能否有同样的值**
2. A 可用性：系统**提供的服务必须一直处于可用的状态**。即使集群中一部分节点故障。
3. P 分区容错性：系统在遇到节点故障，或者网络分区时，任然能对外提供一致性和可用性的服务。以实际效果而言，分区相当于通信的时限要求。系统如果不能在一定实现内达成数据一致性，也就意味着发生了分区的情况。必须就当前操作在 C 和 A 之前作出选择
4. **P ： 代表一个分区挂了，其他机器依旧可以提供服务**

* Q:有状态服务的性能优化， 有状态服务？
* A： 所谓有状态服务，是指进程会在处理完请求后，仍然保存着影响下次请求结果的用户数据，而无状态服务则只从每个请求的输入参数中获取数据，在请求处理完成后并不保存任何会话信息。

##### 怎样舍弃一致性提升性能？

* **纵向上添加缓存**，**横向上添加副本进程**两种做法
* 纵向是请求的处理路径，**横向则是同类服务之间的数据同步路**径。这样，在**纵向上在离客户端更近的位置增加数据的副本**，并把它存放在处理速度更快的物理介质上（缓存），
* ![img](https://static001.geekbang.org/resource/image/5a/06/5a6dbac922c500eea108d374dccc6406.png?wh=1086*733)
* 缓存完全有可能与源数据不同，这就是牺牲一致性来提升性能的典型例子。
* 缓存更新的设计： write back 和 write through
* write back 牺牲了更多的一致性，但带来了更低的请求时延。比如介绍过的 Linux 磁盘高速缓存就采用了 write back 这种设计，它虽然是单机内的一种缓存设计，但在分布式系统中缓存的设计方式也是一样的(先更新缓存再更新数据）。而write through **会在更新数据成功后再更新缓存**，虽然带来了很好的一致性，但写操作的时延会更久.
* ![img](https://static001.geekbang.org/resource/image/de/ac/de0ed171a392b63a87af28b9aa6ec7ac.png?wh=1087*751)
* 如果把缓存和对应的源看做异构的副本，**那write back就能对应于异步复制**，**write through就能对应于同步复制了**。
* 水平方向上，在更多的主机上添加数据副本(并让工作在副本上的进程同时对用户提供服务)：
* 进程间同步数据主要包含两种方式：**同步方式以及异步方式**
* 前者会在每个更新请求完成前，将数据成功同步到每个副本后，请求才会处理完成，而后者则会在处理完请求后，才开始异步地同步数据到副本中。
* ![img](https://static001.geekbang.org/resource/image/d7/f6/d770cf24f61d671ab1f0c1a6170627f6.png?wh=733*728)
* 异步一致性差，但是非常的高效。特别是在主进程宕机后，副本上的进程很容易出现数据丢失，但异步方式的性能要好得多，这不只由于更新请求的返回更快，而且**异步化后主进程可以基于合并、批量操作等技巧**，进行效率更高的数据同步。（比如 MySQL 的主备模式下，默认就使用异步方式同步数据）
* 结合定期更新的 Snapshot 快照，以及实时的 Oplog 操作日志，协作同步数据，这样效率会高很多。（类似 raft 的快照和日志）
* 快照是截止时间 T0 时所有数据的状态(raft de  snapshot），而操作日志则是 T0 时间到当下 T1 之间的所有更新操作（raft 中的日志部分），这样，副本载入快照恢复至 T0 时刻的数据后，再通过有限的操作日志与主进程保持一致。
* 



#### 到底什么是一致性

##### ACID 中的 C

* 通俗地说，它指的是任何一个数据库事务的执行，都应该让整个数据库保持在「一致」的状态。(以转账为例，需要保证转账前后，账户总额保持不变)

* 所谓状态是不是「一致」，其实是由**业务层规定的**。ACID中的「一致性」，其实是体现了**业务逻辑上的合理性**，并不是由数据库本身的技术特性所决定的。

* 为了让事务保持 ACID 的一致性，需要考虑的因素有： 1. 出错情况、2. 并发行为

* > * 第一类，事务本身的**实现逻辑**可能存在错误。(eg: 从账户A向账户B转账100元，在这个事务中，如果我们只从账号A中减去了100元，但忘记了往账户B中增加100元，那么这个事务就是错误的。)
  > * 解决上面的错误，是保持一致性的前提，这需要应用层进行恰当的编码来保证.
  > * 第二类，则是意想不到的各种**软硬件错误**。(eg: 还是从账户A向账户B转账100元，事务本身的**实现逻辑没有问题**，它先执行了从账号A中减去了100元，但在执行往账户B中增加100元之前，却发生了意想不到的错误，比如**进程突然crash了，或是磁盘满了，或是网络突然不通了**，或是其它任何可能的**硬件错误**。)
  > * 解决上面这个问题主要靠： 需要ACID中的**A（原子性）来保障了**。原子性保障了事务的执行要么全部成功，要么全部失败，而**不允许出现“只执行了一半”这种“部分成功”的情况**。
  > * **并发行为也可能会影响事务的一致性**。（同时对同一份数据进行处理，可能造成数据的不一致性）
  > * 解决方案： ACID中的**I（隔离性）来保障**： 对于并发执行的多个事务进行合理的排序，**保障了不同事务的执行互不干扰**。换言之，隔离性这种特性，能够让**并发执行的多个事务就好像是按照「先后顺序」执行的一样**。

* 结论：

* > * ACID中的一致性，是个很**偏应用层的概念**。这跟ACID中的原子性、隔离性和持久性有很大的不同。原子性、隔离性和持久性，都是**数据库本身所提供的技术特性**；而一致性，则是由**特定的业务场景规定的**。
  > * 要真正做到ACID中的一致性，它是要**依赖数据库的原子性和隔离性的**（应对错误和并发）。但是，就算数据库提供了所有你所需要的技术特性，也不一定能保证 ACID 的一致性。**这还取决于你在应用层对于事务本身的实现逻辑**是否正确无误。
  > * 最后，ACID中的一致性，甚至跟分布式都**没什么直接关系**。它跟**分布式的唯一关联在于，在分布式环境下，它所依赖的数据库原子性和隔离性更难实现**。



##### ACID 中的 原子性

* ACID中的原子性，要求事务的执行**要么全部成功，要么全部失败**，而不允许出现“部分成功”的情况。

* 在分布式事务中，这要求参与事务的所有节点，要么**全部执行Commit操作**，要么**全部执行Abort操作**。

* 参与事务的所有节点，需要在“**执行Commit还是Abort”这一点上达成一致**（其实就是共识）: **原子提交问题**

* **原子提交问题**与**共识问题**的关联性：

* > * 共识问题，解决的是如何在分布式系统中的**多个节点之间就某个提议达成共识**。
  > * 原子提交问题，解决的是参与分布式事务的**所有**节点在**“执行Commit还是Abort”这一点上达成共识**。
  > * 原子提交问题是共识问题的一个**特例**。

* 当我们描述**共识问题**的时候，我们说的是在**多个节点**之间达成共识；而当我们描述**原子提交问题**的时候，我们说的是在**所有节点**之间达成共识。这个细微的差别，让这两类问题，几乎变成了完全不同的问题（谁也替代不了谁）。

* 原子提交协议必须保证在参与分布式事务的**所有节点**（**包括故障的节点**）上对于“执行Commit还是Abort”达成共识。

* 原子提交问题被抽象成一个新的一致性问题，称为uniform consensus问题，它是与通常的共识问题（consensus problem）不同的问题，而且是**更难**的问题。uniform consensus，要求**所有节点（包括故障节点）都要达成共识**；**而consensus问题只关注没有发生故障的节点达成共识。**

* 总结：

* > * 共识问题（consensus problem），解决的是如何在分布式系统中的多个节点之间就某个提议达成共识。它**只关注没有发生故障的节点**达成共识就可以了。
  > * 在分布式事务中，ACID中的原子性，引出了**原子提交问题**，它解决的是参与分布式事务的所有节点在“执行Commit还是Abort”这一点上达成共识。原子提交问题属于uniform consensus问题，要求**所有节点**（包括故障节点）都要达成共识，是比consensus问题更难的一类问题。
  > * Paxos、 Raft和解决拜占庭将军问题的算法，解决的是consensus问题；2PC/3PC，解决的是一个特定的uniform consensus问题。



##### CAP 中的 C

* C : 线性一致性/强一致性

* > * 在一个并发执行的环境中，不同的操作之间可能是有严格的**先后关系的**（一个操作执行结束之后另一个操作才开始执行），**也可能是并发执行的**（一个操作还没执行结束，另一个操作就开始执行了）
  > * 如果能够把所有操作排列成一个「**合法**」的全局线性顺序，那么这些操作就是**满足线性一致性的**。
  > * 上面的合法如何理解：eg : 假如我们先往某个数据对象中写入了一个值（假设是1），然后在写入操作结束之后，我们再把这个数据对象读出来（在写入和读取操作之间没有其它的写入操作了）。如果我们发现读取到的值是1，那么就是合法的；而如果读出来的值不是1，那么就是非法的。
  > * 再假设，我们又执行了一次读取操作，发现读出来的值仍然是1，那么就是合法的；否则就是非法的。
  > * 总结： 也就是说，如果一个读操作已经读到了某个值，那么下一个对于同一个数据对象的读操作就必须读取到同样的值（除非在两次读操作之间还存在别的写入操作）。（多次读中间没有写发生，读多次读到都是一样的）

* 线性一致性的含义可以用一个具体的描述来取代：对于任何一个数据对象来说，**系统表现得就像它只有一个副本一样**。

* 如果系统对于**每个数据对象真的只存一个副本，那么肯定是满足线性一致性的**。但是**单一副本不具有容错性**，所以分布式存储系统一般都会对数据进行复制（replication），也就是保存多个副本。





#### 强弱一致性

* 分布式系统的好处： **容错性、可扩展性**

* 获取这些好处需要的代价： **复制 (*replication*) 和分片 (*sharding*)**

* 复制的好处： 

* > * **容错** (*fault tolerance*)。即使某些网络节点发生故障，由于原本保存着在故障节点上的数据在**正常节点上还有备份**，所以整个系统仍然可能是可用的。这也是我们期待分布式系统能够提供的「**高可用**」特性。
  > * **提升吞吐量**。将一份数据复制多份并保存在多个副本节点上，还顺便带来一个好处：对于同一个数据对象的访问请求（至少是读请求）可以由多个副本节点分担，从而使得整个系统可以随着请求量的增加不断扩展。

* 让所有副本在任何时刻都保持一致，是不可能的。因为副本之间的数据同步即使速度再快，也是需要时间的。即使在两次「观察」之间，系统内部出现了**短暂的数据不一致的情况**，**只要系统保证外部用户无论如何都发现不了**，我们也是可以满意的。

* 一个分布式系统对于读写操作的某种排序和执行规则，就定义了一种一致性模型 (*consistency model*)。当一个系统选定了某种特定的一致性模型（比如线性一致性或顺序一致性），那么你就**只能看到这种一致性模型所允许的那些操作序列**。

* 分布式系统的关键概念： 

* > - 整个系统可以看成由多个**进程**和一个共享的**数据存储**组成。对于数据存储的读写操作由进程发起。这里的进程，相当于本文前面提到的系统用户或系统使用者。
  > - 同一个进程发起的读写操作是先后顺序执行的。注意，这里的「进程」概念跟我们平常编程时用到的进程有所不同，进程里面不再分多个线程了。
  > - 数据存储可能有多个副本，但我们在讨论一致性模型的时候，把它看成一个整体来看待，不**区分读写操作提交到了具体哪个副本上。**
  > - 每个操作的执行，从开始调用到执行结束，都需要花一定的时间。因此，一个进程发起的操作还没有执行完的时候，另一个进程的操作可能就已经开始了。

##### 顺序一致性

* 顺序一致性的定义： 如果一个并发执行过程所包含的**所有读写操作能够重排成一个全局线性有序的序列**，并且这个序列满足以下两个条件，那么这个并发执行过程就是满足顺序一致性的：

* > - **条件I**：重排后的序列中每一个**读操作返回的值**，必须等于前面对同一个数据对象的**最近一次写操作所写入的值**。
  >
  > - **条件II**：原来**每个进程 **(同一个进程中) 中各个操作的执行先后顺序，在这个重排后的序列中必须保持一致。
  >
  > - ![图片](https://mmbiz.qpic.cn/mmbiz_png/qR4rtg9WfegHUZOs2jRmCBzF8SvHUuNObj5mS0XsKibFG24FVTegdUlJFTd6cJ9TpuLFUuSnayOsmyh2l86RhdA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
  >
  > - 以上图的执行过程为例，我们重排所有的6个读写操作，可以得到如下的有序序列：
  >
  >   1. *A* --> *w**1*(*x*)
  >   2. *r**3*(*x*) --> *A*
  >   3. *C* --> *w**2*(*x*)
  >   4. *r**3*(*x*) --> *C*
  >   5. *B* --> *w**1*(*x*)
  >   6. *r**3*(*x*) --> *B*
  >
  > - > - 条件I：在这个重排后的序列中，每个读操作都返回了前面最近一次写入的值，比如第2个操作读到的值*A*，是前面第1个操作写入的；第4个操作读到的值*C*，是前面第3个操作写入的。
  >   > - 条件II：原来进程*P*1中的两个写操作，*A* --> *w**1*(*x*)和*B* -->*w**1*(*x*)，在这个重排后的序列中仍然保持了先后顺序。与此类似，原来进程*P*3中的3个读操作，在这个重排后的序列中也保持了原来的先后顺序。
  >
  > - **问题**： **进程*P*2将数据对象*x*的值写成了*C*之后，进程*P*3仍然读到了旧的值（*A*）。**

##### 线性一致性

定义： 在顺序一致性的基础上，增加下面的条件： 

* **条件III**：不同进程的操作，如果在时间上不重叠，那么它们的执行先后顺序，在这个重排后的序列中必须保持一致。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/qR4rtg9WfegHUZOs2jRmCBzF8SvHUuNObj5mS0XsKibFG24FVTegdUlJFTd6cJ9TpuLFUuSnayOsmyh2l86RhdA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 与上面同样的道理，在重排后的序列中，进程*P*2的*C* -->*w**2*(*x*)必须排在进程*P*3的*r**3*(*x*) --> *A*之前。所以上面这张图一定**不满足线性一致性**条件

* > * ![图片](https://mmbiz.qpic.cn/mmbiz_png/qR4rtg9WfegHUZOs2jRmCBzF8SvHUuNO2zNKfo5iaWEma0FPUWG3HD0BXxP5X4cv75H3eIU3dXBqIGIrWGuspwA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
  > * 上图的执行过程，所有操作重排后，可以得到如下的有序序列：
  >   1. *A* --> *w**1*(*x*)
  >   2. *r**3*(*x*) --> *A*
  >   3. *C* --> *w**2*(*x*)
  >   4. *r**3*(*x*) --> *C*
  >   5. *B* --> *w**1*(*x*)
  >   6. *r**3*(*x*) --> *B*
  > * 这个序列是满足所有的条件I、条件II和条件III这三个条件的。因此，这个执行过程满足线性一致性。

* **顺序一致性和线性一致性的比较：**

* > - 它们都试图让系统“表现得像只有一个副本”一样。
  > - 它们都保证了程序执行顺序不会被打乱。体现在条件II对于进程内各个操作的排序保持上。
  > - 线性一致性考虑了**时间先后顺序**，而顺序一致性没有。
  > - 满足线性一致性的执行过程，肯定都满足顺序一致性；反之不一定。



##### 最终一致性

* 线性一致性和顺序一致性属于*safety property*（安全性）；而最终一致性属于*liveness property*（活性）

* 一个并发程序或者一个分布式系统，它们的执行所展现出来的系统属性，可以分为两大类：

* > * **safety**：它表示「**坏事」永远不会发生**。比如，一个系统如果遵守线性一致性或顺序一致性，那么就永远不会出现违反三个（对于顺序一致性来说是两个）条件的执行过程。而一旦系统出现问题，*safety*被违反了，我们也能明确指出是在哪个时间点上出现意外的。
  > * **liveness**：它**表示「好事」最终会发生**。这种属性听起来会比较神奇：在任何一个时间点，你都无法判定*liveness*被违反了。因为，即使你期望的「好事」还没有发生，也不代表它未来不会发生。就像最终一致性一样，即使当前系统处于不一致的状态，也不代表未来系统就不会达到一致的状态。而只要**系统存在“在未来某个时刻达到一致状态”的可能性，最终一致性就没有被违反。**另外，可用性 (*availability*) 也属于*liveness*属性。



















### Redis

#### 基本数据类型

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/redis%E5%91%BD%E4%BB%A4%E6%8F%90%E7%BA%B2.png)

#### Redis 常见数据类型

* **String ：**
* String 类型的底层的数据结构实现主要是 int 和 SDS（简单动态字符串）。
* SET 命令有个 NX 参数可以实现「key不存在才插入」，可以用它来实现分布式锁：
  - 如果 key 不存在，则显示插入成功，可以用来表示加锁成功；
  - 如果 key 存在，则会显示插入失败，可以用来表示加锁失败。
* **List:**
* List 类型的底层数据结构是由**双向链表或压缩列表**实现的：
* 使用场景： 消息队列
* **Hash:**
* Hash 类型的底层数据结构是由**压缩列表或哈希表**实现的：
* 使用场景：
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/%E8%B4%AD%E7%89%A9%E8%BD%A6.png)
* 以用户 id 为 key，商品 id 为 field，商品数量为 value，恰好构成了购物车的3个要素
* 添加商品：`HSET cart:{用户id} {商品id} 1`
* 添加数量：`HINCRBY cart:{用户id} {商品id} 1`
* 商品总数：`HLEN cart:{用户id}`
* 删除商品：`HDEL cart:{用户id} {商品id}`
* 获取购物车所有商品：`HGETALL cart:{用户id}`
* 当前仅仅是将**商品ID存储到了Redis 中**，在回显商品具体信息的时候，还需要拿着**商品 id 查询一次数据库**，获取完整的商品的信息。
* 使用场景：缓存对象
* Q: String + Json也是存储对象的一种方式，那么存储对象时，到底用 String + json 还是用 Hash 呢？
* A：一般对象用 String + Json 存储，对象中某些频繁变化的属性可以考虑抽出来用 Hash 类型存储。
* **SET**
* Set 类型的底层数据结构是由**哈希表或整数集合**实现的：
* Set 类型和 List 类型的区别如下：
  - List 可以存储重复元素，Set 只能存储非重复元素；
  - List 是按照元素的先后顺序存储元素的，而 Set 则是无序方式存储元素的。
* 使用场景：点赞、共同关注、抽奖活动
* **ZSET：**有序 set 
* Zset 类型的底层数据结构是由**压缩列表或跳表**实现的：
* 使用场景: 排行榜、电话姓名排序。
* sortSet中不仅使用了跳表，**与此同时还使用了hashtable来存储对象以及对应的分值，方便快速查询对象对应的分值。**
* **BitMap**：
* Bitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。
* String 类型是会保存为**二进制的字节数组**，所以，Redis 就把字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态，你可以把 Bitmap 看作是一个 bit 数组
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/bitmap.png)
* 应用场景： **Bitmap 类型非常适合二值状态统计的场景**，这里的二值状态就是指集合元素的取值就只有 0 和 1 两种，在记录海量数据时，Bitmap 能够有效地节省内存空间。
* 实际场景：签到统计





#### Redis 数据结构

![img](https://img-blog.csdnimg.cn/img_convert/9fa26a74965efbf0f56b707a03bb9b7f.png)



![img](https://img-blog.csdnimg.cn/img_convert/a9c3e7dc4ac79363d8eb8eb2290a58e6.png)



<img src="https://static001.geekbang.org/resource/image/fb/f0/fb7e3612ddee8a0ea49b7c40673a0cf0.jpg?wh=1332*750" alt="img" style="zoom:33%;" />

##### 压缩列表：(list hash zset)

* Redis 对象（List 对象、Hash 对象、Zset 对象）包含的**元素数量较少，或者元素值不大的情况**才会使用压缩列表作为底层数据结构。

* Redis的配置文件中关于有序集合底层实现的两个配置。
  1、zset-max-ziplist-entries 128:zset采用压缩列表时，元素个数最大值。默认值为128。
  2、zset-max-ziplist-value 64:zset采用压缩列表时，每个元素的字符串长度最大值。默认值为64。

* **当满足以下任一条件时，Redis便会将zset的底层实现由压缩列表转为跳跃表**。
  1、zset中元素个数大于zset_max_ziplist_entries；
  2、插入元素的字符串长度大于zset_max_ziplist_value。
  zset在转为跳跃表之后，即使元素被逐渐删除，也不会重新转为压缩列表。

* 因为ziplist是紧凑存储，没有冗余空间，意味着新插入元素，就需要扩展内存： 

* > * 1、分配新的内存，将原数据拷贝到新内存；  
  > * 2、扩展原有内存; 所以ziplist 不适合存储大型字符串，存储的元素也不宜过多。

* 压缩列表是 Redis 为了节约内存而开发的，它是**由连续内存块组成的顺序型数据结构**，有点类似于数组。

* 缺陷:

* > - 不能保存过多的元素，否则**查询效率就会降低**；
  >
  > - > * 在压缩列表中，如果我们要查找定位**第一个元素和最后一个元素**，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。而**查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了，因此压缩列表不适合保存过多的元素**。
  >
  > - 新增或修改某个元素时，压缩列表占用的内存空间需要重新分配，甚至可能引发**连锁更新**的问题。
  >
  > - > * **压缩列表新增某个元素或修改某个元素时，如果空间不不够，压缩列表占用的内存空间就需要重新分配。而当新插入的元素较大时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起「连锁更新」问题，导致每个元素的空间都要重新分配，造成访问压缩列表性能的下降**。

* ![img](https://img-blog.csdnimg.cn/img_convert/ab0b44f557f8b5bc7acb3a53d43ebfcb.png)

* ***zlbytes***，记录整个压缩列表占用对内存字节数；

* ***zltail***，记录压缩列表「尾部」节点距离起始地址由多少字节，也就是列表尾的偏移量；

* ***zllen***，记录压缩列表包含的节点数量；

* ***zlend***，标记压缩列表的结束点，固定值 0xFF（十进制255）。

![img](https://img-blog.csdnimg.cn/img_convert/a3b1f6235cf0587115b21312fe60289c.png)



压缩列表节点包含三部分内容：

- ***prevlen***，记录了「前一个节点」的长度；
- ***encoding***，记录了当前节点实际数据的类型以及长度；
- ***data***，记录了当前节点的实际数据；

* prevlen 和 encoding 是如何根据**数据的大小和类型来进行不同的空间大小分配**。

*  prevlen 属性的空间大小跟前一个节点长度值有关，比如：

* > - 如果**前一个节点的长度小于 254 字节**，那么 prevlen 属性需要用 **1 字节的空间**来保存这个长度值；
  > - 如果**前一个节点的长度大于等于 254 字节**，那么 prevlen 属性需要用 **5 字节的空间**来保存这个长度值；

* encoding 属性的空间大小跟数据是字符串还是整数，以及字符串的长度有关：

* > - 如果**当前节点的数据是整数**，则 encoding 会使用 **1 字节的空间**进行编码。
  > - 如果**当前节点的数据是字符串，根据字符串的长度大小**，encoding 会使用 **1 字节/2字节/5字节的空间**进行编码。



###### 连续更新缺陷

* 如果将**一个长度大于等于 254 字节的新节点加入到压缩列表的表头节点**，即新节点将成为 e1 的前置节点，如下图：
* ![img](https://img-blog.csdnimg.cn/img_convert/d1a6deff4672580609c99a5b06bf3429.png)
* 因为 e1 节点的 prevlen 属性只有 1 个字节大小，无法保存新节点的长度，此时就需要对压缩列表的空间重分配操作，**并将 e1 节点的 prevlen 属性从原来的 1 字节大小扩展为 5 字节大小**。（这样 e1 长度页超过 254 字节）
* ![img](https://img-blog.csdnimg.cn/img_convert/1f0e5ae7ab749078cadda5ba0ed98eac.png)
* **这种在特殊情况下产生的连续多次空间扩展操作就叫做「连锁更新」**。
* 空间扩展操作也就是重新分配内存，因此**连锁更新一旦发生，就会导致压缩列表占用的内存空间要多次重新分配，这就会直接影响到压缩列表的访问性能**。







##### 跳表

* zset 使用的是 跳表 +  hash 表

* ```cpp
  typedef struct zset{
       //跳跃表
       zskiplist *zsl;
       //字典
       dict *dice;
  } zset;
  
  ```

* 字典的**键保存元素的值**，字典的**值则保存元素的分值**；跳跃表节点的 object 属性保存元素的值，跳跃表节点的 score 属性保存元素的分值。

* Q : 为什么不直接使用跳跃表

* 假如我们单独使用 字典，虽然能以 **O(1) 的时间复杂度查找成员的分值**，但是因为字典是以无序的方式来保存集合元素，所以**每次进行范围操作的时候都要进行排序**

* 假如我们单独使用跳跃表来实现，虽然能执行范围操作，但是查找操作有 O(1)的复杂度变为了O(logN)。因此**Redis使用了两种数据结构来共同实现有序集合**。

![img](https://static001.geekbang.org/resource/image/5c/c9/5c438c652d38823f88fa8dedd9144bc9.jpg?wh=1920x766)

* 虽然单点查询的效率确实不如哈希表，但跳表可以很好地支持范围查询，只要找到对应的范围节点，然后顺次在链表上遍历就可以了，

* 插入的例子

* > * 希望插入 val=87 的元素:
  > * ![img](https://static001.geekbang.org/resource/image/08/23/08f310416b63e2f3b0bd399730299d23.jpg?wh=1920x766)
  > * 第一步当然就是要做搜索，找到链表中离 87 最近的 86，并记录路径上每一层的节点：
  > * ![img](https://static001.geekbang.org/resource/image/af/da/af3804d1bfed4f6379af338c0c76efda.jpg?wh=1920x766)
  > * 比如我们连续抛了两次正面的硬币，第三次抛了反面，最终就只在 1、2、3 层中添加了 87 节点：
  > * ![img](https://static001.geekbang.org/resource/image/87/43/87fb3596yyc6c5fc2fa45c72f894a443.jpg?wh=1920x766)

* 跳表和红黑树比唯一的劣势可能就是整体内存占的多一些了

* 增加删除搜索 ： O(logn)， 空间复杂度： O(n)

* 虽然**单点查询的效率确实不如哈希表，但跳表可以很好地支持范围查询，只要找到对应的范围节点，然后顺次在链表上遍历就可以了**，这一点比红黑树也有明显优势。

* **sortSet中不仅使用了跳表，与此同时还使用了hashtable来存储对象以及对应的分值，方便快速查询对象对应的分值。**

```cpp
#include<bits/stdc++.h>

using namespace std;

struct Node {
    // 至少需要向右向下指针

    Node* right;
    Node* down;
    int val;
    Node(Node* right, Node* down, int val) : right(right), down(down), val(val) {}
};

Node* head;

bool search(int target) {
    Node* p = head;
    while (p) {
        // 左右寻找区间，总是通过大区间确定一个个小区间
        while (p -> right && p -> right -> val < target) {
            p = p -> right;
        }
        // 没找到目标，继续向下寻找
        if (!p -> right || p -> right -> val > target) {
            p = p -> down;
        } else {
            return true;
        }
    }
    return false;
}

void add(int num) {
    // 从上至下记录搜索路径
    vector<Node*>pathList;
    Node* p = head;
    // 从上到下搜索次数小于 Num 的数字
    while (p) {
        while (p -> right && p -> right -> val < num) {
            p = p -> right;
        }
        pathList.push_back(p);
        p = p -> down;
    }
    // 从上至下搜索路径回溯， 50% 概率
    // 这里实现是会保证不会超过当前层数，然后靠头结点去额外的加层，即每次新增一层。
    Node* downNode = nullptr;
    bool insertUp = true;
    while (insertUp && pathList.size() > 0) {
        Node* insert = pathList.back();
        pathList.pop_back();
        // add 新结点
        insert -> right = new Node(insert -> right, downNode, num);
        downNode = insert -> right; // 从下向上插入的
        // 注意这个时候 insert -> right 就变成刚刚插入的结点了
        // 50% 的概率
        insertUp = (rand() & 0x1) == 0;
    }
    if (insertUp) {
        // 表示的是需要起一个新的层了。
        head = new Node(new Node(NULL, downNode, num), head, -1);
    }
}

bool erase(int num) {
    Node* p = head;
    bool seen = false;
    while (p) {
        // 我们都是找的 p -> right 结点，这样也是方便我们的删除操作
        while (p -> right && p -> right -> val < num) {
            p = p -> right;
        }
        // 无效点，或者 p -> right -> val > num 那么我们向下找一下
        if (!p -> right || p -> right -> val > num) {
            p = p -> down;
        } else {
            // 找到了这个元素
            seen = true;
            p -> right = p -> right -> right;
            p = p -> down; // 还需要接着往下找，因为可能有多个结点指向当前的元素
        }
    }
    return seen; // 返回删除成功或者失败
}

int main() {
    head = new Node(nullptr, nullptr, -1);
    add(0);
    search(0);
    erase(0);
    return 0;
}
```



##### hash 表(hash / set)

* **Redis 采用了「链式哈希」来解决哈希冲突**，在不扩容哈希表的前提下，将具有相同哈希值的数据串起来，形成链接起，以便这些数据在表中仍然可以被查询到。
* 链接法（Java 标准库采用的方式）虽然实现简单，**还允许存放元素个数大于数组的大小**（也叫装载因子大于 1），但链接法序**列化数**据的代价很大，**因为使用了指针后，内存是不连续的**。（内存不连续，序列化比较难）
* 链式哈希局限性也很明显，**随着链表长度的增加，在查询这一位置上的数据的耗时就会增加，毕竟链表的查询的时间复杂度是 O(n)。**
* 序列化： 面对上亿条数据，为了保证可靠性，需要做灾备恢复，我们可以结合快照 +oplog 方式恢复数据，但内存中的哈希表如何快速地序列化为快照文件？
* 如果能将数据完整的放进数组，那么开放寻址法已经解决了序列化问题，所以我们应该选择开放寻址法。

###### rehash

```cpp
typedef struct dict {
    …
    //两个Hash表，交替使用，用于rehash操作
    dictht ht[2]; 
    …
} dict;
```

* 之所以定义了 2 个哈希表，是因为进行 rehash 的时候，需要用上 2 个哈希表了。

* ![img](https://img-blog.csdnimg.cn/img_convert/2fedbc9cd4cb7236c302d695686dd478.png)

* 执行流程

* > - 给「哈希表 2」 分配空间，一般会比「哈希表 1」 大 2 倍；
  > - 将「哈希表 1 」的数据迁移到「哈希表 2」 中；
  > - 迁移完成后，「哈希表 1 」的空间会被释放，并把「哈希表 2」 设置为「哈希表 1」，然后在「哈希表 2」 新创建一个空白的哈希表，为下次 rehash 做准备。

* ![img](https://img-blog.csdnimg.cn/img_convert/cabce0ce7e320bc9d9b5bde947b6811b.png)

* 问题

* > * **如果「哈希表 1 」的数据量非常大，那么在迁移至「哈希表 2 」的时候，因为会涉及大量的数据拷贝，此时可能会对 Redis 造成阻塞，无法服务其他请求**。





###### 渐进 rehash 

* 出现的原因： 为了避免 rehash 在数据迁移过程中，因拷贝数据的耗时，影响 Redis 性能的情况，所以 Redis 采用了**渐进式 rehash**

* 将数据的迁移的工作不再是一次性迁移完成，而是**分多次迁移**。

* 步骤：

* > - 给「哈希表 2」 分配空间；
  > - **在 rehash 进行期间，每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上**；
  > - 随着处理客户端发起的哈希表操作请求数量越多，最终在某个时间点会把「哈希表 1 」的所有 key-value 迁移到「哈希表 2」，从而完成 rehash 操作。

* 渐进 rehash 阶段查找过程：首选去哈希表 1 中进行查找，如果没有找到，就会继续去 哈希表 2 里面进行查找。



###### 执行 rehash 的时机：

- **当负载因子大于等于 1 ，并且 Redis 没有在执行 bgsave 命令或者 bgrewiteaof 命令，也就是没有执行 RDB 快照或没有进行 AOF 重写的时候，就会进行 rehash 操作。**
- **当负载因子大于等于 5 时，此时说明哈希冲突非常严重了，不管有没有有在执行 RDB 快照或 AOF 重写，都会强制进行 rehash 操作。**
- ![img](https://img-blog.csdnimg.cn/img_convert/85f597f7851b90d6c78bb0d8e39690fc.png)
- 









##### ListPack (hash / zset)

* 设计目的：替代压缩列表（彻底避免连锁更新的问题）

*  特点： **listpack 中每个节点不再包含前一个节点的长度了**，压缩列表每个节点正因为需要保存前一个节点的长度字段，就会有连锁更新的隐患。

* ![img](https://img-blog.csdnimg.cn/img_convert/c5fb0a602d4caaca37ff0357f05b0abf.png)

* listpack 节点的主要内容：

* > - encoding，定义该元素的编码类型，会对不同长度的整数和字符串进行编码；
  > - data，实际存放的数据；
  > - len，encoding+data的总长度；

* **listpack 没有压缩列表中记录前一个节点长度的字段了，listpack 只记录当前节点的长度，当我们向 listpack 加入一个新元素的时候，不会影响其他节点的长度字段的变化，从而避免了压缩列表的连锁更新问题**.









##### 整数集合(set)

* 当一个 Set 对象只包含整数值元素，并且元素数量不大时，就会使用整数集这个数据结构作为底层实现.

* ```c
  typedef struct intset {
      //编码方式
      uint32_t encoding;
      //集合包含的元素数量
      uint32_t length;
      //保存元素的数组
      int8_t contents[];
  } intset;
  ```









##### quicklist (list)

* List 对象的底层数据结构是双向链表或者压缩列表。然后在 Redis 3.2 的时候，List 对象的底层改由 quicklist 数据结构实现。

* 其实 quicklist 就是「双向链表 + 压缩列表」组合，因为一个 quicklist 就是一个链表，而链表中的每个元素又是一个**压缩列表**。

* 压缩列表的连锁更新如何解决： 

* > * quicklist 解决办法，**通过控制每个链表节点中的压缩列表的大小或者元素个数，来规避连锁更新的问题。因为压缩列表元素越少或越小，连锁更新带来的影响就越小，从而提供了更好的访问性能。**

* ![img](https://img-blog.csdnimg.cn/img_convert/f46cbe347f65ded522f1cc3fd8dba549.png)

* quicklist 会**控制 quicklistNode 结构里的压缩列表的大小或者元素个数**，来规避潜在的连锁更新的风险，但是这并没有完全解决连锁更新的问题。













##### SDS 简单动态字符串

* Redis 是用 C 语言实现的，但是它没有直接使用 C 语言的 char* 字符数组来实现字符串，而是自己封装了一个名为**简单动态字符串（simple dynamic string，SDS） 的数据结构来表示字符串**，也就是 Redis 的 String 数据类型的底层数据结构是 SDS。

* 除了字符串的末尾之外，**字符串里面不能含有 “\0” 字符**，否则最先被程序读入的 “\0” 字符将被误认为是字符串结尾，这个限制使得 C 语言的字符串只能保存文本数据，**不能保存像图片、音频、视频文化这样的二进制数据（\*这也是一个可以改进的地方\*）**

*  C 语言标准库中字符串的操作函数是很不安全的，对程序员很不友好，稍微一不注意，就会导致缓冲区溢出。

*  C 语言的字符串不足之处以及可以改进的地方：

* > * 获取字符串长度的时间复杂度为 O（N）；
  > * 字符串的结尾是以 “\0” 字符标识，字符串里面不能包含有 “\0” 字符，因此不能保存二进制数据；
  > * 字符串操作函数不高效且不安全，比如有缓冲区溢出的风险，有可能会造成程序运行终止；

![img](https://img-blog.csdnimg.cn/img_convert/516738c4058cdf9109e40a7812ef4239.png)

* **len，记录了字符串长度**。这样获取字符串长度的时候，只需要返回这个成员变量值就行，时间复杂度只需要 O（1）。

* **alloc，分配给字符数组的空间长度**。这样在修改字符串的时候，可以通过 `alloc - len` 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会**自动**将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出的问题。

* > * 内存不够如何扩容：
  >
  > * > * **当判断出缓冲区大小不够用时，Redis 会自动将扩大 SDS 的空间大小（小于 1MB 翻倍扩容，大于 1MB 按 1MB 扩容）**
  >   > * 

* **flags，用来表示不同类型的 SDS**。一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64，后面在说明区别之处。

* > * **之所以 SDS 设计不同类型的结构体，是为了能灵活保存不同大小的字符串，从而有效节省内存空间**。比如，在保存小字符串时，结构头占用空间也比较少。
  > * Redis 在编程上还**使用了专门的编译优化来节省内存空间**，即在 struct 声明了 `__attribute__ ((packed))` ，它的作用是：**告诉编译器取消结构体在编译过程中的优化对齐，按照实际占用字节数进行对齐**。

* **buf[]，字符数组，用来保存实际数据**。不仅可以保存字符串，也可以保存二进制数据。

###### 优势：

* 解决 c 字符串数组的哪些问题：

* > * O(1) 时间复杂度 获取字符串的长度
  >
  > * 二进制安全
  >
  > * > * ，而是**有个专门的 len 成员变量来记录长度，所以可存储包含 “\0” 的数据**。但是 SDS 为了兼容部分 C 语言标准库的函数， SDS 字符串结尾还是会加上 “\0” 字符。
  >   > * SDS 的 API 都是以处理二进制的方式来处理 SDS 存放在 buf[] 里的数据，程序不会对其中的数据做任何限制，数据写入的时候时什么样的，它被读取时就是什么样的。
  >   > * 使用二进制安全的 SDS，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，也可以**保存任意格式的二进制数据**
  >
  > * 不会出现缓冲区溢出问题
  >
  > * > * 大多数（比如 strcat 追加字符串函数）都是不安全的，因为这些函数把缓冲区大小是否满足操作需求的工作交由开发者来保证，**程序内部并不会判断缓冲区大小是否足够用**，当发生了缓冲区溢出就有可能造成程序异常结束。
  >   > * Redis 的 SDS 结构里引入了 alloc 和 len 成员变量，这样 SDS API 通过 `alloc - len` 计算，可以算出**剩余可用的空间大小**，这样在对字符串做修改操作的时候，就可以由**程序内部判断缓冲区大小是否足够用**。
  >   > * **当判断出缓冲区大小不够用时，Redis 会自动将扩大 SDS 的空间大小（小于 1MB 翻倍扩容，大于 1MB 按 1MB 扩容）**，以满足修改所需的大小。
  >   > * 



#### Redis 如何应对并发访问

* Q : 比如说如果多个用户同时下单，就会对缓存在 Redis 中的商品库存并发更新?

* A : 把多个操作在 Redis 中实现成一个操作，也就是单命令操作；把多个操作写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本。

* Q: Redis 是单线程为什么会存在并发访问数据问题

* > * 单线程只保证了单操作的原子性。(例如这个单操作： 读操作，写操作)，但是这些单操作之间可能存在并发访问问题
  > * 比如现在有一组操作GET+SET, 这两个操作（两个客户端请求）对应着两个到redis server的 TCP连接，请求A和B同时过来，A和B先后get，然后比如得到结果10，**此时10是放到了调用方的内存里**，然后A和B各调用一次SET(比如都+1)，此时A和B的操作都是set(key,11), A和B的SET的tcp连接到redis后，会被顺序执行(单进程)，也就被设置了两个11, 对于我们来说，应该是一个设置11，一个设置12才对。所以需要把这两个动作放到一个lua脚本里面，把整个脚本通过一个tcp连接发过去，由于redis单进程的特性，此时lua脚本就会被完整执行，在A的lua脚本中，先得到10，然后set成11，在B的脚本中，先得到11，然后set成12。对于我们来说，GET和SET就是原子的了。
  > * ![img](https://static001.geekbang.org/resource/image/dc/5c/dce821cd00c1937b4aab1f130424335c.jpg?wh=2925*1801)
  > * 临界区代码中的客户端**读取数据、更新数据、再写回数据**涉及了三个操作，而**这三个操作在执行时并不具有互斥性**，多个客户端基于相同的初始值进行修改，而不是基于前一个客户端修改后的值再修改。





##### 并发访问如何进行控制？

* 对**多个客户端访**问操作**同一份数据**的过程进行控制，以保证任何一个客户端发送的操作在 Redis 实例上执行时具有互斥性

* eg: 客户端 A 的访问操作在执行时，客户端 B 的操作不能执行，**需要等到 A 的操作结束后，才能执行**。

* 客户端修改数据过程： （注意不是直接在服务器端修改）

* > * 客户端先把数据读取到本地，在本地进行修改；
  >
  > * 客户端修改完数据后，再写回 Redis。
  >
  > * 这个流程叫做**“读取 - 修改 - 写回”**操作（这三个操作不是原子性的）
  >
  > * ```cpp
  >   current = GET(id)  // Get() 操作是 Redis 的操作，符合单线程原子性
  >   current-- // 这个操作不同客户端会自己完成自己的，出错
  >   SET(id, current) // 这个是 Redis 操作原子性，但是多个客户端会输入相同的数据
  >   ```
  >
  > * 解决方法： 1.加锁 2. 原子操作
  >
  >   ```cpp
  >   LOCK() // 以下操作只能一个客户端执行成功
  >   current = GET(id)
  >   current--
  >   SET(id, current)
  >   UNLOCK()
  >   ```
  >
  > * 加锁保证了互斥性，但是**加锁也会导致系统并发性能降低**。 加锁之后同一时间段只能允许一个线程操作，其他线程就会进入等待状态，当并发量大的时候，就会有大量线程进入等待，导致性能变慢。
  >
  > * 原子操作也能实现并发控制，但是原子操作对系统并发性能的影响较小





##### Redis 两种原子操作

* 读可能需要原子操作，写一定需要原子操作。

* 把多个操作在 Redis 中实现成一个操作，也就是单命令操作；（适用范围比较小）

* 把多个操作写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本。

* Redis 是使用单线程来**串行处理客户端的请求操作命令**的，所以，当 Redis 执行某个命令操作时，其他命令是无法执行的，这相当于**命令操作是互斥执行**的

* 如果我们执行的 RMW 操作是对数据进行增减值的话，Redis 提供的原子操作 INCR 和 DECR 可以直接帮助我们进行并发控制。（三个命令合并成一个 Redis 命令）

* 执行的操作不是简单地增减数据，而是有**更加复杂的判断逻辑或者是其他操作**， 如何处理？ 

* > * Lua 脚本
  >
  > * Redis 会**把整个 Lua 脚本作为一个整体执行**，在执行的过程中不会被其他命令打断，从而保证了 Lua 脚本中操作的原子性。
  >
  > * 复杂逻辑无法一个 Redis 命令完成： 我们可以把访问次数加 1、判断访问次数是否为 1，以及设置过期时间这三个操作写入一个 Lua 脚本。
  >
  > * ```lua
  >   // Lua 脚本，三个命令放一起
  >   local current
  >   current = redis.call("incr",KEYS[1])
  >   if tonumber(current) == 1 then
  >       redis.call("expire",KEYS[1],60)
  >   end
  >   ```
  >
  > * ```lua
  >   // 我们编写的脚本名称为 lua.script，我们接着就可以使用 Redis 客户端，带上 eval 选项，来执行该脚本。脚本所需的参数将通过以下命令中的 keys 和 args 进行传递。
  >   redis-cli  --eval lua.script  keys , args
  >   ```
  >
  > * 减少 lua 脚本中的命令，可以降低Redis执行脚本的时间，避免阻塞 Redis。（**但是在高并发中还是可能需要将读操作加入到 Lua 脚本中，不仅仅是写操作才需要并发控制**）







#### Redis 分布式锁

* 下文提及的客户端是什么？
* A： 客户端是指向Redis发送读写请求的**应用程序**。你举的例子中，app的下单请求先是被发送到后端的服务器上，一般来说，服务器上还有业务程序先解析了app的请求，然后再向Redis发送读写请求，在这种情况下，**服务器上的业务程序就是Redis的客户端**。而app、浏览器算是服务器上业务程序的客户端。



* 在分布式系统中，当有**多个客户端需要获取锁时**，我们需要分布式锁。此时，**锁是保存在一个共享存储系统中的，可以被多个客户端共享访问和获取。**

* Redis 本身可以被多个客户端共享访问，正好就是一个共享存储系统，可以用来保存分布式锁。

* Redis 的读写性能高，可以应对高并发的锁操作场景

* 分布式锁同样可以用一个变量来实现。客户端加锁和释放锁的操作逻辑，也和单机上的加锁和释放锁操作逻辑一致：**加锁时同样需要判断锁变量的值，根据锁变量值来判断能否加锁成功；释放锁时需要把锁变量值设置为 0，表明客户端不再持有锁。**

* 在**分布式场景下，锁变量需要由一个共享存储系统来维护，只有这样，多个客户端才可以通过访问共享存储系统来访问锁变量**。（Redis 就是一个非常理想的共享存储空间）

* 加锁和释放锁的操作就变成了**读取、判断和设置共享存储系统中的锁变量值**。

* 设计分布式锁的要求：

* > * 要求一：分布式锁的加锁和释放锁的过程，涉及多个操作。所以，在实现分布式锁时，我们需要保证这些锁操作的**原子性**；
  > * 要求二：共享存储系统保存了锁变量，如果**共享存储系统发生故障或宕机**，那么客户端也就无法进行锁操作了。在实现分布式锁时，我们需要考虑保证共享存储系统的可靠性，进而保证锁的**可靠性**。(基于多个 Redis 节点)





##### 单 Redis 实现分布式锁

* Redis 可以使用**键值对来保存锁变量**，再接收和处理不同客户端发送的加锁和释放锁的操作请求。

* 赋予锁变量一个变量名，把这个**变量名作为键值对的键**，而**锁变量的值**，则是键值对的值，这样一来，Redis 就能保存锁变量了，客户端也就可以通过 Redis 的命令操作来实现锁操作。

* ![img](https://static001.geekbang.org/resource/image/1d/45/1d18742c1e5fc88835ec27f1becfc145.jpg?wh=2820*2250)

* Redis 如何保证这些操作是原子的？

* > * Redis 可以用哪些**单命令操作**实现加锁操作? SETNX;
  >
  > * SETNX 命令，它用于设置键值对的值。具体来说，就是这个**命令在执行时会判断键值对是否存在**，如果不存在，就设置键值对的值，如果存在，就不做任何设置。
  >
  > * 我们就可以用 SETNX 和 DEL 命令组合来实现加锁和释放锁操作。
  >
  > * ```cpp
  >   // 加锁
  >   SETNX lock_key 1
  >   // 业务逻辑
  >   DO THINGS
  >   // 释放锁
  >   DEL lock_key
  >   ```
  >
  > * SETNX  & Del 可能存在的问题？
  >
  > * > * 第一个风险是，假如某个客户端在执行了 SETNX 命令、加锁之后，紧接着却在操作共享数据时发生了异常，结果一直没有执行最后的 DEL 命令释放锁(这样锁就一直不会释放,其他客户端就不会拿到锁，无法访问共享的数据和执行后序的操作)
  >   >
  >   > * 解决方案： 给锁变量设置一个过期时间，缺陷： 不好控制过期时间的长短，过长导致长时间不释放；过短导致提前释放。
  >   >
  >   > * 对于这个问题目前主流的做法是**每获得一个锁时**，只**设置一个很短的超时时间**，同时**起一个线程在每次快要到超时时间时去刷新锁的超时时间**。在释放锁的同时结束这个线程。如redis官方的分布式锁组件redisson,就是用的这种方案。
  >   >
  >   > * 第二个风险。如果客户端 A 执行了 SETNX 命令加锁后，假设客户端 B 执行了 DEL 命令释放锁（B 没有锁但是还是可以执行 DEL 命令），此时，客户端 A 的锁就被误释放了。如果客户端 C 正好也在申请加锁，就可以成功获得锁，进而开始操作共享数据。这样一来，客户端 A 和 C 同时在对共享数据进行操作，数据就会被修改错误，这也是业务层不能接受的。
  >   >
  >   > * 解决方案： **我们需要能区分来自不同客户端的锁操作， 只有自己才能释放自己加的锁**。
  >   >
  >   > *  **自己只能释放自己加的锁，不允许释放别人加的锁！**
  >   >
  >   > * 可以让每个**客户端给锁变量设置一个唯一值**，这里的唯一值就可以用来标识当前操作的客户端。
  >   >
  >   > * **我们使用 SET 命令带上 NX 选项来实现加锁；**
  >   >
  >   > * ```cpp
  >   >   // 加锁, unique_value作为客户端唯一性的标识
  >   >   SET lock_key unique_value NX PX 10000
  >   >   ```
  >   >
  >   > * 因为在加锁操作中，每个客户端都使用了一个唯一标识，所以在释放锁操作时，**我们需要判断锁变量的值，是否等于执行释放锁操作的客户端的唯一标识**
  >   >
  >   > * ```lua
  >   >   //释放锁 比较unique_value是否相等，避免误释放
  >   >   // KEYS[1]表示 lock_key，ARGV[1]是当前客户端的唯一标识
  >   >   if redis.call("get",KEYS[1]) == ARGV[1] then  // 都是在服务端执行
  >   >       return redis.call("del",KEYS[1])
  >   >   else
  >   >       return 0
  >   >   end
  >   >   ```
  >   >   
  >   > * 正确的释放锁姿势——**锁的判断和删除都在服务端（Redis）**，**使用 lua 脚本保证原子性**：
  >   >
  >   > * 第三个风险： **锁超时问题**：
  >   >
  >   > * 客户端 1 请求锁成功了，但是由于业务处理、GC、操作系统等原因导致它处理时间过长，超过了锁的时间，这时候 Redis 会自动释放锁，这种情况可能导致问题：
  >   >
  >   > * ![img](https://pic4.zhimg.com/80/v2-5e6e7a7cb9a948aa4048bd01d4d53e7b_720w.jpg)
  >   >
  >   > * 如何解决这种问题？—— **续期** ，Java 里我们可以使用 TimerTask 类来实现自动续期的功能，伪代码如下：
  >   >
  >   > * ```java
  >   >   Timer timer = new Timer();
  >   >   ```
  >   >
  >   > * 这个机制在 Redisson 框架中已经实现，而且还有一个比较霸气的名字 watchdog（看门狗）：**加锁时没有指定加锁时间时会启用 watchdog 机制，默认加锁 30 秒，每 10 秒钟检查一次，如果存在就重新设置 过期时间为 30 秒（即 30 秒之后它就不再续期了）**
  >   >
  >   > * **![img](https://pic1.zhimg.com/80/v2-b23b757379417d76d4ce377f76105104_720w.jpg)**
  >   >
  >   > * 如果在执行计算期间发现锁快要超时了，**客户端可以给redis服务实例发送一个Lua脚本让redis服务端延长锁的时**
  >   >
  >   > * **启动另外一个线程**去检查的问题，这个key是否超时，在某个时间还没释放。
  >   >
  >   > * ```c
  >   >   if redis.call("get",KEYS[1]) == ARGV[1] then
  >   >             redis.call("set",KEYS[1],ex=3000)  
  >   >   else
  >   >             getDLock();//重新获取锁 
  >   >   ```
  >   >
  >   > * 
  
* 单 Redis 需要满足的三个条件：

* > * 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 **SET 命令带上 NX 选项来实现加锁**；
  > * 锁变量需要**设置过期时间**，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间；
  > * **锁变量的值需要能区分来自不同客户端的加锁操作**，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令**设置锁变量值时**，每个客户端设置的值是一个唯一值，用于标识客户端。





##### 多 Redis 实现分布式锁

* 如果单个 Redis 实例发生故障宕机了，那么锁变量就没有了。所以这个时候就需要引入 Redis 集群的概念了。
* 为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了**分布式锁算法 Redlock**。（不推荐）
* 基本思想： （多数原则），如果客户端能够和**半数以上的实例成功地完成加锁操作**，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。(和  raft 有点类似)

###### Redlock 算法

* 第一步是，客户端获取当前时间。

* 第二步是，客户端按顺序依次向 N 个 Redis 实例执行加锁操作。

* > * Redlock 算法能够继续运行，我们需要给加锁操作设置一个超时时间。

* 第三步是，一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。

* 客户端只有在满足下面的这两个条件时，才能认为是加锁成功。

* > * 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 实例上成功获取到了锁；
  > * 条件二：客户端获取锁的总耗时没有超过锁的有效时间。

* 在 Redlock 算法中，释放锁的操作和在**单实例上释放锁的操作一样**，只要执行释放锁的 Lua 脚本就可以了。



Q : 是否可以使用 SETNX + EXPIRE 来完成加锁操作？

A : 不可以这么使用。使用 2 个命令无法保证操作的原子性，在异常情况下，加锁结果会不符合预期。

> * 1、SETNX 执行成功，执行 EXPIRE 时由于网络问题设置过期失败
> * 2、SETNX 执行成功，此时 Redis 实例宕机，EXPIRE 没有机会执行 
> * 3、SETNX 执行成功，客户端异常崩溃，EXPIRE 没有机会执行
> * 如果发生以上情况，并且客户端在释放锁时发生异常，没有正常释放锁，那么**这把锁就会一直无法释放，其他线程都无法再获得锁。**



###### 使用场景

* 如果业务场景简单，并发量也不是很高的话，可以采用 Redis实现分布式锁
* 如果对锁数据的可靠性要求非常的高，可以使用 etcd 这种通过一致性保证数据可靠性的锁的方案,但是可靠性的背后是较低的吞吐量和较高的延迟.



##### 可重入锁

* 所谓不可重入锁，即若**当前线程执行某个方法已经获取了该锁，那么在方法中尝试再次获取锁时，就会获取不到被阻塞**。 同一个人拿一个锁 ，只能拿一次不能同时拿2次。
* 可重入锁，也叫做递归锁，指的是在同一线程内，外层函数获得锁之后，内层递归函数仍然可以获取到该锁。 说白了就是同一个线程再次进入同样代码时，可以再次拿到该锁。 它的作用是：**防止在同一线程中多次获取锁而导致死锁发生。**
* Redis 如何实现可重入锁的： 看ReentrantLock的源码我们知道，它是加锁成功了，记录了当前持有锁的线程，并通过一个int类型的数字，来记录了加锁次数。
* 







#### 缓存雪崩、击穿、穿透

![图片](https://img-blog.csdnimg.cn/img_convert/61781cd6d82e4a0cc5f7521333049f0d.png)



##### 缓存雪崩

* 通常我们为了保证缓存中的数据与数据库中的数据一致性，会给 Redis 里的数据设置过期时间，当缓存数据过期后，用户访问的数据如果不在缓存里，业务系统需要重新生成缓存，因此就会访问数据库，并将数据更新到 Redis 里，这样后续请求都可以直接命中缓存。

* ![图片](https://img-blog.csdnimg.cn/img_convert/e2b8d2eb5536aa71664772457792ec40.png)

* **大量缓存数据在同一时间过期（失效）或者 Redis 故障宕机**时，如果此时有大量的用户请求，都无法在 Redis 中处理，于是全部请求都直接访问数据库，从而导致数据库的压力骤增，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃，这就是**缓存雪崩**的问题。

* ![图片](https://img-blog.csdnimg.cn/img_convert/717343a0da7a1b05edab1d1cdf8f28e5.png)

* 发生缓存雪崩有两个原因：

  - 大量数据同时过期；
  - Redis 故障宕机；

* 解决方案：

* > * **均匀设置过期时间** : **给这些数据的过期时间加上一个随机数**
  > * **互斥锁** : **如果发现访问的数据不在 Redis 里，就加个互斥锁，保证同一时间内只有一个请求来构建缓存**
  > * **双 key 策略**: 一个是**主 key，会设置过期时间**，一个是**备 key，不会设置过期** , 当业务线程访问不到「主 key 」的缓存数据时，就直接返回「备 key 」的缓存数据，**然后在更新缓存的时候**(还是需要更新缓存)，**同时更新「主 key 」和「备 key 」的数据。**
  > * **后台更新缓存**:**让缓存“永久有效”，并将更新缓存的工作交由后台线程定时更新**。缓存数据不设置有效期，并不是意味着数据一直能在内存里，因为**当系统内存紧张的时候，有些缓存数据会被“淘汰”**.(数据丢失了)
  > * 在业务线程发现缓存数据失效后（缓存数据被淘汰），**通过消息队列发送一条消息通知后台线程更新缓存**（抖音项目使用的方式）
  > * **缓存预热**： 在业务刚上线的时候，我们最好**提前把数据缓起来**，而不是等待用户访问才来触发缓存构建.

* Redis 故障宕机：

* > - 服务熔断或请求限流机制: **暂停业务应用对缓存服务的访问，直接返回错误**(保护下层的数据库)
  > - 熔断的缺点： 为了减少对业务的影响，我们可以启用**请求限流**机制，**只将少部分请求发送到数据库进行处理，再多的请求就在入口直接拒绝服务**。等到 Redis 恢复正常并把缓存预热完后，再解除请求限流的机制。
  > - 构建 Redis 缓存高可靠集群: **主从节点的方式构建 Redis 缓存高可靠集群**。（当某个节点崩了，可以切换从节点为主节点，继续提供缓存服务）





##### 缓存击穿

* 定义： 如果缓存中的**某个热点数据过期**了，此时大量的请求访问了该热点数据（比如秒杀活动），就无法从缓存中读取，直接访问数据库(大量线程访问数据库更新缓存数据），数据库很容易就被高并发的请求冲垮

* ![图片](https://img-blog.csdnimg.cn/img_convert/acb5f4e7ef24a524a53c39eb016f63d4.png)

* 解决方案：

* > * 互斥锁方案，保证同一时间只有一个业务线程更新缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。
  > * 缓存永久有效： 不给热点数据设置过期时间，由后台异步更新缓存，
  > * 在**热点数据准备要过期前**，提前通知后台线程更新缓存以及重新设置过期时间；



##### 缓存穿透

当发生缓存雪崩或击穿时，数据库中还是保存了应用要访问的数据，一旦**缓存恢复相对应的数据，就可以减轻数据库的压力**，而缓存穿透就不一样了

* **在 4 核 8G 的机器上运行 MySQL 5.7 时，大概可以支撑 500 的 TPS 和 10000 的 QPS。**

* **数据库对于并发的承受能力是比较脆弱的**。一旦数据库承受不了用户大量刷新商品页面、定向搜索衣服信息，查询就会变慢，大量的请求也会阻塞在数据库查询上，**造成应用服务器的连接和线程资源被占满，最终导致你的电商系统崩溃**。
* 我们的**核心缓存的命中率要保持在 99% 以上**，**非核心缓存的命中率也要尽量保证在 90%**，如果低于这个标准你可能就需要优化缓存的使用方式了。

* 定义：当用户访问的数据，**既不在缓存中，也不在数据库中**，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求。那么当有大量这样的请求到来时，数据库的压力骤增。

* ![图片](https://img-blog.csdnimg.cn/img_convert/b7031182f770a7a5b3c82eaf749f53b0.png)

* 产生的原因：

* > * 业务误操作，缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据；
  > * 黑客恶意攻击，故意大量访问某些读取不存在数据的业务；

* 解决方案：

* > * 第一种方案，非法请求的限制.( **API 入口**处我们要判断求请求参数是否合理, 如果判断出是恶意请求就直接返回错误)
  >
  > * 第二种方案，缓存空值或者默认值；(线上业务发现缓存穿透的现象时，可以针对查询的数据，在**缓存中设置一个空值或者默认值**,避免访问数据库)
  >
  > * 回空值的问题： 如果有**大量获取未注册用户信息的请求**，缓存内就会有有大量的空值缓存，也就会**浪费缓存的存储空间**，如果缓存空间被占满了，还会**剔除掉一些已经被缓存的用户信息反而会造成缓存命中率的下降**。
  >
  > * 我建议你在使用的时候应该**评估一下缓存容量是否能够支撑**。
  >
  > * 第三种方案，使用**布隆过滤器快速判断数据是否存在**，避免通过查询数据库来判断数据是否存在；(保护好数据库才是关键)
  >
  > * 大量请求只会查询 Redis 和布隆过滤器，而不会查询数据库!
  >
  > * 布隆过滤器缺点：
  >
  > * > * 它在判断元素是否在集合中时是有一定错误几率的，比如它会把不是集合中的元素判断为处在集合中；
  >   > * 不支持删除元素。
  >
  > * 布隆过滤器的其他使用场景： 布隆过滤器可以用在一些资讯app的新闻展示中，**给用户推送新的资讯用来过滤掉那些用户已经浏览过的记录**







![图片](https://img-blog.csdnimg.cn/img_convert/061e2c04e0ebca3425dd75dd035b6b7b.png)



#### 数据库和缓存如何保持一致性

* **主要解决写的问题(更新的时候存在不一致性问题），读不存在不一致的问题**

##### **先更新数据库，再删缓存**

![图片](https://img-blog.csdnimg.cn/img_convert/1cc7401143e79383ead96582ac11b615.png)



* **但是在实际中，这个问题出现的概率并不高**。**因为缓存的写入通常要远远快于数据库的写入**，所以在实际中很难出现请求 B 已经更新了数据库并且删除了缓存，请求 A 才更新完缓存的情况。
* **「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的**。
* 缺点：每次更新数据库的时候，都需要将缓存的数据删除，这样会对缓存的命中率带来影响.

##### 更新数据库 + 更新缓存

* **如果我们的业务对缓存命中率有很高的要求，我们可以采用「更新数据库 + 更新缓存」的方案，因为更新缓存并不会出现缓存未命中的情况**。

* ![图片](https://img-blog.csdnimg.cn/img_convert/8febac10b14bed16cb96d1d944cd08da.png)

* 数据库中的数据是 2，而缓存中的数据却是 1，**出现了缓存和数据库中的数据不一致的现象**。

* 

* 解决方案：

* > * 在更新缓存前先加个**分布式锁**，保证同一时间只运行一个请求更新缓存，就会不会产生并发问题了，当然引入了锁后，对于写入的性能就会带来影响。
  > * 在更新完缓存时，给缓存加上较短的**过期时间**，这样即时出现缓存不一致的情况，缓存的数据也会很快过期(这样我们会再次取数据库中查找数据，更新数据），对业务还是能接受的。

  

##### 先更新数据库再删除缓存的问题：

* **删除缓存（第二个操作）的时候失败了**，导致缓存还是旧值，而数据库是最新值，造成数据库和缓存数据不一致的问题，会对敏感业务造成影响。

* 解决方案：延迟双删

* > * 重试机制: 引入**消息队列**，将第二个操作（删除缓存）要操作的数据加入到消息队列，由消费者来操作数据,删除失败，执行重试机制，如果删除成功就从消息队列 中移除，避免重复操作.（如何判断删除失败，消费者执行操作 return false 则删除失败）
  > * ![图片](https://img-blog.csdnimg.cn/img_convert/a4440f0d572612e0832b903e4a62bd2b.png)
  > * 订阅 MySQL binlog，再操作缓存。



#### Redis 持久化

##### AOF 持久化

* 定义： 保存写操作命令到日志的持久化方式，就是 Redis 里的 **AOF(\*Append Only File\*)** 持久化功能，**注意只会记录写操作命令，读操作命令是不会被记录的**

  

* ![img](https://img-blog.csdnimg.cn/img_convert/6f0ab40396b7fc2c15e6f4487d3a0ad7.png)

* 执行步骤：

* > * 先执行写操作命令后，才将该命令记录到 AOF 日志里的
  >
  > * > * 第一个好处，**避免额外的检查开销。**(额外的检查操作，因为需要检查是否合法，才将其保存到 aof 文件中，不检查可能出现语法错误，下次恢复的时候出错)。而如果先执行写操作命令再记录日志的话，**只有在该命令执行成功后，才将命令记录到 AOF 日志**。
  >   > * 第二个好处，**不会阻塞当前写操作命令的执行**,因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。
  >
  > * 潜在的风险：
  >
  > * > * 第一个风险，执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有**丢失的风险**。(这个问题我也很困惑)
  >   > * 第二个风险，前面说道，由于写操作命令(主线程完成的)执行成功后才记录到 AOF 日志，所以不会阻塞当前写操作命令的执行，但是**可能会给「下一个」命令带来阻塞风险**。
  >   > * ![img](https://img-blog.csdnimg.cn/img_convert/28afd536c57a46447ddab0a2062abe84.png)
  >   > * AOF 日志写回磁盘（可以另外开一个线程）的时机非常的重要.

###### 写回策略

* ![img](https://img-blog.csdnimg.cn/img_convert/4eeef4dd1bedd2ffe0b84d4eaa0dbdea.png)

* > 1. **Redis 执行完写操作命令后，会将命令追加到 `server.aof_buf` 缓冲区**；
  > 2. 然后通过 write() 系统调用，将 aof_buf 缓冲区的数据写入到 AOF 文件，此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区 page cache，等待内核将数据写入硬盘；(**以上这两步都是 Redis 单线程执行的**，就是写入到内核缓存也是 redis 主线程执行的)
  > 3. 具体内核缓冲区的数据什么时候写入到硬盘，由内核决定。

* **「主进程阻塞」**和「**减少数据丢失**」的问题，因为两个问题是对立的，偏向于一边的话，就会要牺牲另外一边

* Always： (每一次都立刻写入磁盘，阻塞进程，但是减少数据丢失）**高可靠** **(always 会阻塞主线程执行**,处理命令和刷盘是同步执行的)

* 此模式下**同步操作是由 Redis 主进程执行的，所以在同步执行期间，主进程会被阻塞，不能接受命令请求**。

* Everysec ： 折中方案,**避免了 Always 策略的性能开销，也比 No 策略更能避免数据丢失**（Everysec 策略就会创建一个**异步任务来执行 fsync() 函数，redis创建一个子线程进行异步任务**）

* ```cpp
  // 后台异步线程创建
  void bioInit(void) {
      ...
      for (j = 0; j < BIO_NUM_OPS; j++) {
          void *arg = (void*)(unsigned long) j;
          // 创建线程
          if (pthread_create(&thread,&attr,bioProcessBackgroundJobs,arg) != 0) {
              serverLog(LL_WARNING,"Fatal: Can't initialize Background Jobs.");
              exit(1);
          }
          bio_threads[j] = thread;
      }
  }
  ```

* No： (交给操作系统执行，由操作系统决定什么时候写入磁盘，在内核中的 page cache )：性能好一点，但是数据丢失概率大 **高性能**

* 实现原理： 控制 `fsync()` 函数的调用时机！

* ![img](https://img-blog.csdnimg.cn/img_convert/f64829ffc2e9e006b090f9aae51035ee.png)

* 如果想要应用程序向文件写入数据后，能**立马将数据同步到硬盘，就可以调用 `fsync()` 函数**，这样**内核就会将内核缓冲区的数据直接写入到硬盘**，等到硬盘写操作完成后，该函数才会返回。

* > - Always 策略就是每次写入 AOF 文件数据后，就执行 fsync() 函数；
  > - Everysec 策略就会创建一个异步任务来执行 fsync() 函数；
  > - No 策略就是永不执行 fsync() 函数;



###### AOF 重写机制

* 定义： Redis 为了避免 AOF 文件越写越大，提供了 **AOF 重写机制**，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。（类似 snapshot 快照技术）
* ![img](https://img-blog.csdnimg.cn/img_convert/723d6c580c05400b3841bc69566dd61b.png)
* 重写 AOF 的时候，不直接复用现有的 AOF 文件，而是先写到新的 AOF 文件再覆盖过去。原因是： **如果 AOF 重写过程中失败了，现有的 AOF 文件就会造成污染**，可能无法用于恢复使用。



###### AOF 后台重写

* 写入 AOF **日志**的操作虽然是在主进程完成的，因为它写入的内容不多，所以一般不太影响命令的操作。

* Redis 的**重写 AOF 过程是由后台子进程 \* bgrewriteaof \* 来完成的**(不是主进程完成的)

* 优势：

* > * 子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程
  > * 子进程带有主进程的**数据副本**（copy on write）,父子进程是共享内存数据的，不过这个共享的内存只能以**只读的方式**，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」(**在发生写操作的时候，操作系统才会去复制物理内存**)，于是父子进程就有了独立的数据副本，就**不用加锁来保证数据安全。**,使用线程数据共享，需要加锁解决这个问题,开销比较大
  > * 发生写时复制，这时候被修改的物理内存就会复制多一份，然后**修改自己对应的物理内存**，**发生写时复制后的物理内存并不是共享的**

* 问题： 重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？

* Redis 设置了一个 **AOF 重写缓冲区**，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会**同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」**。

* 下图可以发现**写时复制只会重新复制需要修改的页**，不需要修改的页不发生复制，还可以共享,当前**被修改的数据的物理内存**就会被复制一份。

* 那么极端情况下，**如果所有的共享内存都被修改，则此时的内存占用是原先的 2 倍。**

* ![在这里插入图片描述](https://img-blog.csdnimg.cn/202105270918298.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70#pic_center)

* 在 bgrewriteaof 子进程执行 AOF 重写期间，**主进程需要执行以下三个工作**:（注意下面三个任务是主进程执行的）

* > * 执行客户端发来的命令；
  > * 将执行后的写命令追加到 「AOF 缓冲区」；
  > * 将执行后的写命令追加到 「AOF 重写缓冲区」；

* **子进程完成 AOF 重写工作**（*扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志*）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。

* 主进程需要做以下两个事情：(主进程阻塞了)

* > * 将 **AOF 重写缓冲区**中的所有内容追加到新的 AOF 的文件中
  > * 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件
  > * AOF 的落盘操作是由：主线程执行的（当然和写回策略相关）

* 整个 AOF 后台重写过程中，除了发生 1.**写时复制会对主进程造成阻塞**，还有 2. **信号处理函数执行时也会对主进程造成阻塞**，在其他时候，**AOF 后台重写都不会阻塞主进程**。(子线程执行这些操作)



###### AOF 不足

* 用 AOF 日志的方式来恢复数据其实是很慢的，因为 **Redis 执行命令由单线程负责的**，而 **AOF 日志恢复数据的方式是顺序执行日志里的每一条命令**，如果 AOF 日志很大，这个「重放」的过程就会很慢了。







##### RDB 快照持久化(默认)

- AOF 文件的内容是操作命令； AOF 文件记录的是**命令操作的日志**(eg set x = 1 这种命令， 而不是获取最终 x 的值)，而不是实际的数据

- RDB 文件的内容是二进制数据。RDB 快照就是记录某一个瞬间的内存数据，记录的是**实际数据**

- **RDB 恢复数据的效率会比 AOF 高**些，因为**直接将 RDB 文件读入内存就可以**，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。

- Redis 的RDB 快照是**全量快照**，也就是说每次执行快照，都是把内存中的「**所有数据**」都记录到磁盘中。

-  Redis 在使用 bgsave 快照过程中，如果主线程修改了内存数据，不管是否是共享的内存数据，**RDB 快照都无法写入主线程刚修改的数据**，因为此时主线程（父进程）的内存数据和子进程的内存数据已经分离了，子**进程写入到 RDB 文件的内存数据只能是原本的内存数据**。

- 刚刚修改的数据**只能在下一次 RDB 快照的时候更新写入**.

- 尽管 RDB 比 AOF 的数据**恢复速度快**，但是快照的频率不好把握 (至少 5 分钟才保存一次快照)：

- > * 如果频率太低，两次快照间一旦服务器发生宕机，就可能会比较多的数据丢失；
  > * 如果频率太高，**频繁写入磁盘和创建子进程**会带来额外的性能开销。(全量快照)

- Q : 那有没有什么方法不仅有 RDB 恢复速度快的优点，又有 AOF 丢失数据少的优点呢？

- A: 将 RDB 和 AOF 合体使用,**混合使用 AOF 日志和内存快照**，也叫混合持久化。

- 什么是混合持久化？

  * 混合持久化工作在 **AOF 日志重写过程**。 （不是 RDB 快照持久化过程）

  * 当开启了混合持久化时，在 AOF 重写日志时.

  * 1. `fork` 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，

  * 2. 然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的**增量命令**会以 AOF 方式写入到 AOF 文件.
    3. 写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。

  * AOF 文件的**前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据**。

  * 优点：

  * > * 重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样**加载的时候速度会很快**。
    > * 加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得**数据更少的丢失**。
    > * ![图片](https://img-blog.csdnimg.cn/img_convert/f67379b60d151262753fec3b817b8617.png)

##### 为什么RDB 使用子进程而不是子线程

* 主要是出于Redis性能的考虑
* (1)Redis RDB持久化机制会阻塞主进程，这样主进程就无法响应客户端请求。
* (2)我们知道Redis对客户端响应请求的工作模型是单进程和单线程的，如果在主进程内启动一个线程，这样会造成对数据的竞争条件，为了避免使用锁降低性能。



#### 主从复制如何实现

* 为什么需要主从复制：

* > - 如果服务器发生了宕机，由于数据恢复是需要点时间，那么这个期间是无法服务新的请求的；
  > - 如果这台服务器的硬盘出现了故障，可能数据就都丢失了。

* 问题： 如何保证多台服务器之间数据的一致性?

* Redis 提供了**主从复制模式**(**异步**进行同步的，不满足一致性），来避免上述的问题。（当客户端在redis的主节点修改了数据后**立即返回，即使在主从网络断开的情况下，主节点依旧可以正常对外提供修改服务，所以redis满足可用性。）

* redis保证**最终一致性**，从节点会努力追赶主节点，最终从节点的状态会和主节点的状态保持一致。

* 主从服务器之间采用的是**「读写分离」**的方式。**主服务器可以进行读写操作**，当**发生写操作时自动将写操作同步给从服务器**，而**从服务器一般是只读**，并接受主服务器同步过来写操作命令，然后执行这条命令。

* ![img](https://static001.geekbang.org/resource/image/80/2f/809d6707404731f7e493b832aa573a2f.jpg?wh=3214*2119)

* 主从复制的三种模式：

##### 第一次同步

* ![img](https://static001.geekbang.org/resource/image/63/a1/63d18fd41efc9635e7e9105ce1c33da1.jpg?wh=3765*1676)

* > * 第一阶段是建立链接、协商同步；(主服务器 runID, 复制进度 offset) , FULLRESYNC 响应命令的意图是采用**全量复制**的方式
  >
  > * 第一阶段的工作时为了全量复制做准备。下一阶段执行
  >
  > * 第二阶段是主服务器同步数据给从服务器；
  >
  > * 下面三个期间的写操作命令并没有记录到刚刚生成的 RDB 文件中，这时主从服务器间的数据就不一致了。那么为了保证主从服务器的数据一致性，**主服务器在下面这三个时间间隙中将收到的写操作命令，写入到 replication buffer 缓冲区里。**
  >
  > * > - 主服务器生成 RDB 文件期间；
  >   > - 主服务器发送 RDB 文件给从服务器期间；
  >   > - 「从服务器」加载 RDB 文件期间；
  >
  > * 第三阶段是主服务器发送**新写操作命令给从服务器**。
  >
  > * **主服务器生成的 RDB 文件发送完，从服务器加载完 RDB 文件后**，**然后将 replication buffer 缓冲区里所记录的写操作命令发送给从服务器，**然后「从服务器」重新执行这些操作，至此主从服务器的数据就一致了。



##### 命令传播

* 后续主服务器可以通过这个连接继续将**写操作命令传播给从服务器**(是写的具体命令，而不是最终的数据），然后从服务器执行该命令，使得与主服务器的数据库状态相同。
* 这个 TCP 连接是**长连接**，目的是避免频繁的 **TCP 连接和断开带来的性能开销**。



##### 分摊主服务器压力

* 主服务器主要压力： **生成 RDB 文件和传输 RDB 文件**。
* 生成 RDB 文件：通过 bgsave 命令来生成 RDB 文件的，那么主服务器就会忙于使用 fork() 创建子进程，如果主服务器的内存数据非大，在执行 **fork() 函数时是会阻塞主线程的**， 这时Redis 无法处理请求。
* 传输 RDB 文件,**占用主服务器的网络带宽**，会对主服务器响应命令请求产生影响.
* ![img](https://static001.geekbang.org/resource/image/40/45/403c2ab725dca8d44439f8994959af45.jpg?wh=3543*1791)
* 通过这种方式，**主服务器生成 RDB 和传输 RDB 的压力可以分摊到充当经理角色的从服务器**。会把**数据同步给自己旗下的从服务器**，从而减轻主服务器的负担。





##### 增量复制

* 如果主从服务器在命令同步时出现了网络断开又恢复的情况，从服务器就会和主服务器重新进行一次全量复制，很明显这样的开销太大了，必须要改进一波，使用增量复制,**只会把网络断开期间主服务器接收到的写操作命令，同步给从服务器。**

* **主服务器怎么知道要将哪些增量数据发送给从服务器呢？**

* > - **repl_backlog_buffer**，是一个「**环形**」缓冲区，用于主从服务器断连后，从中找到差异的数据；
  > - **replication offset**，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用 master_repl_offset 来记录自己「*写*」到的位置，从服务器使用 slave_repl_offset 来记录自己「*读*」到的位置。

* Q : repl_backlog_buffer 缓冲区是什么时候写入的呢？

* A : 在主服务器进行命令传播时，不仅会将写命令发送给**从服务器**，还会将写命令**写入到 repl_backlog_buffer 缓冲区**里

* 从服务器会通过 **psync 命令**将自己的复制偏移量 **slave_repl_offset 发送给主服务器**.主服务器根据自己的 **master_repl_offset 和 slave_repl_offset 之间的差距**，然后来决定对从服务器执行哪种同步操作.

* repl_backlog_buffer 中存在数据，那么主服务器采用增量同步的方式，没有的话采用全量同步的方式。

* 那么在网络恢复时，如果从服务器想读的数据**已经被覆盖了**，主服务器就会采用**全量同步**，这个方式比增量同步的性能损耗要大很多。（所以需要调整 repl_backlog_buffer 缓冲区的大小）

* ![img](https://static001.geekbang.org/resource/image/13/37/13f26570a1b90549e6171ea24554b737.jpg?wh=4000*1065)

* 上图： 主库和从库之间相差了 put d e 和 put d f 两个操作，在增量复制时，主库只需要把它们同步给从库，就行了。

##### Question

* Q: 如何判断一个 redis 节点是否正常工作？

* A: redis 判断接点是否正常工作，基本都是通过互相的 ping-pong 心态检测机制，如果**有一半以上的节点**去 ping 一个节点的时候没有 pong 回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接。

* Q：主从架构中， redis 如何同步过期的 key?

* A : 主节点处理了一个key或者通过淘汰算法淘汰了一个key，这个时间**主节点模拟一条del命令发送给从节点**，从节点收到该命令后，就进行删除key的操作.

* Q : 集群产生脑裂怎么办？

* > * A : 哨兵因为之前已经选举出一个新主节点了，它就会把旧主节点降级为从节点（A），然后从节点（A）会向新主节点请求数据同步，**因为第一次同步是全量同步的方式，此时的从节点（A）会清空掉自己本地的数据，然后再做全量同步。所以，之前客户端在过程 A 写入的数据就会丢失了，也就是集群产生脑裂数据丢失的问题**。
  >
  > * 由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。
  >
  > * redis 中设置参数限制出现脑裂数据丢失。
  >
  > * > - min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写数据。
  >   > - min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果超过，主节点会禁止写数据。
  >
  > * **原主库就会被限制接收客户端写请求，客户端也就不能在原主库中写入新数据了**。
  >
  >   **等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。我再来给你举个例子。**

* Q : Redis 主节点挂了如何处理？

* A:  主节点挂了 ，从节点是无法自动升级为主节点的，**这个过程需要人工处理**，在此期间 redis 无法对外提供写操作。

* A：此时，redis 哨兵机制就登场了，**哨兵在发现主节点出现故障时，由哨兵自动完成故障发现和故障转移**，并通知给应用方，从而实现高可用性。



#### 为什么单线程 Redis 那么快

* 在内存中执行
* 高效的数据结构
* 单线程避免线程切换的损耗（采用单线程的一个核心原因是避免多线程开发的并发控制问题）
* 采用了IO多路复用技术，可以高效的处理多个客户端的请求。
* **内存操作+高效的数据结构(hash 表和跳表)+多路复用机制=redis单线程的高性能**

Redis 单线程的问题：

* 请求太耗时，业务阻塞
* 无法发挥多核优势 （Redis 6.0 支持多线程）

Redis 单线程优势

* 一方面单线程的程序可以避免线程上下文切换带来的性能损耗
* 单线程程序不存在临界资源，不会出现锁以及并发竞争的情况(数据竞争问题)
* 

##### Redis 单线程定义

* Redis 是单线程，主要是指 **Redis 的网络 IO 和键值对读写是由一个线程来完成的**(redis服务器本身是个多线程的，有发布--订阅、缓存、数据库等功能。每个功能都需要相应的线程去完成，但是它读写数据是单线程的)，这也是 Redis 对外提供键值存储服务的主要流程。**但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。**



##### IO 多路复用技术

![img](https://static001.geekbang.org/resource/image/e1/c9/e18499ab244e4428a0e60b4da6575bc9.jpg?wh=2700*1493)



* 在这里的网络 IO 操作中，有潜在的阻塞点，分别是 accept() 和 recv()。当 Redis 监听到一个客户端有连接请求，但一直未能成功建立起连接时，会阻塞在 accept() 函数这里，导致其他客户端无法和 Redis 建立连接。类似的，当 Redis 通过 recv() 从一个客户端读取数据时，如果数据一直没有到达，Redis 也会一直阻塞在 recv()。
* **导致 Redis 整个线程阻塞，无法处理其他客户端请求**，效率很低。不过，幸运的是，socket 网络模型本身支持非阻塞模式。
* ![img](https://static001.geekbang.org/resource/image/1c/4a/1ccc62ab3eb2a63c4965027b4248f34a.jpg?wh=3744*1077)
* 针对监听套接字，我们可以设置非阻塞模式：当 Redis 调用 accept() 但一直未有连接请求到达时，Redis 线程可以返回处理其他操作，而不用一直等待。但是，你要注意的是，**调用 accept() 时，已经存在监听套接字了。**
* 这里的redis线程可以不用等待，返回处理：其实是指的**redis不会被阻塞在一个连接上**，可以**处理其他连接**，当然实际是轮询的事件队列。
* 这样才能保证 Redis 线程，既不会像**基本 IO 模型中一直在阻塞点等待**，也**不会导致 Redis 无法处理实际到达的连接请求或数据**.
* io多路复用：**由内核监听多个套接字，一旦套接字上有连接或者数据请求到来，内核将请求放入事件队列中交给redis单线程处理。**redis单线程**扫描这些事件**，通过调用事件回调函数进行处理。
* ![img](https://static001.geekbang.org/resource/image/00/ea/00ff790d4f6225aaeeebba34a71d8bea.jpg?wh=3472*2250)
* Redis 网络框架调用 **epoll 机制，让内核监听这些套接字**。此时，Redis 线程不会阻塞在某一个特定的监听或已连接套接字上.
* **把监听转到了内核去做**，有了事件再交给 Redis 主线程。



###### Redis 单线程缺陷

* Redis单线程处理IO请求性能瓶颈主要包括2个方面： 

* > 1. 任意**一个请求在server中一旦发生耗时，都会影响整个server的性能**，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。耗时的操作包括以下几种： 
  >
  >    > * a、**操作bigkey(value 值）**：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时； 
  >    > * b、使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据； 
  >    > * c、大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长； 
  >    > * d、淘汰策略：**淘汰策略也是在主线程执行的**，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长； 
  >    > * e、AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能； 
  >    > * f、主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但**fork这一瞬间也是会阻塞整个线程的**，实例越大，阻塞时间越久； 
  >
  > 2. 并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是**读写客户端数据**依旧是**同步IO**，**只能单线程依次读取客户端的数据，无法利用到CPU多核**。
  >
  > 3.  针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。 
  >
  > 4. 针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的。（内核空间跟外部介质（比如网卡或磁盘）之间的数据拷贝用了多路复用，但是**内核空间与用户空间之间的数据拷贝以及在用户空间内部的的数据处理是单线程**的。）

* 总结： 关键点在于accpet和recv时可能会阻塞线程，使用IO多路复用技术可以让线**程先处理其他事情，等需要的资源到位后epoll会调用回调函数通知线程**，然后线程再去处理存/取数据；这样**一个redis服务端线程就可以同时处理多个客户端请求了**。
* Redis 事件队列： 事件队列只有一个instance，队列都是**按照先进先出的原则**进栈出栈。所以如果**某个事件处理很耗时，后面的事件没有办法处理导致性能瓶颈。**
* Q: Redis读取客户端数据和读内存是一个线程？
* A: Redis 6.0前的版本是用一个线程来读取网络请求（网络IO）并进行解析，并根据请求的具体命令操作进行数据读写的。网络IO是对socket的读和写，数据读写我理解应该使用command操作内存中的数据(key-value 数据对)





#### Redis 为什么会慢

##### 原因

[Redis 单线程缺陷](#Redis 单线程缺陷)





##### 如何解决 bigkey 问题？















#### 哨兵模式

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E5%93%A8%E5%85%B5/%E5%93%A8%E5%85%B5%E6%8F%90%E7%BA%B2.png)



##### 为什么需要哨兵机制

* 在 Redis 的主从架构中，由于**主从模式是读写分离的**，如果**主节点（master）挂了**，那么将没有主节点来服务客户端的写操作请求，也没有主节点给从节点（slave）进行数据同步了。
* ![主节点挂了](https://img-blog.csdnimg.cn/db568766644a4d10b8a91cdd2f8a4070.png)
* 这时如果要恢复服务的话，需要**人工介入**，选择一个「从节点」切换为「主节点」，然后让其他从节点指向新的主节点，同时还需要通知上游那些连接 Redis 主节点的客户端，将其配置中的主节点 IP 地址更新为「新主节点」的 IP 地址。
* Redis 在 2.8 版本以后提供的**哨兵（Sentinel）机制**，它的作用是实现**主从节点故障转移**。
* 它会监测主节点是否存活，如果发现主节点挂了，它就会**选举一个从节点切换为主节点**，并且**把新主节点的相关信息通知给从节点和客户端**。



##### 哨兵工作机制

* 哨兵其实是一个运行在**特殊模式下的 Redis 进程**，所以它也是一个节点。从“哨兵”这个名字也可以看得出来，它相当于是**“观察者节点”，观察的对象是主从节点**。
* 哨兵的主要工作： **监控、选主、通知**。
* ![哨兵的职责](https://img-blog.csdnimg.cn/775865f6bd894dfba8d373ee54d79af1.png)



##### 如何判断节点故障

* 哨兵会每隔 1 秒给所有**主从节点发送 PING 命令**，当主从节点收到 PING 命令后，会发送一个响应命令给哨兵，这样就可以判断它们是否在正常运行。

* ![哨兵监控主从节点](https://img-blog.csdnimg.cn/26f88373d8454682b9e0c1d4fd1611b4.png)

* 如果主节点或者从节点没有在规定的时间内响应哨兵的 PING 命令，哨兵就会将它们标记为「**主观下线**」。这个「规定的时间」是配置项 `down-after-milliseconds` 参数设定的，单位是毫秒。

* 主观下线和客观下线？

* > * 针对「**主节点**」设计「主观下线」和「客观下线」两个状态.
  > * 有可能「主节点」其实并没有故障，可能只是因为主节点的系统压力比较大或者网络发送了拥塞，导致主节点没有在规定时间内响应哨兵的 PING 命令。（为了减少误判）
  > * 为了减少误判的情况，哨兵在部署的时候不会只部署一个节点，而是用多个节点部署成**哨兵集群**（*最少需要三台机器来部署哨兵集群*），**通过多个哨兵节点一起判断，就可以就可以避免单个哨兵因为自身网络状况不好，而误判主节点下线的情况**。（误判率非常的低，当然不是 100% 确保没有误判）
  > * ![img](https://img-blog.csdnimg.cn/13e4361407ba46979e802eaa654dcf67.png)
  > * 多数投票的原则。**过半数可以确定主节点下线了**。哨兵就要开始在多个「从节点」中，选出一个从节点来做新主节点。
  > * 当这个哨兵的**赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后**，这时主节点就会被该哨兵标记为「**客观下线**」。



##### 由哪个哨兵进行故障转移

* 还需要在**哨兵集群中选出一个 leader**，让 leader 来执行主从切换。

* 如何选举 leader 投票的过程。（得有一个候选者）

* Q： 那谁来作为候选者呢？

* **哪个哨兵节点判断主节点为「客观下线」**，这个哨兵节点就是候选者，所谓的候选者就是想当 Leader 的哨兵。

* ![img](https://img-blog.csdnimg.cn/d0bed80d28a543fd8dcd299d4b06cf04.png)

* Q： 候选者**如何选举成为 Leader？**

* A:  候选者会向其他哨兵发送命令，表明希望成为 Leader 来执行主从切换，并让所有其他哨兵对它进行投票。(每一个候选者只有一张票，且只能投给一个候选者)

* 那么在投票过程中，任何一个「候选者」，要满足两个条件：

* > * 第一，拿到半数以上的赞成票；
  > * 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。

* 所以，**quorum 的值建议设置为哨兵个数的二分之一加1**，例如 3 个哨兵就设置 2，5 个哨兵设置为 3，而且**哨兵节点的数量应该是奇数**。



##### 主从故障转移过程

* 在**哨兵集群中通过投票的方式**，选举出了哨兵 leader 后，就可以进行主从故障转移的过程了，如下图：

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E5%93%A8%E5%85%B5/%E4%B8%BB%E4%BB%8E%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB.png)

* 主要步骤：

* > * 第一步：在已下线主节点（旧主节点）属下的所有「从节点」里面，挑选出一个从节点，并将其转换为主节点。
  >
  > * 选谁？
  >
  > * > * 我们首先要把网络状态不好的从节点给过滤掉。
  >   >
  >   > * 把已经下线的从节点过滤掉
  >   >
  >   > * Q： 怎么判断从节点之前的网络连接状态不好呢？
  >   >
  >   > * A： Redis 有个叫 down-after-milliseconds * 10 配置项，其down-after-milliseconds 是主从节点断连的最大连接超时时间。如果**在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。**如果发生**断连的次数超过了 10 次，就说明这个从节点的网络状况不好，不适合作为新主节点**。
  >   >
  >   > * 要对所有从节点进行三轮考察：**优先级、复制进度、ID 号**。
  >   >
  >   >   * 第一轮考察：哨兵首先会根据从节点的优先级来进行排序，优先级越小排名越靠前，
  >   >     * Redis 有个叫 slave-priority 配置项，可以给从节点设置优先级。
  >   >     * 每一台从节点的服务器配置不一定是相同的，我们可以根据**服务器性能配置**来设置从节点的优先级。
  >   >
  >   >   * 第二轮考察：如果优先级相同，则查看复制的下标，**哪个从「主节点」接收的复制数据多**，哪个就靠前。
  >   >     * 什么是复制进度？主从架构中，**主节点会将写操作同步给从节点**，在这个过程中，主节点会用 master_repl_offset 记录当前的**最新写操作在 repl_backlog_buffer 中的位置**
  >   >     * 而从节点会用 slave_repl_offset 这个值记录当前的复制进度（如下图中的「从服务器要读的位置」的位置）。
  >   >     * ![img](https://img-blog.csdnimg.cn/img_convert/2db4831516b9a8b79f833cf0593c1f12.png)
  >   >     * 如果某个**从节点的 slave_repl_offset 最接近 master_repl_offset**，说明它的**复制进度是最靠前**的，于是就可以将它选为新主节点。
  >   >   * 第三轮考察：如果优先级和下标都相同，就选择**从节点 ID 较小**的那个。
  >   >     * 什么是 ID 号？**每个从节点都有一个编号，这个编号就是 ID 号，是用来唯一标识从节点的。**
  >
  > * 第二步：让已下线主节点属下的所有「从节点」修改复制目标，修改为复制「新主节点」；
  >
  > * > * 让已下线**主节点属下的所有「从节点」指向「新主节点」**，这一动作可以通过向**「从节点」发送 `SLAVEOF` 命令**来实现。
  >   > * **哨兵 leader** 向所有从节点（server3和server4）发送 `SLAVEOF` ，让它们成为新主节点的从节点。
  >   > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E5%93%A8%E5%85%B5/%E4%BB%8E%E8%8A%82%E7%82%B9%E6%8C%87%E5%90%91%E6%96%B0%E4%B8%BB%E8%8A%82%E7%82%B9.png)
  >   > * 所有从节点指向新主节点后的拓扑图如下：
  >   > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E5%93%A8%E5%85%B5/%E4%BB%8E%E8%8A%82%E7%82%B9%E8%BD%AC%E6%8D%A2%E6%88%90%E5%8A%9F.png)
  >   > * 
  >
  > * 第三步：将**新主节点的 IP 地址和信息**，通过「发布者/订阅者机制」通知给客户端；
  >
  > * > * 经过前面一系列的操作后，**哨兵集群终于完成主从切换的工作**(以上操作都是通过哨兵集群完成的），那么新主节点的信息要如何通知给客户端呢？
  >   > * 这主要**通过 Redis 的发布者/订阅者机制来实现**的。每个**哨兵节点**提供发布者/订阅者机制，客户端可以从哨兵订阅消息。
  >   > * **哨兵提供的消息订阅频道有很多**，不同频道包含了主从节点切换过程中的不同关键事件，几个常见的事件如下：
  >   > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E5%93%A8%E5%85%B5/%E5%93%A8%E5%85%B5%E9%A2%91%E9%81%93.webp)
  >   > * **客户端和哨兵建立连接后，客户端会订阅哨兵提供的频道**。
  >   > * **主从切换完成后，哨兵就会向 `+switch-master` 频道发布新主节点的 IP 地址和端口的消息**
  >   > * **这个时候客户端就可以收到这条信息，然后用这里面的新主节点的 IP 地址和端口进行通信了**。
  >
  > * 第四步：继续**监视旧主节点，当这个旧主节点重新上线时，将它设置为新主节点的从节点**.
  >
  > * > * 故障转移操作最后要做的是，继续监视旧主节点，当旧主节点重新上线时，**哨兵集群**(这一切都是哨兵做的事）就会向它发送 `SLAVEOF` 命令，让它成为新主节点的从节点，如下图：
  >   > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E5%93%A8%E5%85%B5/%E6%97%A7%E4%B8%BB%E8%8A%82%E7%82%B9%E5%8F%98%E4%B8%BA%E6%96%B0%E4%B8%BB%E8%8A%82%E7%82%B9.png)





##### 哨兵集群组成

* 哨兵集群的**每一个哨兵都要和每一个redis数据节点（主节点和所有从节点）建立连接**。

* 搭建哨兵集群只需要填下面这几个参数，设置主节点名字、主节点的 IP 地址和端口号以及 quorum 值。

* ```c
  sentinel monitor <master-name> <ip> <redis-port> <quorum> 
  ```

* **哨兵节点之间是通过 Redis 的发布者/订阅者机制来相互发现的**。(相互感知相互连接)

* 在下图中，**哨兵 A 把自己的 IP 地址和端口的信息发布到`__sentinel__:hello` 频道上**，哨兵 B 和 C 订阅了该频道。那么此时，哨兵 B 和 C 就可以从这个频道直接获取哨兵 A 的 IP 地址和端口号。然后，哨兵 B、C 可以和哨兵 A 建立网络连接。

* ![img](https://img-blog.csdnimg.cn/a6286053c6884cf58bf397d01674fe80.png)

* Q: 哨兵集群会对「从节点」的运行状态进行监控，那哨兵集群**如何知道「从节点」的信息**？

* 主节点知道所有「从节点」的信息，所以哨兵会每 10 秒一次的频率**向主节点发送 INFO 命令来获取所有「从节点」的信息。**

* 哨兵 B 给主节点发送 INFO 命令，主节点接受到这个命令后，就会把**从节点列表返回给哨兵**。接着，哨兵就可以根据从节点**列表**中的连接信息，和每个从节点建立连接，并在这个连接上持续地对从节点进行监控。**哨兵 A 和 C 可以通过相同的方法和从节点建立连接**。

* ![img](https://img-blog.csdnimg.cn/fdd5f695bb3643258662886f9fba0aab.png)

* 正式**通过 Redis 的发布者/订阅者机制，**哨兵之间可以相互感知，然后组成集群，同时，哨兵又通过 INFO 命令，在**主节点里获得了所有从节点连接信息**，于是就能和从节点建立连接，并进行监控了。









#### Redis 删除和淘汰策略

##### 过期删除策略

###### 如何判定 Key 已近过期

* Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。

* 过期字典存储在 redisDb 结构中，如下：

* ```c
  typedef struct redisDb {
      dict *dict;    /* 数据库键空间，存放着所有的键值对 */
      dict *expires; /* 键的过期时间 */
      ....
  } redisDb;
  ```

* 过期字典的 **key 是一个指针，指向某个键对象**；

* 过期字典的 **value 是一个 long long 类型的整数，这个整数保存了 key 的过期时间**；

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5/%E8%BF%87%E6%9C%9F%E5%AD%97%E5%85%B8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png)

* 字典实际上是哈希表，哈希表的最大好处就是让我们可以用 O(1) 的时间复杂度来快速查找。当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：

* > * 如果不在，则正常读取键值；
  > * 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，**如果比系统时间大，那就没有过期，否则判定该 key 已过期。**

* 

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5/%E8%BF%87%E6%9C%9F%E5%88%A4%E6%96%AD%E6%B5%81%E7%A8%8B.jpg" alt="img" style="zoom:50%;" />





###### 过期删除策略有哪些

*  常见的三种过期删除策略：

* **定时删除:**

* > * 定时删除策略的做法是，**在设置 key 的过期时间时，同时创建一个定时事件，当时间到达时，由事件处理器自动执行 key 的删除操作。**
  > * 优点：
  >   * 可以保证过期 key 会被尽快删除，也就是**内存可以被尽快地释放**。因此，定时删除对内存是最友好的。
  > * 缺点：
  >   * 在过期 key 比较多的情况下，**删除过期 key 可能会占用相当一部分 CPU 时间**，在内存不紧张但 CPU 时间紧张的情况下，将 CPU 时间用于删除和当前任务无关的过期键上，无疑会对服务器的响应时间和吞吐量造成影响。所以，**定时删除策略对 CPU 不友好。**

* **惰性删除:**

* > * 惰性删除策略的做法是，**不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。**
  > * 优点：
  >   * 因为每次访问时，才会检查 key 是否过期，所以此策略只会**使用很少的系统资源**，因此，惰性删除策略对 CPU 时间最友好。
  > * 缺点：
  >   * 如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，**惰性删除策略对内存不友好**。

* **定期删除:**

* > * 定期删除策略的做法是，**每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。**(一种 CPU 和内存之间的 trade off)
  > * 优点：
  >   * 通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。
  > * 缺点：
  >   * 内存清理方面没有定时删除效果好，同时没有惰性删除使用的系统资源少。
  >   * **难以确定删除操作执行的时长和频率**。如果执行的太频繁，定期删除策略变得和定时删除策略一样，对CPU不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。



###### Redis 采用的是什么过期删除策略

*  **Redis 选择「惰性删除+定期删除」这两种策略配和使用**，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。

* Q： Redis 是怎么实现惰性删除的？

* A: Redis 的惰性删除策略由 db.c 文件中的 `expireIfNeeded` 函数实现，代码如下：

* ```C
  int expireIfNeeded(redisDb *db, robj *key) {
      // 判断 key 是否过期
      if (!keyIsExpired(db,key)) return 0;
      ....
      /* 删除过期键 */
      ....
      // 如果 server.lazyfree_lazy_expire 为 1 表示异步删除，反之同步删除；
      return server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) :
                                           dbSyncDelete(db,key);
  }
  ```

* Redis 在**访问或者修改 key 之前**，都会调用 expireIfNeeded 函数对其进行检查，检查 key 是否过期：

* > * 如果过期，则删除该 key，至于选择异步删除，还是选择同步删除，根据 `lazyfree_lazy_expire` 参数配置决定（Redis 4.0版本开始提供参数），然后返回 null 客户端；
  > * 如果没有过期，不做任何处理，然后返回正常的键值对给客户端；

* Q : Redis 是怎么实现定期删除的？

* > * *1、这个**间隔检查的时间是多长呢**？*
  > * 在 Redis 中，默认**每秒进行 10 次过期检查一次数据库**，此配置可通过 Redis 的配置文件 redis.conf 进行配置，配置键为 hz 它的默认值是 hz 10。
  > * 特别强调下，每次检查数据库并不是遍历过期字典中的所有 key，而是**从数据库中随机抽取一定数量的 key** 进行过期检查。
  > * *2、**随机抽查的数量是多少呢**？*
  > * 定期删除的实现在 expire.c 文件下的 `activeExpireCycle` 函数中，其中随机抽查的数量由 `ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP` 定义的，它是写死在代码中的，**数值是 20**。
  > * 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「**已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%**，则继续重复步骤 1(重复执行随机抽取 20 个 key)；如果**已过期的 key 比例小于 25%，则停止继续删除过期 key**，然后等待下一轮再检查。
  > * **Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象**，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。
  > * <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5/%E5%AE%9A%E6%97%B6%E5%88%A0%E9%99%A4%E6%B5%81%E7%A8%8B.jpg" alt="img" style="zoom:33%;" />
  > * 



##### 内存淘汰策略

* 当 Redis 的运行内存已经超过 Redis 设置的最大内存之后，则会**使用内存淘汰策略删除符合条件的 key**，以此来保障 Redis 高效的运行

##### 如何设置 Redis 的最大运行内存

* 在配置文件 redis.conf 中，可以**通过参数 `maxmemory <bytes>` 来设定最大运行内存**，只有在 Redis 的运行内存达到了我们设置的最大运行内存，才会触发内存淘汰策略。 

* > * 在 64 位操作系统中，**maxmemory 的默认值是 0，表示没有内存大小限制**，那么不管用户存放多少数据到 Redis 中，Redis 也不会对可用内存进行检查，**直到 Redis 实例因内存不足而崩溃也无作为**。
  > * 在 32 位操作系统中，maxmemory 的默认值是 3G，因为 32 位的机器最大只支持 4GB 的内存，而系统本身就需要一定的内存资源来支持运行，所以 **32 位操作系统限制最大 3 GB** 的可用内存是非常合理的，这样可以避免因为内存不足而导致 Redis 实例崩溃。



###### 淘汰策略

* 1、不进行数据淘汰的策略

* **noeviction**（Redis3.0之后，**默认的内存淘汰策略**） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是**不再提供服务，直接返回错误**。

* 2、进行数据淘汰的策略

* 「在**设置了过期时间的数据中进行淘汰**」和「在**所有数据范围内进行淘汰**」这两类策略。

* 在设置了**过期时间的数据**中进行淘汰：

* > * **volatile-random**：**随机淘汰设置了过期时间的任意键值**；
  > * **volatile-ttl**：优先淘汰更早过期的键值。
  > * **volatile-lru**（**Redis3.0 之前，默认的内存淘汰策略**）：淘汰所有设置了过期时间的键值中，最久未使用的键值；
  > * **volatile-lfu**（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值；

* 在**所有数据范围内**进行淘汰：

* > * **allkeys-random**：随机淘汰任意键值;
  > * **allkeys-lru**：淘汰整个键值中最久未使用的键值；
  > * **allkeys-lfu**（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。



###### Redis LRU 

* Redis 实现的是一种**近似 LRU 算法**，目的是为了更好的节约内存，它的**实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间**。

* 当 Redis 进行内存淘汰时，会使用**随机采样的方式来淘汰数据**，它是随机取 5 个值（此值可配置），然后**淘汰最久没有使用的那个**。

* 优点：

* > * **不用为所有的数据维护一个大链表，节省了空间占用**；
  > * **不用在每次数据访问时都移动链表项**，提升了缓存的性能；

  







#### Redis 大 key 对持久化的影响

* 大 key 指 **key 对应的 value 很大**。 一般而言，下面这两种情况被称为大 key：
* 1. String 类型的值大于 10 KB； 
  2. **Hash、List、Set、ZSet 类型的元素的个数超过 5000个**。
* 

##### 大 key 对 AOF 日志的影响

- Always 策略就是每次写入 AOF 文件数据后，就执行 fsync() 函数；同步将 AOF 日志数据写回硬盘；等到硬盘写操作完成后，该函数才会返回。

- Everysec 策略就会创建一个异步任务来执行 fsync() 函数；

- No 策略就是永不执行 fsync() 函数; 意味着**不由 Redis 控制写回硬盘的时机**，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。

- 在持久化大 Key 的时候，会影响什么？

- > * **当使用 Always 策略的时候，如果写入是一个大 Key，主线程在执行 fsync() 函数的时候，阻塞的时间会比较久，因为当写入的数据量很大的时候，数据同步到硬盘这个过程是很耗时的**。
  > * 当使用 Everysec 策略的时候，由于是异步执行 fsync() 函数，所以大 Key 持久化的过程（数据同步磁盘）不会影响主线程。
  > * 当使用 No 策略的时候，由于永不执行 fsync() 函数，所以大 Key 持久化的过程不会影响主线程。

##### 大 Key 对 AOF 重写和 RDB 的影响

* 当 AOF 日志写入了很多的大 Key，AOF 日志文件的大小会很大，那么很快就会触发 **AOF 重写机制**。

* **AOF 重写机制和 RDB 快照（bgsave 命令）的过程**，都会分别通过 `fork()` 函数创建一个子进程来处理任务。

* 在创建子进程的过程中，**操作系统会把父进程的「页表」复制一份给子进程**（会阻塞 Redis 单线程），这个页表记录着虚拟地址和物理地址映射关系，而不会复制物理内存，也就是说，**两者的虚拟空间不同，但其对应的物理空间是同一个**。

* ![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfyKst8QpumWAlMcdhiarA6w9nuL1LxHCoUBMnUHOer2WQicZp7jbgzgxWIrXJJ66iaKZMY8zLC0PQQg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

* 子进程就共享了父进程的物理内存数据了，这样能够节约物理内存资源，页表对应的页表项的属性会标记该物理内存的权限为**只读**。

* 随着 Redis 存在越来越多的大 Key，那么 Redis 就会占用很多内存，**对应的页表就会越大**。

* 在通过  `fork()` 函数创建子进程的时候，虽然不会复制父进程的物理内存，但是**内核会把父进程的页表复制一份给子进程，如果页表很大，那么这个复制过程是会很耗时的，那么在执行 fork 函数的时候就会发生阻塞现象**。

* fork 函数是由 Redis 主线程调用的，如果 **fork 函数发生阻塞，那么意味着就会阻塞 Redis 主线程**。

* 由于 Redis 执行命令是在主线程处理的，所以当 **Redis 主线程发生阻塞，就无法处理后续客户端发来的命令**。

* fork 耗时过大，如何处理：

* > * 在主从架构中，要适当**调大 repl-backlog-size**，避免因为  repl_backlog_buffer 不够大，导致主节点频繁地使用全量同步的方式，全量同步的时候，是会创建 RDB 文件的，也就是会调用 fork 函数。
  > * 如果 Redis 只是当作纯缓存使用，不关心 Redis 数据安全性问题，可以考虑关闭 AOF 和 AOF 重写，这样就不会调用 fork 函数了。
  > * **单个实例的内存占用控制在 10 GB 以下**，这样 fork 函数就能很快返回。

  

* 如果创建完子进程后，**父进程对共享内存中的大 Key 进行了修改，那么内核就会发生写时复制，会把物理内存复制一份，由于大 Key 占用的物理内存是比较大的，那么在复制物理内存这一过程中，也是比较耗时的，于是父进程（主线程）就会发生阻塞**。

* 有两个阶段会导致阻塞父进程：

* > * 创建子进程的途中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大，阻塞的时间也越长；
  > * 创建完子进程后，如果子进程或者父进程修改了共享数据，就会发生写时复制，这期间会拷贝物理内存，如果内存越大，自然阻塞的时间也越长；





##### 大 key 可能的其他影响

* **客户端超时阻塞**。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。
* **引发网络阻塞**。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。
* **阻塞工作线程**。如果使用 del 删除大 key 时，会阻塞**工作线程**，这样就没办法处理后续的命令。
* **内存分布不均**。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大。





##### 如何避免大 Key 呢？

* 最好在设计阶段，就把大 key 拆分成一个一个小 key。
* 定时检查 Redis 是否存在大 key ，如果该大 key 是可以删除的，**不要使用 DEL 命令删除**，因为该命令删除过程会阻塞主线程，**而是用 unlink 命令（Redis 4.0+）删除大 key**，因为该命令的**删除过程是异步的，不会阻塞主线程**。 



#### 切片集群

* 要用 Redis 保存 5000 万个键值对，每个键值对大约是 512B，为了能快速部署并对外提供服务，我们采用云主机来运行 Redis 实例，那么，该如何选择云主机的内存容量呢？
* 这些键值对所占的内存空间大约是 25GB（5000 万 *512B）
* 选择一台 32GB 内存的云主机来部署 Redis。因为 32GB 的内存能保存所有数据，而且还留有 7GB，可以保证系统的正常运行。同时，我还采用 RDB 对数据做持久化，以确保 Redis 实例故障后，还能从 RDB 恢复数据。
* **Redis 的响应有时会非常慢**。后来，我们使用 INFO 命令查看 Redis 的 latest_fork_usec 指标值（表示最近一次 fork 的耗时），结果显示这个指标值特别高，**快到秒级别了**。
* 这跟 Redis 的持久化机制有关系。在使用 RDB 进行持久化时，Redis 会 fork 子进程来完成，fork 操作的用时和 Redis 的数据量是正相关的，而 **fork 在执行时会阻塞主线程**。
* **这块儿消耗主要就在页表的数量上，数据越大，页表越多，复制就越花时间。**
* Redis 的切片集群。虽然组建切片集群比较麻烦，但是它可以保存大量数据，而且对 Redis 主线程的阻塞影响较小。
* 切片集群，也叫分片集群，就是指启动多个 Redis 实例组成一个集群，**然后按照一定的规则，把收到的数据划分成多份，每一份用一个实例来保存。**
* ![img](https://static001.geekbang.org/resource/image/79/26/793251ca784yyf6ac37fe46389094b26.jpg?wh=2501*1342)
* 在切片集群中，实例在为 5GB 数据生成 RDB 时，数据量就小了很多，fork 子进程一般不会给主线程带来较长时间的阻塞。
* 采用多个实例保存数据切片后，我们既能保存 25GB 数据，又避免了 fork 子进程阻塞主线程而导致的响应突然变慢。



##### 如何保存更多的数据

* 这两种方法分别对应着 Redis 应对数据量增多的两种方案：纵向扩展（scale up）和横向扩展（scale out）。

* **纵向扩展**：升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU。就像下图中，原来的实例内存是 8GB，硬盘是 50GB，纵向扩展后，内存增加到 24GB，磁盘增加到 150GB。

* **横向扩展**：横向增加当前 Redis 实例的个数，就像下图中，原来使用 1 个 8GB 内存、50GB 磁盘的实例，现在使用三个相同配置的实例。

* **![img](https://static001.geekbang.org/resource/image/7a/1a/7a512fec7eba789c6d098b834929701a.jpg?wh=2745*1365)**

* 纵向扩展的好处是，实施起来简单、直接。不过，这个方案也面临两个潜在的问题：

* > * 第一个问题是，当使用 RDB 对数据进行持久化时，如果数据量增加，需要的内存也会增加，**主线程 fork 子进程时就可能会阻塞**（比如刚刚的例子中的情况）。如果你不要求持久化保存 Redis 数据，那么，纵向扩展会是一个不错的选择。
  > * 你还要面对第二个问题：**纵向扩展会受到硬件和成本的限制**。这很容易理解，毕竟，把内存从 32GB 扩展到 64GB 还算容易，但是，要想扩充到 1TB，就会面临硬件容量和成本上的限制了。

* 采用这种方案的话，只用增加 Redis 的实例个数就行了，不用担心单个实例的硬件和成本限制。**在面向百万、千万级别的用户规模时，横向扩展的 Redis 切片集群会是一个非常好的选择。**

* 切片集群不可避免地涉及到多个实例的分布式管理问题。要想把切片集群用起来，我们就需要解决两大问题：

* > * 数据切片后，在多个实例之间如何分布？
  > * 客户端怎么确定想要访问的数据在哪个实例上？



##### 数据切片和实例的对应关系

* **Redis Cluster 方案采用哈希槽（Hash Slot，接下来我会直接称之为 Slot），来处理数据和实例之间的映射关系**。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。

* 具体的映射过程分为两大步：首先根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值；然后，再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。

* 我们在部署 Redis Cluster 方案时，可以使用 cluster create 命令创建集群，此时，**Redis 会自动把这些槽平均分布在集群实例上**。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384/N 个。

* 可以**根据不同实例的资源配置情况，使用 cluster addslots 命令手动分配哈希槽**。

* ![img](https://static001.geekbang.org/resource/image/7d/ab/7d070c8b19730b308bfaabbe82c2f1ab.jpg?wh=2486*1266)

* ```cpp
  
  redis-cli -h 172.16.19.3 –p 6379 cluster addslots 0,1
  redis-cli -h 172.16.19.4 –p 6379 cluster addslots 2,3
  redis-cli -h 172.16.19.5 –p 6379 cluster addslots 4
  ```



##### 客户端如何定位数据的

* 一般来说，客户端和集群实例建立连接后，实**例就会把哈希槽的分配信息发给客户端**。但是，在集群刚刚创建的时候，每个实例只知道自己被分配了哪些哈希槽，是不知道其他实例拥有的哈希槽信息的。

* 客户端为什么可以在访问任何一个实例时，都能获得所有的哈希槽信息呢？这是因为，Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成**哈希槽分配信息的扩散**。当**实例之间相互连接后，每个实例就有所有哈希槽的映射关系了**。

* 在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个：

* > * 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽；
  > * 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。

* 实例之间还可以通过相互传递消息，获得最新的哈希槽分配信息，但是，**客户端是无法主动感知这些变化的**。这就会导致，它缓存的分配信息和最新的分配信息就不一致了，那该怎么办呢？

* **Redis Cluster 方案提供了一种重定向机制**，所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，**这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令**。

* 如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回下面的 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。

* ```cpp
  
  GET hello:key
  (error) MOVED 13320 172.16.19.5:6379
  ```

* MOVED 命令表示，客户端请求的键值对所在的哈希槽 13320，实际是在 172.16.19.5 这个实例上。通过返回的 MOVED 命令，就相当于把哈希槽所在的新实例的信息告诉给客户端了。这样一来，客户端就可以直接和 172.16.19.5 连接，并发送操作请求了。

* ![img](https://static001.geekbang.org/resource/image/35/09/350abedefcdbc39d6a8a8f1874eb0809.jpg?wh=2818*2250)

* 客户端向实例 2 发送请求，但此时，Slot 2 中的数据**只有一部分迁移到了实例 3**，还有部分数据没有迁移。在这种迁移部分完成的情况下，客户端就会收到一条 ASK 报错信息.

* ```cpp
  
  GET hello:key
  (error) ASK 13320 172.16.19.5:6379
  ```

* 这个结果中的 ASK 命令就表示，客户端请求的键值对所在的哈希槽 13320，在 172.16.19.5 这个实例上，但是这个哈希槽正在迁移。此时，客户端需要先给 172.16.19.5 这个实例发送一个 ASKING 命令。这个命令的意思是，让**这个实例允许执行客户端接下来发送的命令**。然后，**客户端再向这个实例发送 GET 命令，以读取数据**。

* ASK 命令表示两层含义：第一，表明 Slot 数据还在迁移中；第二，ASK 命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 ASKING 命令，然后再发送操作命令。

* ![img](https://static001.geekbang.org/resource/image/e9/b0/e93ae7f4edf30724d58bf68yy714eeb0.jpg?wh=3000*2250)

* 哈希槽在迁移过程中，**有的key还在原节点，则直接返回客户端**，如果**key已经迁移到新节点，则发送ASK命令给客户**端，让客户端去新节点查询。

* 和 MOVED 命令不同，ASK 命令并不会更新客户端缓存的哈希槽分配信息。所以，在上图中，如果客户端再次请求 Slot 2 中的数据，它还是会给实例 2 发送请求。

* **ASK 命令的作用只是让客户端能给新实例发送一次请求，而不像 MOVED 命令那样，会更改本地缓存，让后续所有命令都发往新实例。**

* 









#### Redis  在 weibo 中的实战

##### weibo 如何应对大数据量数据的存储需求

* 微博业务层要保存的数据经常会达到 TB 级别，这就需要扩大 Redis 实例的存储容量了。
* 针对这个需求，微博对**数据区分冷热度**，把**热数据保留在 Redis 中，而把冷数据通过 RocksDB 写入底层的硬盘**。
* 在微博的业务场景中，冷热数据是比较常见的。比如说，**有些微博话题刚发生时，热度非常高，会有海量的用户访问这些话题**，使用 Redis 服务用户请求就非常有必要。
* 等到话题热度过了之后，访问人数就会急剧下降，这些数据就变为冷数据了。这个时候，冷数据就可以从 Redis 迁移到 RocksDB，保存在硬盘中。
* ![img](https://static001.geekbang.org/resource/image/c0/b0/c0fdb8248a3362afd29d3efe8b6b21b0.jpg?wh=2143*1511)
* 实现大容量的单实例在某些业务场景下还是有需求的。虽然**我们可以使用切片集群的多实例分散保存数据，但是这种方式也会带来集群运维的开销，涉及到分布式系统的管理和维护**。
* 切片集群的规模会受限，如果能增加单个实例的存储容量，那么，即使在使用较小规模的集群时，集群也能保存更多的数据。
* 为了能够灵活地支持这些业务需求，微博对 Redis 进行了服务化改造（RedisService）。所谓服务化，就是指，**使用 Redis 集群来服务不同的业务场景需求，每一个业务拥有独立的资源，相互不干扰。**
* 此外，在服务化集群中，还有一个配置中心，它用来管理整个集群的元数据。同时，**实例会按照主从模式运行，保证数据的可靠性**。不同业务的数据部署到不同的实例上，相互之间保持隔离。
* ![img](https://static001.geekbang.org/resource/image/58/ff/58dc7b26b8b0a1df4fd1faeee24618ff.jpg?wh=2672*1873)
* 当把一个通用功能做成平台服务时，我们需要重点考虑的问题，包括平台平滑扩容、多租户支持和业务数据隔离、灵活的路由规则、丰富的监控功能等。
* 如果要进行平台扩容，我们可以借助 Codis 或是 Redis Cluster 的方法来实现。
* 多租户支持和业务隔离的需求是一致，我们需要通过资源隔离来实现这两个需求，也就是**把不同租户或不同业务的数据分开部署**，避免混用资源。
* 对于**路由规则和监控功能来说**，微博目前的方案是不错的，也就是在**代理层 proxy 中来完成这两个功能**。
* 所谓**业务纵切**，是指把不同的**业务数据单独部署**，这样可以避免相互之间的干扰。而**平台横切是指**，**当不同业务线对运行平台具有相同需求时，可以统一起来，通过构建平台级集群服务来进行支撑。**
* 























### OS

#### 操作系统内存分配底层实现



#### 内存分配算法 buddy 算法 和 slab 算法

##### buddy 算法

* 解决内存碎片的方式。
* ![img](https://pic2.zhimg.com/80/v2-dada2ecf1c8fc822f1849a5f4dce54cd_720w.webp)
* 假设这是一段连续的页框，阴影部分表示已经被使用的页框，现在需要申请一个连续的5个页框。这个时候，**在这段内存上不能找到连续的5个空闲的页框，就会去另一段内存上去寻找5个连续的页框，这样子，久而久之就形成了页框的浪费。**
* 把所有的空闲页框分组为11个块链表，每个块链表分别包含大小为1，2，4，8，16，32，64，128，256，512和1024个连续页框的页框块。
* 最大可以申请1024个连续页框，对应4MB大小的连续内存。
* ![img](https://pic3.zhimg.com/80/v2-90378f7a3b2bbaa6aad693d73544b862_720w.webp)
* 假设要申请一个256个页框的块，先从256个页框的链表中查找空闲块，如果没有，就去512个页框的链表中找，找到了则将页框块分为2个256个页框的块，一个分配给应用，另外一个移到256个页框的链表中。
* 如果512个页框的链表中仍没有空闲块，继续向1024个页框的链表查找，如果仍然没有，则返回错误。
* **页框块在释放时，会主动将两个连续的页框块合并为一个较大的页框块**。
* 从上面可以知道Buddy算法一直在对页框做**拆开合并拆开合并**的动作。
* Buddy算法牛逼就牛逼在运用了世界上任何正整数都可以由2^n的和组成。这也是Buddy算法管理空闲页表的本质。 空闲内存的信息我们可以通过以下命令获取：
* ![img](https://pic2.zhimg.com/80/v2-d5fa2563faa7142e9e0346ef5ca994d9_720w.webp)
* 





##### slab 算法

* 在Linux中，伙伴系统（buddy system）是以页为单位管理和分配内存
* 但是现实的需求却**以字节为单位**，假如我们需要申请20Bytes，总不能分配一页吧！那岂不是严重浪费内存。
* slab分配器就应运而生了，专为小内存分配而生。**slab分配器分配内存以Byte为单位**。
* slab分配器并没有脱离伙伴系统，而是基于伙伴系统分配的大内存进一步细分成小内存分配。
* ![img](https://pic2.zhimg.com/80/v2-90fe0938cf0c8ab8836257ed587654d1_720w.webp)
* kmem_cache是一个cache_chain的链表，描述了一个高速缓存，每个高速缓存包含了一个slabs的列表，这通常是一段连续的内存块。存在3种slab：
* slabs_full(完全分配的slab)
* slabs_partial(部分分配的slab)
* slabs_empty(空slab,或者没有对象被分配)。
* **slab是slab分配器的最小单位**，在实现上一个slab有一个或多个连续的物理页组成（通常只有一页）
* 单个slab可以在slab链表之间移动，例如如果一个半满slab被分配了对象后变满了，就要从slabs_partial中被删除，同时插入到slabs_full中去。
* 用struct kmem_cache结构描述的一段内存就称作一个slab缓存池。一个slab缓存池就像是一箱牛奶，一箱牛奶中有很多瓶牛奶，每瓶牛奶就是一个object。分配内存的时候，就相当于从牛奶箱中拿一瓶。总有拿完的一天。当箱子空的时候，你就需要去超市再买一箱回来。超市就相当于partial链表，超市存储着很多箱牛奶。如果超市也卖完了，自然就要从厂家进货，然后出售给你。厂家就相当于伙伴系统。
* ![img](https://pic1.zhimg.com/80/v2-c381d93642b716770c6705dac13a30c8_720w.webp)
* **malloc分配的是虚拟地址空间，而slab/buddy分配的是真正的物理内存**，它们是内存分配的两个阶段。
* Jeff发现对内核中普通对象进行初始化所需的时间超过了对其进行分配和释放所需的时间。因此他的结论是不应该将内存释放回一个全局的内存池，而是将内存保持为针对特定目而初始化的状态。
* 如果内存被分配给了一个互斥锁，那么只需在为互斥锁首次分配内存时执行一次互斥锁初始化函数（mutex_init）即可。后续的内存分配不需要执行这个初始化函数，因为从**上次释放和调用析构之后，它已经处于所需的状态中了**。
*  Linux slab分配器使用了这种思想和其他一些思想来构建一个在空间和时间上都具有高效性的内存分配器。
* 



#### 如何判断一个线程池需要创建多少线程







#### Fork



子进程继承父进程

- 用户号UIDs和用户组号GIDs      
- 环境Environment      
- 堆栈      
- 共享内存      
- 打开文件的描述符      
- 执行时关闭（Close-on-exec）标志      
- 信号（Signal）控制设定      
- 进程组号      
- 当前工作目录      
- 根目录      
- 文件方式创建屏蔽字      
- 资源限制      
- 控制终端    

子进程独有

- 进程号PID      
- 不同的父进程号      
- 自己的文件描述符和目录流的拷贝      
- 子进程不继承父进程的进程正文（text），**数据和其他锁定内存**（memory locks）      
- 不继承异步输入和输出
- **父进程设置的锁，子进程不继承（因为如果是排它锁，被继承的话，矛盾了）**    

父进程和子进程拥有独立的地址空间和PID参数。

**子进程从父进程继承了用户号和用户组号，用户信息，目录信息，环境（表），打开的文件描述符，堆栈，（共享）内存等。**  

经过fork()以后，父进程和子进程拥有相同内容的代码段、数据段和用户堆栈，就像**父进程把自己克隆了一遍**。事实上，**父进程只复制了自己的PCB块**。而代码段，数据段和用户堆栈内存空间并没有复制一份(copy on write)，而是与子进程共享。只有当子进程在运行中出现写操作时，才会产生中断，并为子进程分配内存空间。由于父进程的PCB和子进程的一样，所以在PCB中断中所记录的父进程占有的资源，也是与子进程共享使用的。这里的“共享”一词意味着“竞争”

* 如果父子进程一直对这个页面是同一个页面，知道其中任何一个进程要对共享的页面“写操作”，这时**内核会复制一个物理页面给这个进程使用，同时修改页表**。而**把原来的只读页面标记为“可写”，留给另外一个进程使用**。
* 这俩个**进程将执行相同的程序段**，但是拥有各自不同的堆段，栈段，数据段，每个子程序都可修改各自的数据段，堆段，和栈段。
* 首先我们可以**确定父子进程的代码段是相同的，所以代码段是没必要复制的**，因此内核将代码段标记为只读，这样父子进程就可以安全的共享此代码段了。fork之后在进程创建代码段时，新子进程的进程级页表项都指向和父进程相同的物理页帧
* ![img](https://static.coonote.com/2021/1/V3qEVj.png)
* 而对于父进程的数据段，堆段，栈段中的各页，**由于父子进程要相互独立，所以我们采用写实复制的技术，来最大化的提高内存以及内核的利用率。**刚开始，内核做了一些设置，令这些段的页表项指向父进程相同的物理内存页。调用fork之后，内核会捕获所有父进程或子进程针对这些页面的修改企图(说明此时还没修改)并为将要修改的页面创建拷贝。系统将新的页面拷贝分配给被内核捕获的进程，还会对子进程的相应页表项做适当的调整，现在父子进程就可以分别修改各自的上述段，不再互相影响了
* 写时复制前：
* ![img](https://static.coonote.com/2021/1/RBfU7r.png)
* 写时复制后：
* ![img](https://static.coonote.com/2021/1/6VJvua.png)
* 刚调用完fork()之后，**子进程只是拥有一份和父进程相同的页表**，其中页表中指向RAM代码段的部分是不会改变的，而指向数据段，堆段，栈段的会在**我们将要改变父子进程各自的这部分内容时，才会将要操作的部分进行部分复制**





#### 死锁分析与解决

##### 死锁代码：

* ```cpp
  #include<bits/stdc++.h>
  #include <unistd.h>
  
  std::mutex mtx;
  int data;
  
  // 下面是单线程的死锁
  
  void a1() {
      mtx.lock();
      std::cout << data << "\n";
      mtx.unlock();
  }
  
  void a2() {
      mtx.lock();
      data++;
      a1();
      mtx.unlock();
  }
  
  // 下面是多线程的死锁
  std::mutex mtx1, mtx2;
  int num;
  void a3() {
      std::cout << "a3" << std::endl;
      mtx1.lock();
      sleep(1); // 等待 a4() 加锁
      mtx2.lock();
      std::cout << data << "\n";
      mtx2.unlock();
      mtx1.unlock();
  }
  
  void a4() {
      std::cout << "a4" << std::endl;
      mtx2.lock();
      sleep(1);
      mtx1.lock();
      std::cout << num << std::endl;
      mtx1.unlock();
      mtx2.unlock();
  }
  
  int main() {
      // data = 1;
      // std::thread th1(a2);
      // th1.join();
      num = 10;
      data = 1;
      std::thread th1(a3);
      std::thread th2(a4);
      th1.join();
      th2.join();
      return 0;
  }
  
  ```

##### 排查方式

* pstack 命令可以显示每个线程的栈跟踪信息（函数调用过程），它的使用方式也很简单，只需要 `pstack <pid>` 就可以了。
* 在定位死锁问题时，我们可以多次执行 pstack 命令查看线程的函数调用过程，多次对比结果，确认哪几个线程一直没有变化，**且是因为在等待锁，那么大概率是由于死锁问题导致的**。
* **pstack 输出了我前面模拟死锁问题的进程的所有线程的情况，我多次执行命令后，其结果都一样，可以判定是死锁了**。
* gdb 也可以打印出线程运行的情况。
* 因为线程 B 在等待线程 A 所持有的 mutex_A, 而同时线程 A 又在等待线程 B 所拥有的mutex_B, 所以可以断定该程序发生了死锁

###### gdb

```cpp
// gdb 命令
$ gdb -p 87746

// 打印所有的线程信息
(gdb) info thread
  3 Thread 0x7f60a610a700 (LWP 87747)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
  2 Thread 0x7f60a5709700 (LWP 87748)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
* 1 Thread 0x7f60a610c700 (LWP 87746)  0x0000003720e080e5 in pthread_join () from /lib64/libpthread.so.0
//最左边的 * 表示 gdb 锁定的线程，切换到第二个线程去查看

// 切换到第2个线程
(gdb) thread 2
[Switching to thread 2 (Thread 0x7f60a5709700 (LWP 87748))]#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0 

// bt 可以打印函数堆栈，却无法看到函数参数，跟 pstack 命令一样 
(gdb) bt
#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0
#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0
#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25
#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0
#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6

// 打印第三帧信息，每次函数调用都会有压栈的过程，而 frame 则记录栈中的帧信息
(gdb) frame 3
#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25
27    printf("thread B waiting get ResourceA \n");
28    pthread_mutex_lock(&mutex_A);

// 打印mutex_A的值 ,  __owner表示gdb中标示线程的值，即LWP
(gdb) p mutex_A
$1 = {__data = {__lock = 2, __count = 0, __owner = 87747, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, 
  __size = "\002\000\000\000\000\000\000\000\303V\001\000\001", '\000' <repeats 26 times>, __align = 2}

// 打印mutex_B的值 ,  __owner表示gdb中标示线程的值，即LWP
(gdb) p mutex_B
$2 = {__data = {__lock = 2, __count = 0, __owner = 87748, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, 
  __size = "\002\000\000\000\000\000\000\000\304V\001\000\001", '\000' <repeats 26 times>, __align = 2}  
```





##### 避免死锁的方式

* 避免死锁问题就只需要破环其中一个条件就可以，最常见的并且可行的就是**使用资源有序分配法，来破环环路等待条件**。

* 资源有序分配： 

* > * 线程 A 和 线程 B 获取资源的顺序要一样，当线程 A 是先尝试获取资源 A，然后尝试获取资源 B 的时候，线程 B **同样**也是先尝试获取资源 A，然后尝试获取资源 B。
  > * 也就是说，线程 A 和 线程 B 总是以相同的顺序申请自己想要的资源。
  > * **线程 B 函数，同线程 A 一样，先获取互斥锁 A，然后获取互斥锁 B**

* ```cpp
  // 下面是破坏循环等待，从而避免死锁的多线程代码
  std::mutex mtx1, mtx2;
  int num;
  void a3() {
      std::cout << "a3" << std::endl;
      mtx1.lock();
      sleep(1); // 等待 a4() 加锁
      mtx2.lock();
      std::cout << data << "\n";
      mtx2.unlock();
      mtx1.unlock();
  }
  
  void a4() {
      std::cout << "a4" << std::endl;
      mtx1.lock();
      sleep(1);
      mtx2.lock();
      std::cout << num << std::endl;
      mtx1.unlock();
      mtx2.unlock();
  }
  ```










#### 内存池

* 内存池的思想是，在真正使用内存之前，**预先申请分配**一定数量( Ptmalloc2 为子线程预分配了 64MB 内存池，虽然增大了内存消耗，但却加快了分配速度，这就是以空间换时间的思想 )、大小预设的内存块留作备用。当有新的内存需求时，就从内存池中分出一部分内存块，若内存块不够再继续申请新的内存，当内存释放后就回归到内存块留作后续的复用，使得内存使用效率得到提升，一般也不会产生不可控制的内存碎片。

* 常规的动态内存申请执行系统调用，内核也是维护一个空闲区池，返回一个空闲区列表中一个合适大小的空间。只是**系统调用的开销带来性能损失**，**内存池在user space实现了内存申请释放，速度会有很大提升**。

* 内存碎片 不是内存池能够解决的问题，要靠二次映射，类似MMU里的映射。 内存池是解决频繁申请小内存的速度问题的。

* 原理：

* > * 预申请一个内存区chunk，将内存中按照对象大小划分成多个内存块block
  > * 维持一个空闲内存块链表，通过指针相连，标记头指针为第一个空闲块
  > * 每次新申请一个对象的空间，则将该内存块从空闲链表中去除，更新空闲链表头指针
  > * 每次释放一个对象的空间，则重新将该内存块加到空闲链表头
  > * 如果一个内存区占满了，则新开辟一个内存区，维持一个内存区的链表，同指针相连，头指针指向最新的内存区，新的内存块从该区内重新划分和申请
  > * ![img](https://pic4.zhimg.com/80/v2-cf2aa1d2ff66ffbced2e8eda93bc10df_720w.jpg)
  > * ![img](https://pic4.zhimg.com/80/v2-cf2aa1d2ff66ffbced2e8eda93bc10df_720w.jpg)
  > * ![img](https://pic4.zhimg.com/80/v2-fc003549f02accf6e2bbcafb499a786b_720w.jpg)

#### 进程内存空间

* 进程里的线程可以共享进程的部分资源的，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等

![img](https://pic3.zhimg.com/80/v2-6be4316d656d82282974fd348cedfb22_720w.jpg)

![img](https://pic3.zhimg.com/80/v2-9268ae5118fd1989d3e7606f2ad46bae_720w.jpg)

#### 进程线程协程的区别

* 进程比较重量，占据独立的内存，上下文之间进程的切换比较大（栈、寄存器、虚拟内存、文件句柄），进程间通信的方式比较多：管道，共享内存（需要和信号量协作），消息队列. (32 位系统 ： 4GB 虚拟内存， 1GB 内核， 3GB用户内存)
* 线程自己不拥有资源，只拥有一点在运行中必不可少的资源 （程序计数器，寄存器和栈），但是他可以与同属一个进程的其他线程共享进程的所有资源，线程间通信的方式是：共享内存，上下文切换很快，资源丢失的比较少，但是相对进程不太稳定容易丢失数据 （10MB）,线程的栈大小是 8MB ，linux 内
* 协程：**在用户态** 中的轻量级线程，协程的调度完全是由用户控制的 (2-4KB 级别)，对内核来说是不可见的
* 协程会从进程的堆中分配一段内存作为协程的栈
* 现代操作系统将线程这个概念封装在 os 层面了，对程序员是不可见的，使用起来更加的方便，但是协程这个东西，需要程序员自己执行管理，这样难度比较大对程序员
* 协程是非抢占式的多任务，但是线程是抢占式的多任务，协程需要代码的编写者主动让出控制权，线程不需要规划让出控制权。



#### 进程间通信的方式详解

每一个进程的内存空间都是相互独立的，因为虚拟内存，但是内核空间每一个进程都是共享的，所以进程间通信必须通过内核。

* 管道 （环形队列）

> * 管道是内核区的一个缓冲区，也是一个文件，所以父子进程之间可以进行通信，因为**子进程共享父进程的文件描述符**（在内核区 PCB 中），所以管道这个文件描述符，在子进程和父进程的文件描述符是一样的，所以父子进程可以识别某一个文件描述符。
> * 匿名管道： Linux 中 | 的就是，**管道的数据传输方向是单向的**。如果需要相互通信，必须创建两个管道
> * 命名管道：FIFO，数据按照先进先出的方式传输数据,可以**使任意两个进程**进行通信操作,**命名管道以文件的形式存放在文件系统中**，而**匿名管道对文件系统是不可见的**。仅限于父子进程之间的通信。
> * 管道的创建：使用系统调用函数 ： int pipe(int fd[2])
> * 缺点：管道之间单向通信的效率太低了，不适合进程间频繁的交换数据
> * 所谓的管道就是**内核中的一串缓存**， 其实就是在内核中的一端写一端读数据
> * 在 shell 里面执行 A| B，A 进程和B进程都是 shell 创建的子进程，A 和 B之间不存在父子关系，他两的父进程都是 shell
> * 因为管道没有实体，也就没有管道文件，只能通过 fork 来复制父进程的 fd 来进行通信的目的
> * 管道中，进程写入的数据都是在内核中，另外一个进程读取的时候自然是从内核读取的。
> * 管道的生命周期是随着进程相关的

* 消息队列

> * 消息队列可以将**数据放在消息队列中，进程就可以返回做其他的事情了**。因此消息队列的效率更高的。
> * **消息队列是保存在内核中的消息链表**，链表连接的是一个个消息块，进程之间定义消息体的数据类型。
> * 消息队列的生命周期是**和内核相关的，不随进程相关**。
> * 缺点：通信不及时，同时传输的数据大小有限制。
> * 在内核中，对**每一个消息体的大小都有一定的限制，同时对消息队列的总长度也是有限制的**。
> * 消息队列的通信，存在进程在用户态的数据拷贝到内核态，接受的进程同样需要反向的操作

* 共享内存

> * 消息队列需要数据从用户态拷贝到内核，开销大
>
> * 共享内存不需要从用户态拷贝数据到内核态，使用的是**拿出一块虚拟地址空间，两个进程的这个虚拟空间映射到同一块物理内存**。
>
> * 这样这个进程写入的东西，另外一个进程马上就能看到了，都**不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度**。
>
> * 共享内存是一段在用户态空间的内存段，两个进程中的**虚拟空间的共享内存空间地址是不一样的**，但是虚拟地址通过内核的页表映射到真正的物理地址，**它们的物理地址空间一定是一样的**。所以可以共享同一块内存空间，这里的空间指的是物理地址空间
>
> * ```cpp
>   int write() {
>       // 1. 创建一个共享内存
>       int shmid = shmget(100, 4096, IPC_CREAT | 0777);
>       // 2. 和当前进程进行关联
>       void * addr = shmat(shmid, NULL, 0); // 内核指定其实的地址
>       // 3. 写数据
>       printf("%p\n", addr);
>       // 0x7fc214e92000, 这个地址是虚拟地址
>       char* ptr = "hello zhoubing";
>       memcpy((char* )addr, ptr, strlen(ptr));
>       // 注意这里需要暂停一下，避免删除共享内存，别的进程没法访问这个共享内存
>       printf("输入任何内容暂停操作\n");
>       getchar();
>       // 4. 解除关联
>       shmdt(addr);
>       // 5. 删除关联
>       shmctl(shmid, IPC_RMID, NULL);
>       return 0;
>   }
>   int read() {
>       // 1. 获取一个共享内存
>       int shmid = shmget(100, 0, IPC_CREAT);
>       // 2. 关联一个共享内存
>       void* addr = shmat(shmid, NULL, 0);
>       // 3. 读数据
>       printf("%p\n", addr);
>       // 0x7f42ec3bb000，这个地址是虚拟地址
>       printf("%s\n", (char*)addr);  // 打印出共享内存中的内容
>       // hello zhoubing
>       printf("输入任何字符终止程序执行\n");
>       getchar();
>       // 4. 解除关联
>       shmdt(addr);
>       // 5.删除共享内存
>       shmctl(shmid, IPC_RMID, NULL);
>   <<<<<<< HEAD
>
> =======
>
> >>>>>> 671182d7d0de493b784a7f696addacab3af28b2d
>       return 0;
>   }
>
>   ```
> 
> * 
> 
> * 这样都是在用户态的内存执行的操作，提高通信的速度，需要结合信号量一起使用，因为对共享内存的共享数据操作，避免出现数据错误

* 内存映射 & 共享内存

> ```c
> 共享内存和内存映射的区别
>     1.共享内存可以直接创建，内存映射需要磁盘文件（匿名映射除外）
>     2.共享内存效果更高
>     3.内存
>         所有的进程操作的是同一块共享内存。
>         内存映射，每个进程在自己的虚拟地址空间中有一个独立的内存。
>     4.数据安全
>         - 进程突然退出
>             共享内存还存在
>             内存映射区消失
>         - 运行进程的电脑死机，宕机了
>             数据存在在共享内存中，没有了
>             内存映射区的数据 ，由于磁盘文件中的数据还在，所以内存映射区的数据还存在。
> 
>     5.生命周期
>         - 内存映射区：进程退出，内存映射区销毁
>         - 共享内存：进程退出，共享内存还在，标记删除（所有的关联的进程数为0），或者关机
>             如果一个进程退出，会自动和共享内存进行取消关联。(但是不一定，共享内存就不在了，要是还有别的进程使用这个共享内存区，那么这个共享内存区还是会存在的)
> ```
>
> 

* 信号量

> * 信号量对共享资源的访问提供了保护，在任何一个时刻，只能有一个进程访问这个共享资源。
> * 信号量主要用在进程间的互斥和同步，而不是用于缓存进程的通信

* 信号

> * 以上都是常规状态下进程间的通信，对于**异常情况下的工作模式，需要使用信号的方式通信**
> * 硬件来源： Crtl + C 产生 SIGINT 信号，表示进程终止， 软件来源 ： kill 命令
> * 信号是进程间通信的**唯一的异常通信机制**，一旦存在这个信号，进程需要进行一定的处理:三种处理方式：1.执行默认操作。 2.捕获信号。 3. 忽略信号
> * 流程分为用户空间(User Space)和内核空间（Kernel Space)。从用户空间进入内核空间需要向内核发出syscall，用户空间的程序通过各种syscall来调用用内核空间相应的服务。系统调用是为了让用户空间的程序陷入内核，该陷入动作是由软中断来完成的。用户态的进程进行系统调用后，CPU切换到内核态，开始执行内核函数。`unistd.h`文件中定义了所有的系统中断号，用户态程序通过不同的系统调用号来调用不同的内核服务，通过系统调用号从系统调用表中查看到相应的内核服务。
> * 

* socket

> * 以上的方式都是同一台机器下不同进程的通信方式，使用 socket 可以跨网络在不同主机之间通信。（本地也可以使用socket）
>
> * ```c++
>   int socket(int domain, int type, int protocal) // 系统调用
>   ```
>
> * 分成三种通信方式： 1.TCP协议的通信方式。 2.UDP协议的通信。 3. 本地进程的通信
>
> * TCP 通信首先建立连接，后续再传输数据，之后使用 read 和 write 函数来读写数据，就像往一个文件流中写东西



#### 线程间通信的方式

* **不同进程间线程通信等同于进程间通信，同一进程间可用共享内存实现。**

* 同一个进程下的线程都是共享进程资源的，只要是共享资源都可以做到线程间通信，比如全局变量。
* 线程通信主要关注的是 **互斥和同步**，关注多线程对共享资源的竞争的情况。
* 互斥： 可以保证任意一个时刻只有一个线程访问共享资源
* 同步： 保证线程A是在线程B之前执行

* > * 锁机制：互斥锁、条件变量、读写锁
  > * 信号量机制（Semaphore）
  > * 信号处理 (Signal)



#### 32位 OS 装 8 GB 内存条可以使用吗？

##### 可执行文件到进程

* 这个可执行文件就会加载进**内存**中，成为一个**进程**，运行起来。
* 可执行文件里的机器码也会被加载到内存中，它就像是一张列满todo list的清单，而CPU就对照着这张清单，**一行行的执行上面的机器码**。从效果上来看，进程就动起来了。
* 对CPU来说，它执行到某个特定的编码数值，就会执行特定的操作。比如计算2+3，其实就是通过**总线**把数据2和3从**内存**里读入，然后放到**寄存器**上，再用**加法器相加这两个数值并将结果放入到寄存器里**，最后将这个数值回**写到内存中**，以此循环往复，一行行执行机器码直到退出。
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialeCVBYNfM6Znf0tmwKyPP7o72jnticF32mj4zbXSG0RlNRt95QoxuvTczlPjtQKZ3QRUvBLA1aSfQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



##### CPU 位数含义

* **CPU的寄存器**，说白了就是个存放数值的小盒子，盒子的大小，叫**位宽**。32位CPU能放入最大2^32的数值。64位就是最大2^64的值。这里的32位位宽的CPU就是我们常说的32位CPU，同理64位CPU也是一样。
* **CPU跟内存**之间，是用**总线**来进行信号传输的，总线可以分为**数据总线，控制总线，地址总线**。
* 比如现在有一行是将A地址的数据与B地址的数据相加，那么CPU就会通过**控制总线**，发送信号给内存这个设备，告诉它，现在CPU要通过**地址总线**在内存中找到**A数据的地址**，然后取得A数据的值，假设是100，那么这个100，就会通过**数据总线**回传到CPU的某个寄存器中。
* B也一样，假设B=200，放到另一个寄存器中，此时A和B相加后，结果是300，然后控制CPU通过**地址总线**找到返回的参数地址，再把数据结果通过**数据总线**传回内存中。这一存一取，CPU都是通过**控制总线**对内存发出指令的。
* ![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialeCVBYNfM6Znf0tmwKyPP7hWfH6JvkUM3WvHJ7ZeK0rKbuWAhgeZmDDK5yC4s9eVosUSrsibHOvSw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
* 而**总线，也可以理解为有个宽度**，比如宽度是32位，那么一次可以传32个0或1的信号，那么这个宽度能表达的数值范围就是0到2^32这么多。
* **32位CPU的总线宽度一般是32位**，因为刚刚上面提到了，CPU可以利用地址总线在**内存**中进行寻址操作，那么现在这根**地址总线**，最大能寻址的范围，也就到2^32，其实就是4G。
* **64位CPU，按理说总线宽度是64位，但实际上是48位**（也有看到说是40位或46位的，没关系，你知道它很大就行了），所以寻址范围能到2^48次方，也就是**256T**。





##### 系统和软件的位数含义

* 相应的操作系统，和软件（其实操作系统也能说是软件），也应该按CPU所能支持的范围去构建自己的寻址范围。
* 在操作系统上运行一个用户态进程，会分为用户态和内核态，并设定一定的内存布局。操作系统和软件都需要以这个内存布局为基础运行程序。比如32位，内核态分配了1个G，用户态分配了3G，这种时候，你总不能将程序的运行内存边界设定在大于10G的地方。所以，**系统和软件的位数，可以理解为，这个系统或软件内存寻址的范围位数**。
* **![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIialeCVBYNfM6Znf0tmwKyPP71ZOc7rqic6lL1DWCn9naTKRhj3ib6r01ScMOInJY8Ic1QLm5rOrKSlYw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)**
* 由于现在我们的CPU架构在设计上都是**完全向前兼容**的，别说32位了，16位的都还兼容着，因此**64位的CPU是能装上32位操作系统的**。
* 同理，**64位的操作系统是兼容32位的软件的**，所以32位软件能装在64位系统上。
* 因为32位操作系统只支持4g的内存，而64位的软件在编译的时候就设定自己的内存边界不止4个G，并且64位的CPU指令集内容比32位的要多，所以32位操作系统是肯定不能运行64位软件的。
* 同理，32位CPU也不能装64位的操作系统的。



###### 32 CPU 可以执行 int64 位数值计算吗

* 能
* 但比起64位的CPU，**性能会慢一些**。
* 64位 CPU ： 那么我在计算两个int64的**数值**相加时，我就能将数据通过64位的**总线**，一次性存入到64位的**寄存器**，并在进行计算后返回到内存中。整个过程一步到位，一气呵成。
* 32 位 CPU： 那就憋屈一点了，我虽然在代码里放了个int64的数值，但实际上CPU的寄存器根本放不下这么大的数据，因此最简单的方法是，将int64的数值，拆成前后两半，**现在两个int64相加，就变成了4个int32的数值相加**，并且后半部分加好了之后，拿到**进位，才能去计算前面的部分**，这里光是执行的指令数就比64位的CPU要多。所以理论上，会更慢些。



###### 32 位 CPU 和 OS 可以插 8G 内存条吗

* 系统能**正常工作**，但**一般用不到8G**
* 因为32位系统的总线寻址能力为2的32次方，也就是4G，**哪怕装了8G的内存，真正能被用到的其实只有4g，多少有点浪费。**

* 所以，**你当32位系统最大只能用到4G内存，那也没毛病。**



###### 32位 CPU & 64位 OS 寻址能力 4G 吗

* **是的**
* 寻址能力，除了受到cpu的限制外，还受到操作系统的限制，如果操作系统就是按着32位的指令和寻址范围（4G）来编译的话，那么它就会缺少64位系统该有的指令，它在运行软件的时候就不能做到超过这个限制，因此**寻址能力还会是4G。**



##### 总结

- **CPU位数主要指的是寄存器的位宽**，
- 32位CPU只能装32位的系统和软件，且能计算int64，int32的数值。内存寻址范围是4G。
- 64位CPU，同时兼容32位和64位的系统和软件，并且进行int64数值计算的时候，性能比32位CPU更好，内存寻址范围可以达到256T。
- 32位CPU和操作系统，插入8G的内存，会有点浪费，因为总线寻址范围比较有限，它只能用上4G不到的内存。
- 64位CPU，如果装上32位的操作系统，就算插上8G的内存，效果也还是只能用上4G不到的内存。













#### 内存管理

* **操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

* 虚拟地址：每一个进程都有，大家自己管自己的就行，具体如何映射到物理内存中，os 负责这些事
* 进程持有的虚拟地址会通过 CPU 芯片中的 内存管理单元 (MMU) 的映射关系，将其变成物理地址,之后再通过物理地址访问内存.
* os 如何管理虚拟地址和物理地址之间的关系呢？

> * 内存分段
>
>   > * 段选择子以及段内偏移
>   >
>   > * 段选择子：保存在段寄存器中，段选择子的某一个字段是段号，主要是用作段表的索引，段表里面保存的是段的基地址、段的界限和特权等级等
>   >
>   > * 段内偏移量加上端的基地址就能得到最终的物理地址
>   >
>   > * 缺点：
>   >
>   >   > * 内存碎片： 外部内存碎片：就是指产生多个不连续的小物理内存，导致新的程序无法装进来，没放充分利用多个小的物理内存，内部内存碎片： 程序装入到内存中，但是这部分内存不经常使用，造成内存浪费
>   >   > * 内存交换效率低 ： 我们将部分程序放入到磁盘中，重新组织内存，这样会带来效率问题。
>
> * 内存分页
>
>   > * 在 linux 操作系统下，每一个页面是 4KB，将内存空间分成一个个固定大小的页面
>   > * 可能出现缺页异常，这个时候需要进入内核空间分配内存，更新进程页表，最后在返回到用户空间，恢复进程的运行。缺页可能需要缺页置换算法，LRU FIFO LFU 之类的算法
>   > * 分页不会出现页外的内存碎片，但是可能出现业内的碎片，但是比较小。
>   > * 每一页分成页号和页内偏移，页号就是查找在页表中的索引，第一个页表位置是存储在寄存器中的。拿出物理页号加上偏移量最终就是我们需要的物理内存地址
>   > * 多级页表，可以减少页表占用内存的空间大小，同时符合计算机的局部性原理。
>
> * 段页式内存管理
>
>   > * ![img](https://img-blog.csdnimg.cn/8904fb89ae0c49c4b0f2f7b5a0a7b099.png)
>   > * 地址结构就由**段号、段内页号和页内位移**三部分组成。
>   > * 先将程序划分成多个有逻辑意义的段
>   > * 接着将每个段分成多个页，即对分段出来的连续空间再次划分成多个页面,大小固定
>   > * 需要三次访问内存：第一次访问段表，第二次访问页表，第三次：将物理页号和页内偏移组合，得到物理地址





##### 虚拟内存

* 操作系统为每个进程分配独立的一套「**虚拟地址**」，人人都有，大家自己玩自己的地址就行，互不干涉
* 进程没法访问物理内存，最终**落到物理内存的什么地方是有 OS 决定的**，对进程透明
* OS  如何做内存映射的？
* Q : 进程持有的虚拟地址会**通过 CPU 芯片中的内存管理单元（MMU）的映射关系**，来**转换变成物理地址**，然后再通过物理地址访问内存.
* <img src="https://img-blog.csdnimg.cn/72ab76ba697e470b8ceb14d5fc5688d9.png" alt="img" style="zoom:70%;" />
* Q:  操作系统是如何管理虚拟地址与物理地址之间的关系？
* A : 分页和分段

##### 内存分段

* 分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。



![img](https://img-blog.csdnimg.cn/a9ed979e2ed8414f9828767592aadc21.png)

问题：

- 第一个就是**内存碎片**的问题。
- 第二个就是**内存交换的效率低**的问题。

解决外部内存碎片的问题就是**内存交换**：

* 在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。
* 因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上
* **如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**



##### 内存分页

![img](https://img-blog.csdnimg.cn/08a8e315fedc4a858060db5cb4a654af.png)

* **页表是存储在内存里**的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的工作。

* 由于**每个进程都有自己的页表**，所以每个进程的虚拟内存空间就是相互独立的。**进程也没有办法访问其他进程的页表**，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。

* 当进程**访问的虚拟地址在页表中查不到**时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。

* ![img](https://img-blog.csdnimg.cn/7884f4d8db4949f7a5bb4bbd0f452609.png)

* Q : 简单的分页有什么缺陷吗？

* A：有空间上的缺陷。

  因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。 每一个进程 4MB 页表，那么 100 个进程需要 400 MB 的页表空间



* 多级页表的好处？

* > * 使用**局部性原理**
  > * 但**如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表**。
  > * 一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建

* 局部性原则的另外一个应用：

* > * 程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。
  > * 把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序**最常访问的页表项的 Cache**，这个 Cache 就是 TLB(注意 TLB 中存放的是页表项)





##### Linux 内存管理

* **Linux 内存主要采用的是页式内存管理，但同时也不可避免地涉及了段机制**。
* 虽然每个进程都各自有独立的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。
* ![img](https://img-blog.csdnimg.cn/48403193b7354e618bf336892886bcff.png)
* 不同进程共享同一个 内核空间，因为内核地址空间和物理地址是一样的。
* ![虚拟内存空间划分](https://img-blog.csdnimg.cn/img_convert/b4f882b9447760ce5321de109276ec23.png)
* 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（[跟硬件和内核版本有关 (opens new window)](http://lishiwen4.github.io/linux/linux-process-memory-location)）；
* **堆和文件映射段的内存是动态分配的**。比如说，使用 C 标准库的 `malloc()` 或者 `mmap()` ，就可以分别在堆和文件映射段动态分配内存。
* **已初始化数据段包含 (.data 和 .rodata 这两部分)**

* 全局数据区(静态区) (static)，**全局变量和静态变量的存储是放在一**块的，**初始化的全局变量和静态变量在一块区域**，**未初始化的全局变量和未初始化的静态变量在相邻的另一块区域**。另外文字常量区，常量字符串就是放在这里，**程序结束后有系统释放**。

```cpp
int a = 0;                   //全局初始化区
char *p1;                //全局未初始化区 


int main()
{
        int b;                // 栈
        char s[] = "abc";       //栈
        char *p2;             //栈
        char *p3 = "123456";    //123456\0在常量区(已初始化数据区），而p3在栈上

        static int c =0；    //全局（静态）初始化区 

        p1 = (char *)malloc(10);

        p2 = (char *)malloc(20); //分配得来得10和20字节的区域就在堆区

        strcpy(p1, "123456");    //123456\0放在常量区，编译器可能会将它与p3所指向的"123456"优化成一个地方。

        return 0;
}
```







#### Malloc 地址分配

* **malloc() 并不是系统调用 **(但是其进行分配内存的时候需要用到操作系统申请内存），而是 C 库里的函数，用于动态分配内存。

* malloc 申请内存的过程：

* > - 方式一：通过 brk() 系统调用从堆分配内存
  > - 方式二：通过 mmap() 系统调用在文件映射区域分配内存；

* brk 方式： 

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/brk%E7%94%B3%E8%AF%B7.png)

* mmap 方式： 在**文件映射区分配一块内存**，也就是从文件映射区“偷”了一块内存。如下图：

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/mmap%E7%94%B3%E8%AF%B7.png)

* 使用 brk 和 mmap 系统调用的时机：

* > - 如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；
  > - 如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；

* Q : malloc 分配的是物理内存吗？

* A： 不是的，是虚拟内存

* 如果分配后的虚**拟内存没有被访问的话，是不会将虚拟内存不会映射到物理内存**，这样就不会占用物理内存了。映射的方式是采用页表进行地址转换，可能出现缺页中断，然后**操作系统会建立虚拟内存和物理内存之间的映射关系**。



##### malloc(1) 分配多大内存

* malloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是**会预分配更大的空间作为内存池**。

* 具体会预分配多大的空间，**跟 malloc 使用的内存管理器有关系**，eg : linux 默认的内存管理器 Ptmalloc2

* malloc 申请的内存，free 释放内存会归还给操作系统吗？

*  (brk() 分配内存较少的时候不会，但是分配内存大了还是会释放申请的内存)

* > - malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；
  > - malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。

* 原因：

* > * 这是因为与其把这 1 字节释放给操作系统，不如先缓存着放进 malloc 的内存池里，当进程再次申请 1 字节的内存时就可以直接复用，这样速度快了很多。(内存池在用户态，不需要系统调用，分配速度加快)
  > * 申请内存的操作应该避免频繁的系统调用，如果都用 mmap 来分配内存，等于每次都要执行系统调用。
  > * 因为 mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次 mmap 分配的虚拟地址都是缺页状态的，然后在**第一次访问该虚拟地址的时候**，就会触发缺页中断。
  > * **频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**。

* 好的解决方案：内存池（小内存申请效率高，因为都是在内存中）

* > * malloc 通过 **brk() 系统调用**在堆空间申请内存的时候，由于**堆空间是连续的**，所以**直接预分配更大的内存来作为内存池**，当内存释放的时候，就缓存在内存池中。
  > * **等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗**

* 



##### free 如何知道需要释放的长度

* 一字节的内存起始地址是： 98f84010， 这个 98f84000 是分配的起始地址，后面 0x10 16个字节是存储的内存块的头信息
* ![图片](https://img-blog.csdnimg.cn/img_convert/cb6e3ce4532ff0a6bfd60fe3e52a806e.png)
* 多出来的 16 字节就是保存了该内存块的描述信息，比如有该内存块的大小。
* 这样当执行 free() 函数时，**free 会对传入进来的内存地址向左偏移 16 字节**，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。



#### 内存满了如何处理？ （LRU 算法）

* 如果没有空闲的物理内存，那么内核就会开始进行**回收内存**的工作（这个是内核做的）

* 直接内存回收和后台内存回收。

* > * **后台内存回收**（kswapd）：在物理内存紧张的时候，会唤醒 **kswapd 内核线程**来回收内存，这个回收内存的过程**异步**的，不会阻塞进程的执行。
  > * **直接内存回收**（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是**同步**的，会阻塞进程的执行。
  > * 如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了 ——**触发 OOM （Out of Memory）机制**。
  > * OOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源，如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置。



##### 哪些数据可以回收

* > * **文件页**（File-backed Page）: **内核缓存**的磁盘数据（Buffer）和内核缓存的文件数据（Cache）
  > * **回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存**。
  > * **匿名页**（Anonymous Page）: 如堆、栈数据（在用户态的数据）， 这部分内存很可能还要再次被访问，所以不能直接释放内存，它们**回收的方式是通过 Linux 的 Swap 机制**



##### 回收算法

###### LRU

> - **active_list** 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；
> - **inactive_list** 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；

* 优先回收不活跃的内存。





##### 回收带来的性能问题

* 直接内存回收：这种方式是**同步回收**的，会**阻塞进程**。就会造成很长**时间的延迟**，以及系统的 **CPU 利用率会升高**，最终引起系统负荷飙高。
* 文件页的回收：对于**干净页是直接释放内存**，这个操作不会影响性能，而对于**脏页会先写回到磁盘再释放内存**，这个操作会发生磁盘 I/O 的，这个操作是会影响系统性能的。（可能会影响性能）
* 匿名页的回收：如果**开启了 Swap 机制**，那么 Swap 机制会将不常访问的匿名页换出到磁盘中，下次访问时，再从磁盘换入到内存中，这个操作是会**影响系统性能**的。（总是会影响性能的）



##### 什么条件下才能触发 kswapd 内核线程回收内存呢？

内核定义了三个内存阈值（watermark，也称为水位），用来衡量当前剩余内存（pages_free）是否充裕或者紧张，分别是：

- 页最小阈值（pages_min）；
- 页低阈值（pages_low）；
- 页高阈值（pages_high）；

<img src="https://img-blog.csdnimg.cn/166bc9f5b7c545d89f1e36ab8dd772cf.png" alt="img" style="zoom:67%;" />

* 图中橙色部分：如果剩余内存（pages_free）在页低阈值（pages_low）和页最小阈值（pages_min）之间，说明内存压力比较大，剩余内存不多了。**这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值（pages_high）为止**。虽然会触发内存回收，但是不会阻塞应用程序，因为两者关系是异步的。
* 图中红色部分：如果剩余内存（pages_free）小于页最小阈值（pages_min），说明用户可用内存都耗尽了，此时就会**触发直接内存回收**，这时应用程序就会被阻塞，因为**两者关系是同步的**。



##### OOM 如何杀掉进程

* OOM : 内存溢出(Out Of Memory，简称OOM)是指应用系统中存在无法回收的内存或使用的内存过多，最终使得**程序运行要用到的内存大于能提供的最大内存**。此时程序就运行不了，系统会**提示内存溢出** 

* Linux 内核里有一个 `oom_badness()` 函数，它会把系统中可以被杀掉的进程扫描一遍，并对每个进程打分，得分最高的进程就会被首先杀掉。

* ```c
  // points 代表打分的结果
  // process_pages 代表进程已经使用的物理内存页面数
  // oom_score_adj 代表 OOM 校准值
  // totalpages 代表系统总的可用页面数
  points = process_pages + oom_score_adj*totalpages/1000
  ```

* 每个进程的 oom_score_adj 默认值都为 0，所以最终得分跟进程自身消耗的内存有关，**消耗的内存越大越容易被杀掉**。

* 







#### 可以申请 1T 的内存？

* 32 位操作系统不行，因为虚拟内存大小就是 4G （用户态内存大小是 3G）
* 64 位操作系统是可以的。因为 用户态可以最多申请 128 T 的内存. (程序申请的虚拟内存，如果没有被使用，它是不会占用物理空间的。当访问这块虚拟内存后，操作系统才会进行物理内存分配。)

##### 缺页中断的过程

* 当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU （MMU）就会产生**缺页中断**，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。

* 缺页中断处理函数会看是否有空闲的物理内存：

* > - 如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系。
  > - 如果没有空闲的物理内存，那么内核就会开始进行[回收内存 (opens new window)](#内存满了如何处理？ （LRU 算法）)的工作，如果回收内存工作结束后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了触发 OOM （Out of Memory）机制。

如果申请物理内存大小超过了空闲物理内存大小，就要看操作系统有没有开启 Swap 机制：

> - 如果没有开启 Swap 机制，程序就会直接 OOM；
> - 如果有开启 Swap 机制，程序可以正常运行。







#### 执行 ./a.out OS 发生了什么？













 









#### 进程 & 线程

* 我们编写的代码，只不过是一个存储在硬盘中的静态文件，通过编译后我们会得到一个二进制的可执行文件，当我们运行这个可执行文件后，就会将其装载到内存中，这个时候CPU执行这个二进制的文件，就是指令，那么这个运行中的程序就是进程
* 操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源，当子进程被终止时，其在父进程处继承的资源应当还给父进程。同时，**终止父进程时同时也会终止其所有的子进程**。在 shell 中，shell 就是一个父进程，我们运行一个 python 程序就是一个子进程.
* PCB：

> * 进程描述符：标志各个进程，每一个进程都有一个并且唯一的标识符
> * 用户标识符： 进程归属的用户，用户标识符主要为共享和保护服务
> * 进程当前的状态： new、 ready 、 running 、 waiting 等
> * 进程优先级： 用于抢占 CPU 时的优先级
> * 资源分配清单： 有关内存的地址空间或者虚拟地址空间的信息，所有打开文件 的列表和所使用的 I/O设备信息
> * CPU 相关信息：CPU 中各个寄存器的值，当进程被切换的时候， CPU的状态信息保存在相应的PCB中，以便进程重新执行的时候，能从段点处继续执行



* 每个 PCB 如何组织的

![img](https://pic1.zhimg.com/80/v2-52bef604640dac2e7975a4ff0efe4d5a_720w.jpg?source=d16d100b)

> * 通常采用的是链表的方式组织的，把具有相同状态的链表放在一起，组成各种队列
> * 索引的方式，将相同类型的进程组织在一个索引表中。



* CPU 上下文切换

> * os 中有很多个任务(进程、线程、中断），任务需要交给 CPU 运行，在每个任务运行前，CPU需要知道任务从哪加载，从哪开始运行。
> * os 需要事先帮 CPU 设置好 CPU 寄存器和程序计数器
> * CPU 寄存器就是 CPU 的缓存
> * 程序计数器就是用来存储 CPU 正在执行的指令的位置，或者即将执行的下一条指令的位置.
> * CPU 寄存器和程序计数器就是CPU运行前所必须准备好的环境，就是 CPU 的上下文
> * CPU 上下文切换，需要保存这些信息，保存在什么地方：内核栈中，当CPU 再次运行到这个任务的时候，在从内核栈中弹出这些消息，保证任务运行连续的。





* 进程的上下文切换

> * 一个进程切换到另外一个进程就是进程的上下文切换
> * **进程是由内核管理的，所以进程切换一定是发生在内核态的**
> * **进程上下文包括： 用户空间的：虚拟内存、栈、全局变量等，内核空间：内核栈、寄存器等**
> * 通常需要将交换的信息保存到进程的 PCB 中，当运行另外一个进程的时候，从这个进程的 PCB中取出上下文，然后恢复到 CPU中，使得这个进程可以继续执行.
> * 进程切换代价非常的大



* 进程切换的场景

> * CPU 时间片用完了，需要切换进程
> * 系统资源不足，进程挂起
> * 优先级更高的进程出现，需要切换出当前的进程
> * 发生硬件中断，CPU 上的进程被中断挂起，进而执行内核中的中断服务



* 线程

![img](https://pica.zhimg.com/80/v2-a4048a717466b3762c19cd38696b81ff_720w.jpg?source=d16d100b)

> * 线程优点：
>
>   > * 各个线程之间可以并发执行
>   > * 各个线程之间可以共享地址空间和文件资源
>
> * 缺点
>
>   > * 当进程的一个线程崩溃了，会导致整个进程的所有其他线程崩溃



* 线程上下文切换

> * 线程也拥有自己的私有数据，比如寄存器栈，这些在线程上下文切换都是需要保存的.
> * 当两个线程不属于通一个进程，那么线程上下文切换和进程是一样的麻烦。
> * 当同一个进程下的线程只需要切换私有的数据。
> * 线程切换开销非常的小



* 用户级线程

![img](https://pic2.zhimg.com/80/v2-53f7808f0ac16b7669ffc4f46f23904a_720w.jpg?source=d16d100b)

> * 优点：
>
>   > * TCB 存在用户态中，因为是由用户级线程函数来维护的，操作系统是看不见的。
>   > * 用户线程的整个线程的管理和调度，操作系统都是不直接参与的，而是由用户级线程库函数来完成的。
>   > * 用户级线程的切换不需要用户态和内核态的切换，速度非常的快
>
> * 缺点：
>
>   > * os 不参与线程的调度，如果一个线程发起系统调用阻塞，那么所有线程均阻塞了
>   > * os 无法管理用户级线程，所以 当一个线程执行了 占有CPU，除非这个线程自己主动交出 CPU 使用权，否则没办法打断当前的线程





* 内核级线程

![img](https://pic1.zhimg.com/80/v2-e97d0249f25cfcd3aa321ae2f93a8115_720w.jpg?source=d16d100b)





> * 优点：
>
>   > * 由 os 进行管理的，线程的创建、终止和管理都是由 os 负责的。
>   > * 在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行
>   > * 分配给线程，多线程的进程获得更多的 CPU 运行时间
>
> * 缺点：
>
>   > * 当线程创建终止管理需要通过系统调用的方式实现，这样开销比较大。
>   > * 通过内核来维护进程和线程的上下文，例如 PCB 和 TCB





#### 键入A， OS发生了什么

* 中断

* CPU 是通过设备控制器和设备打交道的，每一个设备都存在一个设备控制器.

* 每一个设备控制器都**有芯片，以及寄存器**，可以和CPU进行通信操作。

* 通过**写入这些设备寄存器**，**OS** 可以命令设备发送数据，接收数据，开启或者关闭。

* 通过从这些**设备寄存器读取数据**，**OS** 可以了解设备的状态，以判断其是否准备好接收一个新的命令

* ![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E8%AE%BE%E5%A4%87%E6%8E%A7%E5%88%B6%E5%99%A8.png)

* 设备控制器的寄存器：（CPU 通过**读写设备控制器中的寄存器** 来控制 I/O 寄存器）

	> * 数据寄存器：CPU 向 I/O 设备**写入需要传输的数据**，并入打印内容：Hello，CPU 就需要发送一个字符 H 给对应的 I/O 设备
	> * 命令寄存器：CPU 向 I/O 设备发送一个**命令**，例如需要进行的是：输入输出操作，于是交给 I/O 设备完成相应的工作
	> * 状态寄存器：目的是告诉 CPU ，**当前 I/O 设备的状态**，如果是正在工作，那么传过来命令也是没用的，只有等工作完成了，才可以再次传输数据过来。

* CPU 通过**读写设备控制器中的寄存器控制设备**，这可比 CPU 直接控制输入输出设备，要方便和标准很多。

* 输入输出设备可分为两大类 ：**块设备（Block Device）**和**字符设备（Character Device）**。

* > * *块设备*，把**数据存储在固定大小的块中**，每个块有自己的地址，硬盘、USB 是常见的块设备。(有自己的存储空间)
  > * *字符设备*，**以字符为单位发送或接收一个字符流**，字符设备是不可寻址的，也没有任何寻道操作，鼠标是常见的字符设备。

* 块设备通常传输的数据量会非常大，于是控制器设立了一个可读写的**数据缓冲区**。

* **数据会先发送到数据缓冲区**，等缓冲区中囤够了一部分，才发送给设备或者拷贝到内存中。目的是为了减少设备的频繁操作

* 那 CPU 是如何与设备的控制寄存器和数据缓冲区进行通信的？存在两个方法：

* > - *端口 I/O*，每个控制寄存器被分配一个 I/O 端口，可以通过特殊的汇编指令操作这些寄存器，比如 `in/out` 类似的指令。
  > - *内存映射 I/O*，将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区。



##### I/O 控制方式

* 方法一： **CPU轮训**， 控制器的寄存器一般会有状态标记位，用来标识输入或输出操作是否完成。让 CPU 一直查寄存器的状态，直到状态标记为完成，它会占用 CPU 的全部时间。（通俗理解： CPU每隔一小段时间取询问键盘是否存在键被按下.）

* 方法二： **中断**，通知操作系统数据已经准备好了。我们一般会有一个硬件的**中断控制器**，当设备完成任务后触发中断到中断控制器，**中断控制器就通知 CPU，一个中断产生了，CPU 需要停下当前手里的事情来处理中断**。

* 中断分类：

* > * 一种**软中断**，例如代码调用 `INT` 指令触发
  > * 一种是**硬件中断**，就是硬件通过中断控制器触发的。

* 但中断的方式对于频繁读写数据的磁盘，并不友好，这样 CPU 容易经常被打断，会占用 CPU 大量的时间。(敲键盘可能不太适合使用中断)

* 解决方式 ： DMA（Direct Memory Access） **它可以使得设备在 CPU 不参与的情况下，能够自行完成把设备 I/O 数据放入到内存**。那要实现 DMA 功能要有 「**DMA 控制器**」硬件的支持。

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/DMA%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.png)

* DMA 基本流程：

* > * CPU 需对 DMA 控制器下发指令，**告诉它想读取多少数据，读完的数据放在内存的某个地方就可以了**；
  > * 接下来，DMA 控制器会向磁盘控制器发出指令，**通知**它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存；
  > * 当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器；
  > * DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，**CPU 就可以直接用内存里面现成的数据了**；

* CPU 当要读取磁盘数据的时候，只需给 DMA 控制器发送指令，然后返回去做其他事情，当磁盘数据拷贝到内存后，DMA 控制机器通过中断的方式，告诉 CPU 数据已经准备好了，可以从内存读数据了。**仅仅在传送开始和结束时需要 CPU 干预**。(剩下使用 DMA 来处理)

*  



##### 设备驱动程序

* 虽然**设备控制器**屏蔽了设备的众多细节，但每种设备的控制器的**寄存器、缓冲区等使用模式都是不同的**，所以为了**屏蔽「设备控制器」的差异**，引入了**设备驱动程序**。

* **设备控制器不属于操作系统范畴，它是属于硬件**，而**设备驱动程序属于操作系统的一部分**，操作系统的内核代码可以像本地调用代码一样使用设备驱动程序的接口，而**设备驱动程序是面向设备控制器的代码**，它发出操控设备控制器的指令后，才可以操作设备控制器。

* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F.png" alt="img" style="zoom:70%;" />

* 不同的设备控制器虽然功能不同，但是**设备驱动程序会提供统一的接口给操作系统**，这样不同的设备驱动程序，就可以以相同的方式接入操作系统。

* 前面提到了不少关于中断的事情，设备完成了事情，则会发送中断来通知操作系统。那**操作系统就需要有一个地方来处理这个中断，这个地方也就是在设备驱动程序里**，它会**及时响应控制器发来的中断请求**，并根据这个中断的类型调用相应的**中断处理程序**进行处理。

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E4%B8%AD%E6%96%AD%E5%B7%A5%E4%BD%9C%E8%BF%87%E7%A8%8B.png)

* 中断处理程序的处理流程：

* > 1. 在 I/O 时，设备控制器如果已经准备好数据，则会通过中断控制器向 CPU 发送中断请求；
  > 2. 保护被中断进程的 CPU 上下文；
  > 3. 转入相应的设备中断处理函数；
  > 4. 进行中断处理；
  > 5. 恢复被中断进程的上下文；

* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/CPU%20%E7%A1%AC%E4%BB%B6%E6%80%BB%E7%BA%BF%E5%9B%BE.png" alt="CPU 的硬件架构图" style="zoom:67%;" />

* C**PU 里面的内存接口，直接和系统总线通信，然后系统总线再接入一个 I/O 桥接器**，这个 I/O 桥接器，另一边接入了内存总线，使得 **CPU 和内存通信**。再另一边，又接入了一个 I/O 总线，用来连接 I/O 设备，比如键盘、显示器等。

* 键入一个字符做了哪些事情？

* > * **键盘控制器**就会产生扫描码数据，并将其缓冲在**键盘控制器的寄存器中**
  > * 键盘控制器通过总线给 CPU 发送**中断请求**。
  > * CPU 收到中断请求后，操作系统会**保存被中断进程的 CPU 上下文**
  > * 然后调用键盘的**中断处理程序**。
  > * 键盘的中断处理程序是在**键盘驱动程序**初始化时注册的，那键盘**中断处理函数**的功能就是从键盘控制器的寄存器的缓冲区读取扫描码，再根据扫描码找到用户在键盘输入的字符，如果输入的字符是显示字符，那就会把扫描码翻译成对应显示字符的 ASCII 码，比如用户在键盘输入的是字母 A，是显示字符，于是就会把扫描码翻译成 A 字符的 ASCII 码。
  > * 得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲区队列」的数据一个一个写入到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕里。
  > * 显示出结果后，**恢复被中断进程的上下文**。









##### 存储系统 I/O 软件分层

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/I_O%E8%BD%AF%E4%BB%B6%E5%88%86%E5%B1%82.png)

* 这三个层的作用： 

* > - 文件系统层，包括虚拟文件系统和其他文件系统的具体实现，它**向上为应用程序统一提供了标准的文件访问接口**，**向下会通过通用块层来存储和管理磁盘数据**。
  > - 通用块层，包括块设备的 I/O 队列和 I/O 调度器，它会对文件系统的 I/O 请求进行排队，再通过 I/O 调度器，选择一个 I/O 发给下一层的设备层。
  > - 设备层，包括硬件设备、设备控制器和驱动程序，负责最终物理设备的 I/O 操作。

* 存储系统的 I/O 是整个系统最慢的一个环节，所以 Linux 提供了不少缓存机制来提高 I/O 的效率。

* > - 为了提高文件访问的效率，会使用**页缓存、索引节点缓存、目录项缓存**等多种缓存机制，目的是为了减少对块设备的直接调用。
  > - 为了提高块设备的访问效率， 会使用**缓冲区**，来缓存块设备的数据。





#### CPU & 存储

##### 寄存器

* 存储器的数量通常在几十到几百之间，每个寄存器可以用来存储一定的字节（byte）的数据。

* > - 32 位 CPU 中大多数寄存器可以存储 `4` 个字节；
  > - 64 位 CPU 中大多数寄存器可以存储 `8` 个字节。

* 寄存器的访问速度非常快，一般要求在**半个 CPU 时钟周期内完成读写**，CPU 时钟周期跟 CPU 主频息息相关，比如 2 GHz 主频的 CPU，那么它的时钟周期就是 1/2G，也就是 0.5ns（纳秒）。



##### CPU Cache

* SRAM 芯片
* 在 SRAM 里面，**一个 bit 的数据，通常需要 6 个晶体管**，所以 **SRAM 的存储密度不高**，同样的物理空间下，能存储的数据是有限的，不过也**因为 SRAM 的电路简单，所以访问速度非常快**。
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/CPU-Cache.png)

###### L1 高速缓存

**每一个 CPU 核心有一个 L1 cache**

* L1 高速缓存的访问速度几乎和寄存器一样快，通常**只需要 `2~4` 个时钟周期**，而**大小在几十 KB 到几百 KB 不等**。
* 组成： 指令缓存和数据缓存
* Linux 指令缓存和数据缓存大小都是 **32KB**



###### L2 高速缓存

**每一个 CPU  核心都有一个 L2 cache**

* 速度更慢一点： **速度在 `10~20` 个时钟周期。**
* 大小： **几百 KB 到几 MB 不等**
* Linux 默认是： 256KB



###### L3 高速缓存

**L3 高速缓存通常是多个 CPU 核心共用的**

* 速度更慢一点： **速度在 `20~60` 个时钟周期。**
* 大小： **几 MB 到几十 MB 不等**



##### 内存

* DRAM 
* 相比 SRAM，DRAM 的密度更高，功耗更低，有更大的容量，而且造价比 SRAM 芯片便宜很多。
* DRAM 存储一个 bit 数据，只需要**一个晶体管和一个电容**就能存储
* 被存储在电容里，电容会不断漏电，所以需要**「定时刷新」电容**，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，**只有不断刷新，数据才能被存储起来**。
* DRAM 的**数据访问电路和刷新电路都比 SRAM 更复杂**，所以访问的速度会更慢，内存速度大概在 `200~300` 个 时钟周期之间



##### SSD

* 内存的读写速度比 SSD 大概快 `10~1000` 倍。
* 内存的优点是断电后数据还是存在的，而内存、寄存器、高速缓存断电后数据都会丢失。
* 



##### CPU 取数据逻辑

* 当 CPU 需要访问内存中某个数据的时候，如果寄存器有这个数据，CPU 就直接从寄存器取数据即可，如果寄存器没有这个数据，CPU 就会查询 L1 高速缓存，如果 L1 没有，则查询 L2 高速缓存，L2 还是没有的话就查询 L3 高速缓存，L3 依然没有的话，才去内存中取数据。
* 一层一层递归向下，直到到达内存，因为 CPU 可以对内存进行寻址操作.



##### CPU 缓存命中率

* 在单核 CPU，虽然只能执行一个线程，但是操作系统给每个线程分配了一个时间片，时间片用完了，就调度下一个线程，于是**各个线程就按时间片交替地占用 CPU**，从宏观上看起来各个线程同时在执行。
* 上述可能造成线程切换，数据的缓存没法命中，CPU 需要再去内存访问数据。
* 而现代 CPU 都是多核心的，线程可能在不同 CPU 核心来回切换执行，这对 CPU Cache 不是有利的，虽然 L3 Cache 是多核心之间共享的，但是 L1 和 L2 Cache 都是每个核心独有的。
* **如果一个线程在不同核心来回切换，各个核心的缓存命中率就会受到影响**
* 如果线程都在同一个核心上执行，那么其数据的 L1 和 L2 Cache 的缓存命中率可以得到有效提高，缓存命中率高就意味着 CPU 可以**减少访问 内存的频率。**
* 可以把**线程绑定在某一个 CPU 核心上**，这样性能可以得到非常可观的提升
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/sched_setaffinity.png)





#### CPU 缓存一致性

* 那么如果数据写入 Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不一致了，于是我们肯定是要把 Cache 中的数据同步到内存里的。
* 写的时机非常的重要！！！

##### 写直达

* 保持内存与 Cache 一致性最简单的方式是，**把数据同时写入内存和 Cache 中**，这种方法称为**写直达**

* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E7%9B%B4%E8%BE%BE.png" alt="img" style="zoom:50%;" />

* > - 如果数据已经在 Cache 里面，先将数据更新到 Cache 里面，再写入到内存里面；
  > - 如果数据没有在 Cache 里面，就直接把数据更新到内存里面。

* 不足： 每次都会写入到内存，性能会受到一定的影响



##### 写回

* 写直达由于**每次写操作都会把数据写回到内存**，而导致影响性能，于是为了要减少数据写回内存的频率，就出现了**写回（\*Write Back\*）的方法**。
* 写回机制中，**当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中**
* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E5%9B%9E1.png" alt="img" style="zoom:67%;" />
* 在把数据写入到 Cache 的时候，**只有在缓存不命中**，**同时数据对应的 Cache 中的 Cache Block 为脏标记的情况下**，才会将数据写到内存中，而在缓存命中的情况下，则在写入后 Cache 后，只需把该数据对应的 Cache Block 标记为脏即可，而不用写到内存里。（类似快照技术一样，只用保存最新的数据）
* Q: 为什么缓存没命中时，还要定位 cache block？
* A : 这个 cache block 是接下来数据需要写入的位置，当然需要先定位一下，判断是否存在数据，存在写回内存中。
* 此时是要判断数据即将写入到 cache block 里的位置，是否被「其他数据」占用了此位置，**如果这个「其他数据」是脏数据，那么就要帮忙把它写回到内存**。



##### 缓存一致性问题

* 现在 CPU 都是多核的，由于 L1/L2 Cache 是多个核心各自独有的，那么会带来多核心的**缓存一致性**

* 因为写回策略的问题，导致缓存不一致。

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98%E4%BE%8B%E5%AD%902.png)

* A 核心修改 i= 1，（写回策略）没有写入内存，但是 i 已经变了，这个时候内存没变，如果这时旁边的 B 号核心尝试从内存读取 i 变量的值，则读到的将会是错误的值，因为刚才 A 号核心更新 i 值还没写入到内存中，内存中的值还依然是 0。

* **所谓的缓存一致性问题，A 号核心和 B 号核心的缓存，在这个时候是不一致，从而会导致执行结果的错误。**

* 两种机制

* > - 第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为**写传播（Write Propagation）**；
  > - 第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为**事务的串行化（Transaction Serialization）**。



* 事务串行化

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E4%BA%8B%E4%BB%B6%E9%A1%BA%E5%BA%8F%E9%97%AE%E9%A2%98.png)

* 上图不满足事务串行化

* 我们要保证 C 号核心和 D 号核心都能看到**相同顺序的数据变化**，比如变量 i 都是先变成 100，再变成 200，这样的过程就是事务的串行化。

* 需要做到一下两点：

* > - CPU 核心对于 Cache 中数据的操作，需要同步给其他 CPU 核心；
  > - 要引入「锁」的概念，如果两个 CPU 核心里有相同数据的 Cache，那么对于这个 Cache 数据的更新，只有拿到了「锁」，才能进行对应的数据更新。



##### 总线嗅探

* 写传播的原则就是**当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心**。
* 总线嗅探方法很简单， **CPU 需要每时每刻监听总线上的一切活动**，但是不管别的核心的 Cache 是否缓存相同的数据，都需要发出一个广播事件，这无疑会加重总线的负载。
* 总线嗅探只是保证了某个 CPU 核心的 Cache 更新数据这个事件能被其他 CPU 核心知道，但是并不能保证事务串行化。



##### MESI 协议

- *Modified*，已修改 ： 「已修改」状态就是我们前面提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存里。尚未写入到内存中
- *Exclusive*，独占 ： 独占状态的时候，数据**只存储在一个 CPU 核心的 Cache 里，而其他 CPU 核心的 Cache 没有该数据**。这个时候，如果要向独占的 Cache 写数据，就可以直接自由地写入，而不需要通知其他 CPU 核心，因为只有你这有这个数据，就不存在缓存一致性的问题了，于是就可以随便操作该数据。与内存数据一致
- *Shared*，共享 ： 「共享」状态代表着相同的数据在多个 CPU 核心的 Cache 里都有，所以当我们要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后再更新当前 Cache 里面的数据。与内存一致。
- *Invalidated*，已失效 ： 「已失效」状态，表示的是这个 Cache Block 里的数据已经失效了，不可以读取该状态的数据。则需要向其他cpu核广播读消息，在接受到其他cpu核的读响应后，更新cache line，并将状态设置为S。
- 上面四个表示的是 caceh line 的四种状态
- 





#### CPU 如何执行任务

* CPU 从内存中读取数据到 Cache 的时候，**并不是一个字节一个字节读取，而是一块一块的方式来读取数据的**，这一块一块的数据被称为 CPU Line（缓存行），所以 **CPU Line 是 CPU 从内存读取数据到 Cache 的单位**。
* 因此，如果我们操作的**数据是数组**，那么访问数组元素的时候，按内存分布的地址顺序进行访问，这样能充分利用到 Cache，程序的性能得到提升。但如果操作的数据不是数组，而是**普通的变量**，并在多核 CPU 的情况下，我们还需要避免 Cache Line 伪共享的问题。



##### 什么是伪共享

* 这个共享的意思是：读取数据读一个缓存行，这个缓存行内部的数据都是可以共享的。但是现在并没有做到缓存共享的效果，是伪共享的状态

* 这种因为多个线程同时读写**同一个 Cache Line 的不同变量**时，而导致 **CPU Cache 失效**的现象称为**伪共享（\*False Sharing\*）**。

* 可以发现如果 1 号和 2 号 CPU 核心这样持续交替的分别修改变量 A 和 B，就会重复 ④ 和 ⑤ 这两个步骤，**Cache 并没有起到缓存的效果**，虽然变量 A 和 B 之间其实并没有任何的关系，但是因为同时归属于一个 Cache Line ，这个 Cache Line 中的任意数据被修改后，都会相互影响，从而出现 ④ 和 ⑤ 这两个步骤。

* A B 核心需要反复的修改共享缓存的数据，因为这两个共享数据同属于一个缓存行，会出现相互影响的情况，从而没有能力做到共享，因为每次都要放回到内存，然后再从内存中读取数据。

* 解决方法：

* > * 对于多个线程共享的热点数据，即经常会修改的数据，应该**避免这些数据刚好在同一个 Cache Line 中**，否则就会出现为伪共享的问题。
  > * 在 Linux 内核中存在 `__cacheline_aligned_in_smp` 宏定义，是用于解决伪共享的问题。(缓存行对齐)
  > * 可以采用上面的宏定义使得变量在 Cache Line 里是对齐的。
  > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E4%BC%AA%E5%85%B1%E4%BA%AB/struct_test1.png)
  > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E4%BC%AA%E5%85%B1%E4%BA%AB/struct_ab1.png)
  > * 这样 a 和 b 变量就不会在同一个 Cache Line 中了
  > * 



##### CPU 如何选择线程

* Linux 中， 进程和线程都是用 `task_struct` 结构体表示的，区别在于**线程的 task_struct 结构体里部分资源是共享了进程已创建的资源，比如内存地址空间、代码段、文件描述符等**，所以 Linux 中的线程也被称为轻量级进程，因为线程的 task_struct 相比进程的 task_struct 承载的 资源比较少，因此以「轻」得名。







#### 中断

* 中断是一种异步的事件处理机制，可以提高系统的并发处理能力。
* 操作系统收到了中断请求，会打断其他进程的运行，所以**中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响。**
* **中断请求的处理程序应该要短且快**，这样才能减少对正常进程运行调度地影响，而且中断处理程序可能会暂时关闭中断，这时如果中断处理程序执行时间过长，可能在还未执行完中断处理程序前，会丢失当前其他设备的中断请求。

##### 软中断

* **为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」**。

* > - **上半部用来快速处理中断**，一般会**暂时关闭中断请求**，主要负责处理跟硬件紧密相关或者时间敏感的事情。
  > - **下半部用来延迟处理上半部未完成的工作**，一般以「内核线程」的方式运行。

* 网卡收到网络包后，会通过**硬件中断**通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来响应该事件，这个事件的处理也是会分成上半部和下半部。

* 上部分要做到快速处理，所以只要**把网卡的数据读到内存中**，然后更新一下硬件寄存器的状态，比如把状态更新为表示数据已经读到内存中的状态值。

* 内核会触发一个**软中断**，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部.

* 中断处理程序上半部分和下半部分：

* > - **上半部直接处理硬件请求，也就是硬中断**，主要是负责耗时短的工作，特点是快速执行；
  > - **下半部是由内核触发，也就说软中断**，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是**延迟执行**；

* 硬中断（上半部）是会**打断 CPU 正在执行的任务**，然后立即执行中断处理程序

* 软中断（下半部）是以**内核线程的方式执行**，并且**每一个 CPU 都对应一个软中断内核线程**，名字通常为「ksoftirqd/CPU 编号」

* **内核自定义事件也属于软中断**，比如内核调度等、RCU 锁（内核里常用的一种锁）

* 软中断：

* > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BD%AF%E4%B8%AD%E6%96%AD/softirqs.png)
  > *  `NET_RX` 表示网络接收中断，`NET_TX` 表示网络发送中断、`TIMER` 表示定时中断、`RCU` 表示 RCU 锁中断、`SCHED` 表示内核调度中断。

* **系统调用是一种软中断**，通常采用一个或者多个中断向量号来表示所有的系统调用（因为中断号有限）Linux **采用 int 0x80 来触发**所有的系统调用

* Q： 操作系统如何知道是哪个系统调用被调用的呢？

* A： 和中断一样，系统调用存在一个系统调用号，用来像身份标识一样表明是哪一个系统调用，这个系统调用号通常就是系统调用在**系统调用表中的位置**（去表中找执行的系统调用函数），例如 LInux里 fork 系统调用号就是 2 。

* eg : 以 Linux 的 0x80 为例，系统调用号是由 eax 来传入的，用户将系统调用号放入 eax, 然后使用 0x80 调用中断，中断服务程序就可以从 eax 里取得系统调用号，进而调用对应的函数.

* Q: 中断基本属性？

* > * 中断存在两个属性：1. 中断号 （0 开始）。 2. 中断处理程序
  > * 中断号和中断处理程序是一一对应的。
  > * 存在一个**数组**中断向量表，这个数组包含的第 n 项包含了指向第 n 号中断的**中断处理程序指针**（指向具体执行的中断函数）。
  > * 中断到来，CPU 暂停当前执行的代码，**根据中断的中断号，在中断向量表中找到对应的中断处理程序**，并调用它。中断处理程序执行完成之后， CPU继续执行之前的代码。

* 





#### 浮点数为什么不能比较

* 负数是正数取反加一，补码。

* 浮点数转二进制

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E5%8D%81%E8%BF%9B%E5%88%B6%E5%B0%8F%E6%95%B0%E8%BD%AC%E4%BA%8C%E8%BF%9B%E5%88%B6.png)



* `0.1` 的**二进制表示是无限循环的， 所以 0.1 没法使用二进制表示出来**
* 二进制转十进制：
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E5%B0%8F%E6%95%B0%E8%BD%AC%E4%BA%8C%E8%BF%9B%E5%88%B62.png)
* 

##### 计算机是如何存储小数的

* 计算机存储小数的采用的是**浮点数**，名字里的「浮点」表示小数点是可以浮动的。

* 所以通常将 `1000.101` 这种二进制数，规格化表示成 `1.000101 x 2^3`，其中，最为关键的是 000101 和 3 这两个东西，

* > - `000101` 称为**尾数**，即小数点后面的数字；
  > - `3` 称为**指数**，指定了小数点在数据中的位置；

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/IEEE%E6%A0%87%E5%87%86.png)

* > - *符号位*：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；
  > - *指数位*：指定了**小数点在数据中的位置**，指数可以是负数，也可以是正数，**指数位的长度越长则数值的表达范围就越大**；（小数点可以偏移的位置）
  > - *尾数位*：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且**尾数的长度决定了这个数的精度**，因此如果要表示精度更高的小数，则就要提高尾数位的长度；

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/float.png)

* float 单精度浮点型数据，double 双精度浮点型数据

* double 有 53 个二进制有效位，float 有 24 个二进制有效位，所以所以它们的精度在十进制中分别是 `log10(2^53)` 约等于 `15.95` 和 `log10(2^24)` 约等于 `7.22` 位，因此 double 的有效数字是 `15~16` 位，float 的有效数字是 `7~8` 位，这些有效位是包含整数部分和小数部分；

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/float%E5%AD%98%E5%82%A8.png)

* 10.625 的浮点数存储格式(1010.101)，如上图所示

* `10000010`（十进制 130） 3 + 127(127 是偏移量)

* Q： 指数为什么需要加上偏移量？

* A： 指数可能是正数，也可能是负数，即指数是有符号的整数，而有符号整数的计算是比无符号整数麻烦的，所以为了减少不必要的麻烦，在实际存储指数的时候，需要把指数转换成**无符号整数**。

* float 的指数部分是 8 位，IEEE 标准规定单精度浮点的指数取值范围是 `-126 ~ +127`，于是为了把指数转换成无符号整数，就要加个**偏移量**，比如 float 的指数偏移量是 `127`，这样指数就不会出现负数了。

* 二进制浮点数的小数点左侧只能有 1 位，并且还只能是 1，**既然这一位永远都是 1，那就可以不用存起来了**。

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/float%E5%85%AC%E5%BC%8F.png)

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/float%E8%BD%AC%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%BE%8B%E5%AD%90.png)





##### 0.1 + 0.2 == 0.3 ？

* **因为有的小数无法可以用「完整」的二进制来表示，所以计算机里只能采用近似数的方式来保存，那两个近似数相加，得到的必然也是一个近似数**。
* 0.1 0.2 在计算机中以如下的格式保存
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/0.1%2B0.2.png)
* IEEE 754 标准定义的浮点数只能根据精度舍入，然后用「近似值」来表示该二进制，那么意味着**计算机存放的小数可能不是一个真实值**。
* 0.1 + 0.2 并不等于完整的 0.3，这主要是因为这两个小数无法用「完整」的二进制来表示，只能根据精度舍入，所以计算机里只能采用近似数的方式来保存，**那两个近似数相加，得到的必然也是一个近似数**。
* **对于超出了精度限制的浮点数，计算机会把它们的精度之外的小数部分截断**。因此，本来不相等的两个浮点数在计算机中可能就变成相等的了.
* 



* ```shell\
  https://zhuanlan.zhihu.com/p/84453627
  ```

* [float 表示的范围比 int 范围大](https://zhuanlan.zhihu.com/p/84453627)

* 同样是占32个坑，那凭啥你float就比int的范围更大？**因为float型虽然范围大，但是精度不足啊！所以各有千秋哦。**

* float型的表示范围是比int大很多的，但有效位确实只有24位。既然float范围大，那么所有的int型都是可以转换为float型的，这是不会产生溢出报错的。
* 但因为int型有效位是32位，是比float型的24位大的，是有可能发生舍入的，即当一个int型数字，转成float型后，可能就不再是原本数字了，损失了一定的精度。







#### CPU 上下文

* Q : 进程在竞争 CPU 的时候并没有真正运行，为什么还会导致系统的负载升高呢？
* A : 进程在**竞争CPU的时候造成的系统负载升高是因为CPU上下文切换**
* 而在每个任务运行前，**CPU 都需要知道任务从哪里加载、又从哪里开始运行**，也就是说，需要**系统事先帮它设置好 CPU 寄存器和程序计数器**（Program Counter，PC）
* CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而**程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置**。
* 它们都是 **CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 CPU 上下文**。
* ![img](https://static001.geekbang.org/resource/image/98/5f/98ac9df2593a193d6a7f1767cd68eb5f.png?wh=438*345)
* **CPU 上下文切换**，就是**先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来**，然后加载新任务的上下文到这些寄存器和程序计数器，最后**再跳转到程序计数器所指的新位置，运行新任务**。
* 而这些保存下来的上下文，会存储在**系统内核中**，并在任务重新调度执行时再次加载进来。
* Q : 操作系统管理的这些“任务”到底是什么呢？
* A : 任务就是进程，或者说任务就是线程。是的，进程和线程正是最常见的任务。但是除此之外，还有没有其他的任务呢？ 硬件通过触发信号，会导致中断处理程序的调用，也是一种常见的任务。
* 根据任务的不同，CPU 的上下文切换就可以分为几个不同的**场景**:
* **进程上下文切换**、**线程上下文切换**以及**中断上下文切换**。

##### 进程上下文切换

* 进程既可以在用户空间运行，又可以在内核空间中运行。进程在用户空间运行时，被称为进程的用户态，而陷入内核空间的时候，被称为进程的内核态。

* 从用户态到内核态的转变，需要通过**系统调用**来完成。比如，当我们查看文件内容时，就需要多次系统调用来完成：首先调用 open() 打开文件，然后调用 read() 读取文件内容，并调用 write() 将内容写到标准输出，最后再调用 close() 关闭文件。

* **系统调用的过程有没有发生 CPU 上下文的切换呢？答案自然是肯定的**。

* CPU 寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。

* 而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。

* 所以，**一次系统调用的过程，其实是发生了两次 CPU 上下文切换。**

* 内核态和用户态都拥有堆栈空间，而**内核态的数据是所有进程共享的**。 在陷入的过程中：需要保存用户态在堆栈寄存器中的数据到堆栈内存中，从而加载内核态的堆栈到堆栈寄存器中。

* 需要注意的是，系统调用过程中，并不会涉及到虚拟内存等进程用户态的资源，也不会切换进程。这跟我们通常所说的进程上下文切换是不一样的：

* > * 进程上下文切换，是指从一个进程切换到另一个进程运行。
  > * 而**系统调用过程中一直是同一个进程在运行**。

* **系统调用过程通常称为特权模式切换，而不是上下文切换**。但实际上，系统调用过程中，CPU 的上下文切换还是无法避免的。

* 进程上下文切换跟系统调用又有什么区别呢？

* > * **进程是由内核来管理和调度的，进程的切换只能发生在内核态。**
  > * 所以，**进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源**，还**包括了内核堆栈、寄存器等内核空间的状态**。
  > * 因此，进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该**进程的虚拟内存、栈等保存下来**；而加载了下一进程的内核态后，还**需要刷新进程的虚拟内存和用户栈**。

* 保存上下文和恢复上下文的过程并不是“免费”的，需要内核在 CPU 上运行才能完成.

* ![img](https://static001.geekbang.org/resource/image/39/6b/395666667d77e718da63261be478a96b.png?wh=966*186)

* 每次**上下文切换都需要几十纳秒到数微秒的 CPU 时间**。这个时间还是相当可观的，特别是在进程上下文切换次数较多的情况下，很容易**导致 CPU 将大量时间耗费在寄存器、内核栈以及虚拟内存等资源的保存和恢复上，进而大大缩短了真正运行进程的时间**。

* 



##### 线程上下文切换

* 线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位。

* **所谓内核中的任务调度，实际上的调度对象是线程**；而进程只是给线程提供了虚拟内存、全局变量等资源。

* > * 当进程只有一个线程时，可以认为进程就等于线程。
  > * 当进程拥有多个线程时，这些线程会共享相同的**虚拟内存和全局变量等资源**。这些资源在上下文切换时是不需要修改的
  > * 另外，线程也有自己的私有数据，比如**栈和寄存器等，这些在上下文切换时也是需要保存的**。

* 线程的上下文切换其实就可以分为两种情况：

* > * 第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以**切换过程就跟进程上下文切换是一样**。
  > * 第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。

* 虽然同为上下文切换，但**同进程内的线程切换，要比多进程间的切换消耗更少的资源**，而这，也正是多线程代替多进程的一个优势。



##### 中断上下文

* 为了**快速响应硬件的事件**，中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件。
* 而在打断其他进程时，**就需要将进程当前的状态保存下来**，这样在**中断结束后，进程仍然可以从原来的状态恢复运行**。
* 跟进程上下文不同，**中断上下文切换并不涉及到进程的用户态**。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实**只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。**
* 中断只是内核态的转换，用户态数据不需要重新加载
* **对同一个 CPU 来说，中断处理比进程拥有更高的优先级**，所以中断上下文切换并不会与进程上下文切换同时发生。



##### 总结

上下文切换的过程：     

(1)记录当前任务的上下文(即寄存器和计算器等所有的状态)；      

(2)找到新任务的上下文并加载；      

(3)切换到新任务的程序计算器位置，恢复其任务。

* **系统调用属于同进程内的CPU上下文切换；**
* 如果**函数调用是类似open、read和write等系统IO函数，则属于同进程内CPU上下文切换**，如果是**普通函数调用，则属于线程间CPU上下文切换**，因为涉及到栈内数据保存，如递归调用。
* **系统调用通过80号中断从用户态进入内核态执行内核代码，所以需要保存用户态上下文**。普通函数调用只需要在栈中压入参数，压入返回地址，**通过call指令调用开始执行函数代码，若函数有返回值，还会更改上下文eax/rax寄存器**。此过程不涉及特权级转移，不需要保存上下文，没有上下文切换一说。





##### 实操案例

* 1. stress和sysbench两个工具在压测过程中的对比发现： 
  2. **stress基于多进程的**，会fork多个进程，导致进程上下文切换，导致us开销很高； 
  3. **sysbench基于多线程的**，会创建多个线程，单一进程基于内核线程切换，导致sy的内核开销很高； 
  4. 具体可以通过vmstat对比 stress -c 8 -i 16 -t 600 
  5. vmstat 1 5 
  6. sysbench --threads=20 --time=300 threads run 
  7. vmstat 1 5

* 一个是 cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数，另一个则是 nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。

* > * 所谓**自愿上下文切换**，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。
  > * 而**非自愿上下文切换**，则是指进程由于**时间片已到**等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。

* 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题；

* 非自愿上下文切换变多了，说明进程都在被强制调度，**也就是都在争抢 CPU，说明 CPU 的确成了瓶颈**；

* 中断次数变多了，**说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型**。

* cpu 使用率高，可能确实是使用率高， **也的可能实际处理不高而是进程太多切换上下文频繁** ， 也可能是**进程内线程的上下文切换频繁**.

* io 使用率高 ， **说明 io 请求比较大， 可能是 文件io 、 网络io** 

* 



#### 为什么用户态到内核态切换耗费时间

* 应用程序的执行必须依托于内核提供的资源，包括CPU资源、存储资源、I/O资源等。为了使上层应用能够访问到这些资源，内核必须为上层应用提供访问的接口：即系统调用。
* 如果一个程序需要从用户态进入内核态，那么它必须执行系统调用语句。
* 当程序中有系统调用语句，程序执行到系统调用时，首先**使用类似int 80H的软中断指令，保存现场，去系统调用，在内核态执行，然后恢复现场**，每个进程都会有两个栈，一个内核态栈和一个用户态栈。
* 当int中断执行时就会由用户态栈转向内核态栈。
* 系统调用时需要进行栈的切换。
* 而且**内核代码对用户不信任，需要进行额外的检查**。系统调用的返回过程有很多额外工作，比如检查是否需要调度等。 
* 系统调用一般都需要保存用户程序得上下文(context), 在**进入内核的时候需要保存用户态的寄存器**，在**内核态返回用户态的时候会恢复这些寄存器的内容**。
* 如果需要在**不同用户程序间切换的话**，那么还要更新cr3寄存器，这样会更换每个程序的虚拟内存到物理内存映射表的地址，也是一个比较高负担的操作







#### 磁盘 IO









#### 用户进程页表 VS 内核页表

* 普通的用户进程的**页表**也是存放在内核空间的。
* 页表的分配是通过调用内核的接口去分配物理内存的
* **内核在启动阶段已经创建了内核页表**
* 用户进程页表可以分成两个部分，第一个部分是用户地址空间，第二部分是内核页表，在启动阶段就创建好了，用户进程页表会复制内核页表，所以所有的进程都**共享同一个内核页表，所以所有进程都共享内核空间**。
* **Linux 中内核页表和真实的物理内存地址是一一对应的**。
* 内核在这里使用了“延迟更新”的策略，将“进程页表”真正更新推迟到第一次访问相关线性地址，发生page fault时，此时在page fault的处理流程中进行“进程页表”的更新。



#### 调度算法

![本文提纲](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E6%8F%90%E7%BA%B2.png)

* 进程调度算法



* 页面置换算法



* 磁盘调度算法





#### 直接IO，异步IO

* 高并发场景处理大文件时，应当使用异步 IO 和直接 IO 来替换零拷贝技术。
* 绕过 PageCache 的 IO 是个新物种，我们把它称为直接 IO。对于磁盘，异步 IO 只支持直接 IO。
* 当调用 read 方法读取文件时，实际上 read 方法会在磁盘寻址过程中阻塞等待，导致进程无法并发地处理其他任务，如下图所示：
* ![img](https://static001.geekbang.org/resource/image/9e/4e/9ef6fcb7da58a007f8f4e3e67442df4e.jpg?wh=3933*2550)

* 异步 IO（异步 IO 既可以处理网络 IO，也可以处理磁盘 IO，这里我们只关注磁盘 IO）可以解决阻塞问题。它把读操作分为两部分，前半部分向内核发起读请求，但不等待数据就位就立刻返回，此时进程可以并发地处理其他任务。
* ![img](https://static001.geekbang.org/resource/image/15/f3/15d33cf599d11b3188253912b21e4ef3.jpg?wh=4040*2172)

* 异步 IO 并没有拷贝到 PageCache 中，这其实是异步 IO 实现上的缺陷。经过 PageCache 的 IO 我们称为缓存 IO，它与虚拟内存系统耦合太紧，导致异步 IO 从诞生起到现在都不支持缓存 IO。
* 异步 IO 主要应用：
* 第一，应用程序已经实现了磁盘文件的缓存，不需要 PageCache 再次缓存，引发额外的性能消耗。比如 MySQL 等数据库就使用直接 IO；
* 第二，高并发下传输大文件，我们上文提到过，大文件难以命中 PageCache 缓存，又带来额外的内存拷贝，同时还挤占了小文件使用 PageCache 时需要的内存，因此，这时应该使用直接 IO。



#### 5 种 IO 模型

##### 阻塞 IO

* 调用者调用了某个函数，等待这个函数返回，期间什么也不做，**不停的去检查这个函数有没有返回**，必
  须等这个函数返回才能进行下一步动作。
  
* [阻塞IO socket 通信](https://vdn6.vzuu.com/SD/34793a3a-9119-11eb-875a-e69333a856fa.mp4?pkey=AAVF_384XzO1-GoCqKyS6orNiuMW9k_TBPmNEpYhFssv7hzzW8xEjQJNp009Dno5HqVm7bHXTgQXQtN4JWdeaMUn&c=avc.0.0&f=mp4&pu=078babd7&bu=078babd7&expiration=1661057959&v=ks6)

* <video src="/home/zhoubing/Documents/阻塞IO.mp4"></video>

* 可以看到，服务端的线程阻塞在了两个地方，**一个是 accept 函数，一个是 read 函数**。

* 如果再把 read 函数的细节展开，我们会发现其阻塞在了两个阶段:

* <video src="/home/zhoubing/Documents/IO1.mp4"></video>

  ![img](https://pic2.zhimg.com/80/v2-f34dd9db3f47e81caa2db675feef19ae_720w.jpg?source=1940ef5c)

* 如果这个连接的客户端一直不发数据，那么**服务端线程将会一直阻塞在 read 函数上不返回，也无法接受其他客户端连接**。





##### 非阻塞 IO

* 非阻塞等待，**每隔一段时间就去检测IO事件是否就绪**。**没有就绪就可以做其他事**（不会阻塞）。非阻塞I/O执行系统调
  用总是立即返回，**不管事件是否已经发生**，若事件没有发生，则返回-1，此时可以根据 errno 区分这两
  种情况，对于accept，recv 和 send，事件未发生时，errno 通常被设置成 EAGAIN
  
* 为了解决上面的问题，其关键在于改造这个 read 函数。

* 每次都创建一个新的进程或线程，去调用 read 函数，并做业务处理。

* <video src="/home/zhoubing/Documents/IO2.mp4"></video>

  不过，这不叫非阻塞 IO，只不过用了**多线程的手段使得主线程没有卡在 read 函数上不往下走罢了。操作系统为我们提供的 read 函数仍然是阻塞的。**

* 所以真正的非阻塞 IO，不能是通过我们用户层的小把戏，**而是要恳请操作系统为我们提供一个非阻塞的 read 函数**。

* ```cpp
  // 操作系统提供了这样的功能，只需要在调用 read 前，将文件描述符设置为非阻塞即可。
  fcntl(connfd, F_SETFL, O_NONBLOCK);
  int n = read(connfd, buffer) != SUCCESS);
  ```

* 就需要用户线程循环调用 read，直到返回值不为 -1，再开始处理业务。

* <video src="/home/zhoubing/Documents/IO3.mp4"></video>

  ![img](https://pic1.zhimg.com/80/v2-e330fee2343f6d3b1c886b3be23d8244_720w.jpg?source=1940ef5c)

* 非阻塞的 read，指的是在数据到达前，即数据**还未到达网卡**，或者**到达网卡但还没有拷贝到内核缓冲区之前**，这个阶段是非阻塞的。

* **当数据已到达内核缓冲区**，此时调用 read 函数仍然是阻塞的，需要等待数据从内核缓冲区拷贝到用户缓冲区，才能返回。

* 





##### IO 复用

* Linux 用 select/poll/epoll 函数实现 IO 复用模型，这些函数也会使**进程阻塞**，但是和阻塞IO所不同的是
  这些函数**可以同时阻塞多个IO操作**。而且可以同时对多个读操作、写操作的IO函数**进行检测**。**直到有数**
  **据可读或可写时，才真正调用IO操作函数**。

###### 传统思路

* 为每个**客户端创建一个线程**，服务器端的线程资源很容易被耗光。

* ![img](https://pica.zhimg.com/80/v2-fe97fb116087445f0e6d1ab12fcf5c69_720w.jpg?source=1940ef5c)

* 当然还有个聪明的办法，我们可以每 accept 一个客户端连接后，将这个**文件描述符（connfd）放到一个数组里。**

* 然后**弄一个新的线程**去不断遍历这个数组，调用每一个元素的非阻塞 read 方法。这样，我们就成功**用一个线程处理了多个客户端连接**。

* ```cpp
  while(1) {
    for(fd <-- fdlist) {
      if(read(fd) != -1) { // 每次遍历遇到 read 返回 -1 时仍然是一次浪费资源的系统调用。
        doSomeThing();
      }
    }
  }
  ```



###### [select](#Select/poll)



##### 信号驱动

* Linux 用套接口进行信号驱动 IO，安装一个信号处理函数，进程继续运行**并不阻塞**，当IO事件就绪，进
  程收到SIGIO 信号，然后处理 IO 事件。
* 内核在**第一个阶段是异步，在第二个阶段是同步**；与非阻塞IO的区别在于它提供了消息通知机制，不需
  要用户进程不断的轮询检查，减少了系统API的调用次数，提高了效率。

#####  异步（asynchronous）

* Linux中，可以调用 **aio_read 函数**告诉内核描述字缓冲区指针和缓冲区的大小、文件偏移及通知的方
  式，然后立即返回，当内核将数据拷贝到缓冲区后，再通知应用程序。







#### IO 多路复用

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/tcp_socket.png)

##### 多进程模型

* 服务器的主进程负责监听客户的连接，一旦与客户端连接完成，accept() 函数就会返回一个「已连接 Socket」，这时就通过 `fork()` 函数创建一个子进程，实际上就把父进程所有相关的东西都**复制**一份，包括文件描述符、内存地址空间、程序计数器、执行的代码等。
* 子进程会**复制父进程的文件描述符**，于是就可以直接使用「已连接 Socket 」和客户端通信了，
* 子进程不需要关心「监听 Socket」，只需要关心「已连接 Socket」；父进程则相反，将客户服务交给子进程来处理，因此父进程不需要关心「已连接 Socket」，只需要关心「监听 Socket」
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E5%A4%9A%E8%BF%9B%E7%A8%8B.png)
* 另外，当「子进程」退出时，实际上内核里还会保留该进程的一些信息，也是会占用内存的，如果不做好“回收”工作，就会变成**僵尸进程**，随着僵尸进程越多，会慢慢耗尽我们的系统资源。
* 两种方式可以在子进程退出后回收资源，分别是调用 `wait()` 和 `waitpid()` 函数。
* 进程的上下文切换不仅包含了**虚拟内存、栈、全局变量等用户空间的资源**，还包括了**内核堆栈、寄存器等内核空间的资源**。



##### 多线程模型

* 进程里的线程可以共享进程的部分资源的，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下文切换时是不需要切换，而只需要切换线程的**私有数据、寄存器等不共享的数据**(栈），因此同一个进程下的线程上下文切换的开销要比进程小得多。
* 如果频繁创建和销毁线程，系统开销也是不小的，解决方案：线程池
* **线程池：** 提前创建若干个线程，这样当由新连接建立时，将这个**已连接的 Socket 放入到一个队列**里，然后**线程池里的线程负责从队列中取出已连接 Socket 进程**处理。
* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E7%BA%BF%E7%A8%8B%E6%B1%A0.png)
* 这个**队列是全局的，每个线程都会操作**，为了避免多线程竞争，线程在操作这个队列前要加锁。



##### IO 多路复用（同步阻塞函数）

* **多路是指网络连接**，**复用指的是同一个线程**.
* IO 多路复用是一种**同步IO模型**，实现一个线程可以监视多个文件句柄；
* 一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；
* 服务器端采用单线程通过 `select/poll/epoll` 等系统调用获取 fd 列表，**遍历有事件的 fd 进行 `accept/recv/send`** ，使其能支持更多的并发连接请求。
* epoll只能工作在 linux 下.(缺点)

思路： **只有当连接存在数据的时候，线程才发起读请求**，而不是使用 while 死循环非阻塞的方式通过 read 轮训，这样耗时太久.

* 在获取事件时，**先把我们要关心的连接传给内核**，再由内核检测。
* **只使用一个进程来维护多个 Socket 呢**？答案是有的，那就是 **I/O 多路复用**技术。
* 我们熟悉的 select/poll/epoll 内核提供给用户态的多路复用系统调用，**进程可以通过一个系统调用函数从内核中获取多个事件**。
* select/poll/epoll 是如何获取网络事件的呢？在获取事件时，**先把所有连接（文件描述符）传给内核**，再由内核返回产生了事件的连接，然后在**用户态中再处理这些连接对应的请求即可**。（读写请求）

###### Select/poll

* select 过程：

* > * 将已连接的 Socket 都放到一个**文件描述符集合**
  > * 调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式。
  > * 当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把**整个**文件描述符集合**拷贝**回用户态里
  > * 用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。

* 对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在**内核态**里，一个次是在**用户态**里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。(文件描述符表原本是在用户态的文件)， 传入到内核的目的是：希望**内核帮忙看看哪些文件描述符到达内核了，就是用户态希望可以查询某些读事件是否到来**。内核修改文件描述符表之后，需要将其拷贝到用户态，用户态再去扫描刚刚返回的表，查看有哪些事件可以处理

* select 在内核层仍然是通过遍历的方式检查文件描述符的就绪状态，**是个同步过程**，只不过**无系统调用切换上下文的开销**。

* select 使用固定长度的 BitsMap，表示文件描述符集合(大小是： **1024(由内核中的 FD_SETSIZE 限制)**，只能监听 0-1023 的文件描述符)

* poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之**用动态数组，以链表形式来组织**，突破了 **select 的文件描述符个数限制**，当然还会受到系统文件描述符限制。

* poll 和 select 本质是一样的：

* > * **都是使用「线性结构」存储进程关注的 Socket 集合**
  > * **因此都需要遍历文件描述符集合来找到可读或可写的 Socket**,时间复杂度 ： O(n)
  > * **而且也需要在用户态与内核态之间拷贝文件描述符集合**
  > * 缺点： 这种方式随着并发数上来，性能的损耗会呈指数级增长
  
* 服务端代码：

* > * 首先**一个线程不断接受客户端连接**，并把 socket 文件描述符放到一个 list 里。
  >
  > * ```cpp
  >   while(1) {
  >     connfd = accept(listenfd);
  >     fcntl(connfd, F_SETFL, O_NONBLOCK);
  >     fdlist.add(connfd);
  >   }
  >   ```
  >
  > * 然后，**另一个线程不再自己遍历**，而是调用 select，**将这批文件描述符 list 交给操作系统去遍历**。
  >
  > * ```cpp
  >   while(1) {
  >     // 把一堆文件描述符 list 传给 select 函数
  >     // 有已就绪的文件描述符就返回，nready 表示有多少个就绪的
  >     nready = select(list);
  >     ...
  >   }
  >   ```
  >
  > * 不过，当 select 函数返回后，**用户依然需要遍历刚刚提交给操作系统的 list。**
  >
  > * 只不过，**操作系统会将准备就绪的文件描述符做上标识**，用户层将不会再有**无意义的系统调用开销**。
  >
  > * ```cpp
  >   while(1) {
  >     nready = select(list);
  >     // 用户层依然要遍历，只不过少了很多无效的系统调用
  >     for(fd <-- fdlist) {
  >       if(fd != -1) {
  >         // 只读已就绪的文件描述符
  >         read(fd, buf);
  >         // 总共只有 nready 个已就绪描述符，不用过多遍历
  >         if(--nready == 0) break;
  >       }
  >     }
  >   }
  >   ```
  >
  > * <video src="/home/zhoubing/Documents/IO4.mp4"></video>
  >
  >   ![img](https://pic1.zhimg.com/80/v2-6d9944887990eaf8714438351a196301_720w.jpg?source=1940ef5c)
  >
  > * 可以看到，这种方式，既做到了**一个线程**处理多个客户端连接（文件描述符），又减少了系统调用的开销（**多个[文件描述符](https://www.zhihu.com/search?q=文件描述符&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1932776593})只有一次 select 的系统调用 + n 次就绪状态的文件描述符的 read 系统调用**）。





###### Epoll

* 先用epoll_create 创建一个 epoll对象 epfd，再通过 epoll_ctl 将需要**监视的 socket 添加到epfd中**（红黑树中），最后调用 **epoll_wait 等待数据**。

```cpp
int s = socket(AF_INET, SOCK_STREAM, 0);
bind(s, ...);
listen(s, ...)

int epfd = epoll_create(...); // 文件描述符
epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中

while(1) {
    int n = epoll_wait(...);
    for(接收到数据的socket){
        //处理
    }
}
```

* epoll 如何解决 select / poll 的问题？

* > * epoll 改进：
  >
  > * > * 内核中**保存一份文件描述符集合**，无需用户每次都重新传入，只需告诉内核**修改的部分**即可。
  >   > * 内核不再通过轮询的方式找到就绪的文件描述符，而是通过**异步 IO 事件唤醒**。
  >   > * 内核仅会将有 IO 事件的文件描述符返回给用户，用户也无需遍历整个文件描述符集合。

* socket 会在第一次加入红黑树就可以了

* > * 1. epoll 在**内核**里使用**红黑树来跟踪进程所有待检测的文件描述字**
  > * **把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里.**(增删改时间复杂度： O(logn))
  > * select/poll 内核里没有类似 epoll 红黑树这种保存所有待检测的 socket 的数据结构，所以需要传入整个文件描述符表
  > * epoll 因为在内核维护了**红黑树，可以保存所有待检测的 socket** 。
  > * **只需要传入一个待检测的 socket**，减少了内核和用户空间大量的数据拷贝和内存分配。（select 需要传入一整张 文件描述符表）拷贝空间复杂度 ： O(1) 不需要拷贝文件描述符表
  > * 2.  epoll 使用**事件驱动**的机制，内核里**维护了一个链表来记录就绪事件**
  > * 当某个 socket 有事件发生时，通过**回调函数**内核会将其加入到这个就绪事件**列表**中。(将有网络事件发生的 socket 移动到链表中) -> 将链表拷贝到用户态.
  > * 当用户调用 `epoll_wait()` 函数时，只会**返回有事件发生**的文件描述符的个数。（elect/poll 那样轮询扫描整个 socket 集合）
  > * 对于**就绪的事件，epoll 使用的是共享内存的方式**，即用户态和内核态都指向了就绪链表，所以就避免了**内存拷贝消耗**。
  > * ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png)
  > * epoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的**进程打开的最大文件描述符个数**
  
* epoll_wait 实现的内核代码中调用了 `__put_user` 函数，这个函数就是将**数据从内核拷贝到用户空间**。（需要拷贝操作，但是只拷贝有事件发生的文件描述符）

* <video src="/home/zhoubing/Documents/IO5.mp4"></video>

  



###### epoll 同步 IO

* epoll 这个**系统调用，是同步的**，也就是必须等待操作系统返回值。
* 而**底层用了 [epoll](https://www.zhihu.com/search?q=epoll&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1932776593}) 的封装后的框架，可以是异步的**，只要你暴露给外部的接口，无需等待你的返回值即可。
* 再多说些，epoll 这个系统调用的**底层内核设计里，每个 IO 事件的通知等待，是异步的**。但这不影响，epoll 这个系统调用对外部来说，是一个同步的接口。





###### 边缘触发和水平触发

* 使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，**服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；(只通知一次)(使用 while 循环一直读，直到所有文件读完为止).
* 使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；（反复的通知你）
* 边缘触发的效率比水平触发的效率要高，因为**边缘触发可以减少 epoll_wait 的系统调用次数**，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。
* select/poll **只有水平触发模式**，epoll **默认的触发模式是水平触发**，但是可以根据应用场景设置为边缘触发模式。
* **oneshot && ET 区别：**
*  oneshot指的**某socket对应的fd事件最多只能被检测一次**，不论你设置的是读写还是异常。因为可能存在这种情况：如果epoll检测到了读事件，数据读完交给一个子线程去处理，如果该线程处理的很慢，在此期间epoll在该socket上又检测到了读事件，则又给了另一个线程去处理，则在同一时间会存在两个工作线程操作同一个socket。ET模式指的是：数据第一次到的时刻才通知，其余时刻不再通知。如果读完了又来了新数据，epoll继续通知。ET模式下可以通知很多次。**监听socket不用设置为oneshot是因为只存在一个主线程去操作这个监听socket**
* **设置了EPOLLONESHOT**，**epoll_wait只是在子线程处理数据期间不通知**，等子线程处理完数据重新注册EPOLLIN后，主线程epoll_wait还是会通知读缓冲区有数据的。







#### Reactor & Proactor

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/reactor%E6%8F%90%E7%BA%B2.jpeg" alt="img" style="zoom:30%;" />

##### **Reactor**

* 定义： **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**。

* Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成

* > * Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
  > * **处理资源池**负责处理事件，如 read -> 业务逻辑 -> send；

* Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：

* > - Reactor 的数量可以只有一个，也可以有多个；
  > - **处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程**；



######  单 reactor 单线程 / 进程

* **Redis**

* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%8D%95%E8%BF%9B%E7%A8%8B.png" alt="img" style="zoom:50%;" />

* > - Reactor 对象的作用是监听和分发事件；
  > - Acceptor 对象的作用是获取连接；
  > - Handler 对象的作用是处理业务；

* 单 Reactor 单进程的方案因为全部工作都在同一个进程内完成，所以实现起来比较简单，不需要考虑进程间通信，也不用担心多进程竞争。

* 缺点：

* > - 第一个缺点，因为只有一个进程，**无法充分利用 多核 CPU 的性能**；
  > - 第二个缺点，Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，**如果业务处理耗时比较长，那么就造成响应的延迟**；

* 

###### 单 Reactor 多线程 / 进程

* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img" style="zoom:50%;" />

* 单 Reator 多线程的方案优势在于**能够充分利用多核 CPU 的能**，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。(加锁处理资源竞争)

* 「单 Reactor」的模式还有个问题，**因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**。

  

###### 多 Reactor 多进程 / 线程

* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E4%B8%BB%E4%BB%8EReactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img" style="zoom:50%;" />

* 优势：

* > - 主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理。
  > - 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。



##### Proactor

* 前面提到的 Reactor 是非阻塞同步网络模式，而 **Proactor 是异步网络模式**。
* **阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程**
* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%98%BB%E5%A1%9E%20I_O.png" alt="阻塞 I/O" style="zoom:50%;" />
* 非阻塞： **这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。**
* <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png" alt="非阻塞 I/O" style="zoom:50%;" />
* 

- **Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件**。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是**同步**的，读取完数据后应用进程才能处理数据。
- **Proactor 是异步网络模式， 感知的是已完成的读写事件**。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要**应用进程主动发起 read/write 来读写数据**，操作系统完成读写工作后，就会通知应用进程直接处理数据。
- **Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」**，而 **Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」**
- 无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 **Reactor 模式是基于「待完成」的 I/O 事件，而 Proactor 模式则是基于「已完成」的 I/O 事件**。





#### 读写锁实现

* 基于互斥锁实现的
* **读写锁适用于能明确区分读操作和写操作的场景**。

```cpp
#include<bits/stdc++.h>

class ReadWriteMutex{
public:
    ReadWriteMutex() = default;
    ~ReadWriteMutex() = default;

    ReadWriteMutex(const ReadWriteMutex& ) = delete;
    ReadWriteMutex& operator = (const ReadWriteMutex& ) = delete;

    ReadWriteMutex(const ReadWriteMutex&& ) = delete;
    ReadWriteMutex& operator = (const  ReadWriteMutex&& ) = delete;

    void lock_read() {
        std::unique_lock<std::mutex> lock(m_mutex);

        while (m_write_count != 0) {
            m_cond_read.wait(lock);
        }

        m_read_count++;
    }

    void unlock_read() {
        std::unique_lock<std::mutex> lock(m_mutex);
        // 读读不互斥，读写互斥，所以读的时候只需要通知可能需要的写请求就可以了
        if (--m_read_count == 0 && m_write_count > 0) {
            m_cond_write.notify_one();
        }
    }

    void lock_write() {
        std::unique_lock<std::mutex> lock(m_mutex);
        m_write_count++;

        while (m_read_count != 0 || m_writing) {
            m_cond_write.wait(lock);
        }

        m_writing = true;
    }

    void unlock_write() {
        std::unique_lock<std::mutex> lock(m_mutex);

        if (--m_write_count == 0) {
            m_cond_read.notify_all();
        } else {
            // 表明可以重新唤醒一个读进程
            m_cond_write.notify_one();
        }

        m_writing = false;
    }

private:
    size_t m_read_count = 0;
    size_t m_write_count = 0;
    bool m_writing = false; // 是否正在写入
    std::mutex m_mutex;
    std::condition_variable m_cond_read;
    std::condition_variable m_cond_write;
};

template<typename ReadWriteLock>
class UniqueReadLock{
public:
    explicit UniqueReadLock(ReadWriteLock& rwlock) : m_ptr_rw_lock(&rwlock) {
        m_ptr_rw_lock -> lock_read();
    }
    ~UniqueReadLock() {
        if (m_ptr_rw_lock) {
            m_ptr_rw_lock -> unlock_read();
        }
    }

    UniqueReadLock() = delete;

    UniqueReadLock(const UniqueReadLock& ) = delete;
    UniqueReadLock& operator = (const UniqueReadLock& ) = delete;
    UniqueReadLock(const UniqueReadLock&& ) = delete;
    UniqueReadLock& operator = (const UniqueReadLock&& ) = delete;

private:
    ReadWriteLock* m_ptr_rw_lock = nullptr;
};

template<typename ReadWriteLock>
class UniqueWriteLock{
public:
    explicit UniqueWriteLock(ReadWriteLock& rwlock) : m_ptr_rw_lock(&rwlock) {
        m_ptr_rw_lock -> lock_write();
    }
    ~UniqueWriteLock() {
        if (m_ptr_rw_lock) {
            m_ptr_rw_lock -> unlock_write();
        }
    }

    UniqueWriteLock() = delete;
    UniqueWriteLock(const UniqueWriteLock& ) = delete;
    UniqueWriteLock& operator = (const UniqueWriteLock& ) = delete;
    UniqueWriteLock(const UniqueWriteLock&& ) = delete;
    UniqueWriteLock& operator = (UniqueWriteLock&& ) = delete;
private:
    ReadWriteLock* m_ptr_rw_lock = nullptr;
};

void readData(ReadWriteMutex& rwmutex, std::string& msg, int &lastLen) {
    UniqueReadLock<ReadWriteMutex> readLock(rwmutex);

    if (lastLen != msg.size()) {
        lastLen = msg.size();
        std::cout << ">>> [ " << lastLen << " ] " << msg << std::endl;
    }
}

void writeData(ReadWriteMutex& rwmutex, std::string& msg, std::string in) {
    UniqueWriteLock<ReadWriteMutex> writeLock(rwmutex);
    msg += "_" + in;
}

void threadRead(ReadWriteMutex& rwmutex, std::string& msg, std::atomic<bool>& end) {
    int len = 0;
    while (!end) {  // 注意这个其实是一个死循环
        readData(rwmutex, msg, len);
        std::this_thread::sleep_for(std::chrono::milliseconds(10));
    }
}

void threadWrite() {
    std::string msg = "hello";
    ReadWriteMutex rwmutex;
    std::atomic<bool>end(false);
    std::thread th_read(threadRead, std::ref(rwmutex), std::ref(msg), std::ref(end));  // 使用引用技术 所以是 std::ref()

    int n = 10;

    for (int i = 0; i < n; i++) {
        std::string newMsg;
        std::cout << std::endl << "[" << i + 1 << '/' << n << "] : ";
        std::cin >> newMsg;
        writeData(rwmutex, msg, newMsg);
    }

    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    end = true;
    th_read.join();
}

int main() {
    std::thread th_write(threadWrite);
    th_write.join();
    return 0;
}
```



#### 互斥锁和自旋锁实现

* 是最基础的锁，所以不能使用锁实现这两个锁

* 采用 CAS （compare and swap）机制实现锁，是一种无锁实现机制，主要是基于 c++ 11 新增的原子操作类 Atomic 中的 compare_exchange_weak 和 compare_exchange_strong 这两个CAS 操作，这两个交换操作是原子的。

* compare_exchange_weak 函数 ： 参数会要求在这里传入期待的数值和新的数值。它们对比变量的值和期待的值是否一致，如果是，则替换为用户指定的一个新的数值。如果不是，则将变量的值和期待的值交换。

* ```cpp
  bool CAS(V, A, B) // 成功修改 V 为 B 
  {
      if (V == A)
      {
          swap(V, B);
          return true;
      }
      
      return false;
  }
  // 需要强调的是上面的操作是原子的，要么不做，要么全部完成。
  ```

* **CAS 操作（原子操作）**的情况下如何实现一个自旋锁呢？首先回忆自旋锁的用途，本质上我们是希望能够让一个线程在不满足进入临界区的条件时，**不停的忙等轮询**，直到可以运行的时候再继续（进入临界区）.

* ```cpp
  b = true;
  while(!CAS(flag, false, b));
  //do something
  flag = false;
  // 判断操作与写入操作已经成为了一个整体，当一个线程的CAS操作成功的时候会阻止其他线程进入临界区，到达互斥访问的目的。
  // 因为如果 flag 变成 false 了表示成功,与此同时将 flag = true 整个过程是原子的，这样别的线程是进不来的
  ```

* ```cpp
  ai= 3;
  
  // tst_val != ai   ==>  tst_val is modified
  //这个时候由于ai为3，tst_val为4，所以对比就会失败，那么tst_val就会被修改为3, exchanged false
  exchanged= ai.compare_exchange_strong( tst_val, new_val );
  //这个时候对比就会成功，那么ai的值就会被修改为new_val
  // tst_val == ai   ==>  ai is modified, exchanged 为 true
  exchanged= ai.compare_exchange_strong( tst_val, new_val );
  
  ```

* ```cpp
  //  Spinlock.cpp
  #include<bits/stdc++.h>
  
  class SpinLock {
  public:
      SpinLock() : flag_(false) {}
      ~SpinLock() = default;
      void lock() {
          bool expected = false;
          while (!flag_.compare_exchange_weak(expected, true)) { // compare_exchange_weak 原子操作
              // 这就是阻塞在这里了，表示的是 flag_ == true, 因为之前已经有对象拿到锁了
              // 那么你只能阻塞在这里， flag_ 只有拿到锁的人才可以将其设置成 false;
              expected = false;
              // 这里一定要将expect复原，执行失败时expect结果是未定的
          }
      }
  
      void unlock() {
          flag_.store(false);  // 这个也是原子操作，原子赋值操作
      }
  private:
      std::atomic<bool>flag_;
  };
  
  // 每个线程自增的数量
  const int kIncNum = 1000000;
  // 线程数
  const int kWorkNum = 10;
  // 自增计数器
  int count = 0;
  SpinLock spinLock;
  
  void IncFunc() {
      for (int i = 0; i < kIncNum; i++) {
          spinLock.lock();
          count++;
          spinLock.unlock();
      }
  }
  
  int main() {
      std::vector<std::thread>workers;
      count = 0;
      // 创建十个工作线程执行自增的操作, 每一个线程都会执行自增的操作
  
      for (int i = 0; i < kWorkNum; i++) {
          workers.push_back(std::move(std::thread(IncFunc)));
      }
  
      for (auto& c : workers) {
          c.join();
      }
  
      std::cout << "count is : " << count << std::endl;
      if (count == kWorkNum * kIncNum) {
          std::cout << "Success!\n";
      } else {
          std::cout << "Failed!\n";
      }
  
      return 0;
  }
  ```

* ```cpp
  // mutex.cpp
  // 依旧是基于 atomic 来实现的
  #include<bits/stdc++.h>
  
  class Mutex{
  public:
      Mutex() : alock(ATOMIC_FLAG_INIT) {}
      ~Mutex() = default;
  
      void lock() {
          while (true == alock.test_and_set()) {}
      }
  
      void unlock() {
          alock.clear();
      }
  private:
      std::atomic_flag alock;
  };
  
  int count = 0;
  Mutex mtx;
  
  void foo() {
      for (int i = 0; i < 100000; i++) {
          mtx.lock();
          // std::cout << "foo : " << count << std::endl;
          count++;
          mtx.unlock();
      }
  }
  
  int main() {
      std::vector<std::thread>workers;
      for (int i = 0; i < 10; i++) {
          // 开启十个线程
          workers.push_back(std::move(std::thread(foo)));
      }
  
      for (int i = 0; i < 10; i++) {
          workers[i].join();
      }
  
      if (count == 100000 * 10) {
          std::cout << "Success\n";
      } else {
          std::cout << "Failed\n";
      }
      return 0;
  }
  ```

* 



##### 区别

* **互斥锁**加锁失败后，线程会**释放 CPU** ，给其他线程；

* **自旋锁**加锁失败后，线程会**忙等待**，直到它拿到锁；

* 互斥锁是一种「独占锁」，比如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只要线程 A 没有释放手中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，**既然线程 B 释放掉了 CPU，自然线程 B 加锁的代码就会被阻塞**。

  **对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的**。

* ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E4%BA%92%E6%96%A5%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

* Q : 线程的上下文切换的是什么？

* A : 当两个线程是属于同一个进程，**因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。**上下文切换时间是： 几十 纳秒到几微米

* **如果你能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。**

* 





#### 乐观锁和悲观锁

* 悲观锁做事比较悲观，它认为**多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁**。

* 乐观锁： 如果多线程同时修改共享资源的概率比较低，就可以采用乐观锁。(无锁编程)

* > * 工作流程: 
  >
  > * > * **先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作**。
  >
  > * 使用场景：
  >
  > * > * 虽然**重试的成本很高**，但是冲突的概率足够低的话，还是可以接受的。
  >   > * 在线文档
  >   > * 常见的 SVN 和 Git 也是用了乐观锁的思想 ：先让用户编辑代码，然后提交的时候，通过版本号来判断是否产生了冲突，发生了冲突的地方，需要我们自己修改后，再重新提交。
  >   > * 乐观锁虽然去除了加锁解锁的操作，但是一旦发生冲突，重试的成本非常高，所以**只有在冲突概率非常低，且加锁成本非常高的场景时，才考虑使用乐观锁。**

* 



#### 用户态到内核态切换

* 而用户态就是提供应用程序运行的空间，为了使应用程序访问到内核管理的资源例**如CPU，内存，I/O**。内核必须提供一组通用的**访问接口**，这些接口就叫**系统调用**。

* 用户态到内核态的三种方式：

* > * 1、系统调用，这个上面已经讲解过了。其实系统调用本身就是**中断**，但是软件中断，跟硬中断不同。
  > * 2、异常：如果当前进程运行在用户态，如果这个时候发生了异常事件，就会触发切换。例如：**缺页异常**。
  > * 3、外设中断：当外设完成用户的请求时，会向CPU发送中断信号。



##### 切换过程

* 当前进程在用户态用汇编指令 int <中断向量号> 发出中断请求，这个中断请求里面有中断向量。在系统内存中存放着“中断向量表”，这个中断向量表里面存放着各种中断描述符，中断描述符里存放着中断服务函数的入口地址，以及他所需要的特权级别，内核态的特权级为0，用户态为3。
* CPU 在总线上捕捉到中断向量 ，通过寄存器 IDTR （里面保存了中断向量表的首地址和长度）找到中断向量表，然后找到中断向量对应的中断描述符，进行权限的验证。
* 如果权限符合（中断描述符里面一般有四类中断服务函数是用户态可以调用的：断点、溢出、边界检查、系统调用），接下来**用户栈切换到内核栈**，因为这两个栈里面保存的代码和数据都是不同的，切换的关键在于栈地址的切换，而用户栈和内核栈的栈地址都保存在了”任务状态段“TSS中。**从用户栈切换到内核栈之后，就完成了从用户态到核心态的切换**（之后可能就要去执行中断服务函数了）。











#### 进程切换和线程切换

* 在任何一个时刻，**单处理器系统都只能执行一个进程的代码**。

* > * 操作系统实现这种交错执行的机制称为**上下文切换**。
  > * 操作系统保持跟踪**进程运行所需的所有状态信息**，这种状态，也就是**上下文**，它包括许多信息，例如PC和寄存器文件的当前值，以及主存的内容。



##### 进程切换

* 进程上下文： **其实就是整个虚拟地址空间里的东西，包括用户空间和内核空间**

* **PCB也在内核栈里面**，一般切换时信息都是保存在PCB中，PCB装不下就用个指针指向内核栈进行保存。

* 上下文是由程序正确运行所需的状态组成的，这个状态主要是存放在存储器中的程序的代码和数据、用户栈，**通用目的寄存器**的内容，程序计数器，环境变量以及打开**文件描述符的集合、内核栈**。

* > * 通用目的寄存器
  > * 浮点寄存器
  > * 程序计数器
  > * 用户栈
  > * 状态寄存器
  > * 内核栈
  > * 各种内核数据结构：比如描绘地址空间的页表，包含有关当前进程信息的进程表，以及包含进程已打开文件的信息的文件表





##### 线程切换

* 虚拟内存： 是操作系统为每个进程提供的一种抽象，每个进程都有属于自己的、私有的、地址连续的虚拟内存，当然我们知道最终进程的数据及代码必然要放到物理内存上，那么必须有某种机制能记住虚拟地址空间中的某个数据被放到了哪个物理内存地址上，这就是所谓的地址空间映射，那么操作系统是如何记住这种映射关系的呢，答案就是页表。
* 为了加速页表，还引入了高速缓存TLB
* 页表是放在内存中，**每个进程有自己的页表**；**TLB放在 CPU MMU中，是进程间共享的**



##### 区别

* **进程切换涉及虚拟地址空间的切换而线程不会**
* 为什么虚拟地址空间切换会比较耗时呢？因为**cache和TLB会失效**
* 每次上下文切换时都会刷新整个TLB，这可能会非常昂贵，因为这意味着新进程不得不经历缺页、查找页表和插入条目整个过程
* 有些处理器，会给每个TLB项添加一个额外的唯一字段`ASID(address space ID)`，这意味这每个地址空间都有自己的ID，且标记在TLB上。因此**上下文切换的时候TLB不需要被刷新**，新来的进程讲会有不同的**地址空间ID**，甚至可以请求相同的虚拟地址，因为地址空间ID不同，翻译之后的物理地址也会不同。这种方式能减少清空、增加系统性能，但是需要**更多的TLB硬件来存储ASIB位**

















#### CPU 基本知识

##### 大端小端

* 大端和小端

	> * 数据在内存中的存储方式：大端和小端 (和 CPU 的架构相关)
	>
	> * 大端： 高字节存放在内存的低地址
	>
	> * 小端： 低字节存放在内存的低地址 （通常使用的是小端）Intel X86 和 ARM
	>
	> * ![img](https://img-blog.csdnimg.cn/20190825193434559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d3d2x5ajEyMzMyMQ==,size_16,color_FFFFFF,t_70)
	>
	> * 上图表示的是 int 型整数： 0x12345678
	>
	> * 如何判断大端还是小端，可以使用写代码的方式：
	>
	> * ```cpp
	> 	union myunion {
	> 	    int a;
	> 	    char b;
	> 	};
	> 	int small_large() {
	> 	    int x = 0x12345678;
	> 	    union myunion my;
	> 	    my.a = x;
	> 	    if (my.b == 0x78) {
	> 	    // 返回的是小端
	> 	    	return 1;
	> 	    }
	> 	    // 返回的是大端
	> 	    return 0;
	> 	}
	> 											
	> 	```
	>
	
* 网络字节序：

  > * 大部分网络协议采用的都是大端进行传输操作。可以理解网络中字节按照大端传输
  >
  > * 小端系统为了数据能在网络中传输，需要转化（大端系统一般不需要）
  >
  > * > * htons —— 把unsigned short类型从主机序转成网络字节序 (host to network short)
  >   > * ntohs —— 把unsigned short类型从网络字节序转成主机序
  >   > * htonl —— 把unsigned long类型从主机序转成网络字节序
  >   > * ntohl —— 把unsigned long类型从网络字节序转成主机序















### 网络编程

#### Socket 参数

* 端口 和 IP 的字节序转换

```cpp
/*

    网络通信时，需要将主机字节序转换成网络字节序（大端），
    另外一段获取到数据以后根据情况将网络字节序转换成主机字节序。

    转换端口， 端口是 16 位， 所以总共存在 65536 个端口
    short 是 2 个字节在 c/c++ 中
    uint16_t htons(uint16_t hostshort);		// 主机字节序 - 网络字节序
    uint16_t ntohs(uint16_t netshort);		// 网络字节序 - 主机字节序

    转IP， IP 地址是 32 位
    uint32_t htonl(uint32_t hostlong);		// 主机字节序 - 网络字节序
    uint32_t ntohl(uint32_t netlong);		// 网络字节序 - 主机字节序

*/
```







#### Socket & 文件描述符

* Linux 文件描述符（fd): 一个 Linux 进程可以打开很多个文件，为了表示和区分这些文件，Linux 会为每一个文件分配一个 整数编号，这个编号就是文件描述符，表示文件身份的作用。

* **Linux 文件描述符保存在内核区，由 PCB 进行控制的，通过使用 fd 找到指定的文件的位置.**

* PCB 每一个进程都存在一个，所以我们可以使用 PCB 去管理文件描述符表。文件描述符表的前三个是标准输入，标准输出，标准错误 (这三个默认打开），剩下的1021 个是存放的是文件描述符.

* 每次打开一个文件，都会占用一个文件描述符，而且是空闲的最小的文件描述符

* ![Linux文件描述符表示意图](http://c.biancheng.net/uploads/allimg/190410/1-1Z4101H45S13.gif)

* 然而，要想真正读写文件，还得通过打开文件表的 i-node 指针进入 i-node 表，inode 表包含一下信息：
	- 文件类型，例如常规文件、套接字或 FIFO。
	- 文件大小。
	- 时间戳，比如创建时间、更新时间。
	- 文件锁。

* 文件表包含以下信息：
	- 文件偏移量，也就是文件内部指针偏移量。调用 read() 或者 write() 函数时，文件偏移量会自动更新，当然也可以使用 lseek() 直接修改。
	- 状态标志，比如只读模式、读写模式、追加模式、覆盖模式等。
	- i-node 表指针。

* socket 编程的基本步骤：

	> * 创建一个 socket
	> * 识别这个 socket
	> * 在服务器上，等待输入的连接
	> * 发送以及接收信息
	> * 关闭 socket

* Step1 ： 创建一个 socket(是由 socket 系统调用获得的)

	> * 创建套接字可以类比成是向电话公司请求电话线。
	>
	> ```c
	> int server_fd = socket(domain, type, protocol);
	> // 所有传入的参数和返回值都是整数.
	> // 这个文件描述符表示的是监听 socket
	> ```
	>
	> * domain or address family : AF_INET(IP), AF_INET6(IPv6), AF_UNIX(local channel, similar to pipe),AF_ISO(ISO 协议)，AF_NS(施乐网络协议）
	>
	> * type :服务的类型，SOCK_STREAM(虚拟环路服务)，SOCK_DGRAM(数据包服务), SOCK_RAW(直接 IP 服务),
	>
	> * protocol：指示用于支持套接字操作的特定协议。
	>
	> * ```cpp
	> 	#include <sys/socket.h>
	> 	if ((fd = socket(AF_INET, SOCK_STREAM, 0)) < 0) { // 套接字创建失败
	> 	  	perror("cannot create socket")
	> 	    return 0;
	> 	}
	> 	```

* Step 2: 识别 socket  （bind 系统调用）

	> * 在套接字中，我们称这种操作是绑定地址，并且为此使用绑定系统调用。
	>
	> * ```cpp
	> 	int bind(int socket, const struct sockaddr* address, socklen_t address_len);
	> 	```
	>
	> * Socket: 使用 socket 系统调用获得的。
	>
	> * Sockaddr : 通用的容器，只允许操作系统能够读取标识地址族的前几个字节.对于 IP 网络，我们使用 struct sockaddr_in 
	>
	> * ```cpp
	> 	struct sockaddr_in{
	> 	  	__uint8_t sin_len;
	> 	  	sa_family_t sin_family; // 例如 AF_INET
	> 	  	in_port_t sin_port; // 端口号，如果是客户端，并且不会接入连接，那么直接使用默认的端口号，或者 OS 分配的任意端口号，如果是服务器，则需要分配特定的端口号，因为客户端需要知道要连接的端口号。
	> 	  	struct in_addr sin_addr; // 机器的地址，就是机器的IP地址，对于每一个网络端口只有一个 IP， 如果含有 WIFI 以及以太网连接，那么我们会有两个 地址，
	> 	  	char sin_zero[8]; // 这个就是 sizeof(struct sockaddr_in)
	> 	};
	> 	```
	>
	> * ```cpp
	> 	#include <sys/socket.h>
	> 	struct socket_in address;
	> 	const int PORT = 8080;  // 客户端可以到达的 端口号
	> 	/* htonl converts a long integer (e.g. address) to a network representation */ 
	> 	/* htons converts a short integer (e.g. port) to a network representation */ 
	> 	/*
	> 	int a = 'F';
	> 	cout << (char *)&a << " " << a << endl;
	> 	// 首先是 a 地址的数据取地址操作，之后对其解引用，然后对其强制类型转换成 char
	> 	// 输出 ： 'F'  70
	> 	cout << (char) a << " " << a << endl; // 和上面输出是一样的。
	> 	*/
	> 	memset((char* )&address, 0, sizeof(address));
	> 	address.sin_family = AF_INET;
	> 	address.sin_addr.s_addr = htonl(INADDR_ANY);
	> 	address.sin_port = htons(PORT);
	> 											
	> 	if (bind(server_fd, (struct sockaddr *)&address, sizeof(address)) < 0) {
	> 	    perror("bind failed");
	> 	    return 0;
	> 	}
	>

* Step 3 : 在服务端，等待输入连接 （listen & accept系统调用)

	> * 在客户端可以连接到服务器之前，服务端有一个 socket 准备接收这个连接，listen 系统调用告诉一个 socket ,他有能力接收这个输入的连接
	>
	> * ```cpp
	> 	#include <sys/socket.h>
	> 	int listen(int socket, int backlog);
	> 	// backlog 表示的是在连接被拒绝之前，最多排队连接的数量
	>

>* accept 系统调用就是：被阻塞在队列中的第一个第一个连接请求，**同时为这个连接创建一个新的 socket**
>
>* **为侦听而设置的原始套接字仅用于接受连接**，而不用于交换数据。 默认情况下，套接字操作是同步的或阻塞的，accept 将阻塞直到队列中存在可用连接。
>
>* ```cpp
>	#include <sys/socket.h>
>	int accept(int socket, struct sockaddr* restrict, socklen_t *restrict address_len // 返回一个全新的文件描述符，代表的是与客户端简历的 TCP 连接，称之为已连接 socket
>

>* socket: 设置为通过listen 接受连接的套接字。
>
>* Address: 是客户端执行 connect 的地址，第三个参数，是地址结构的长度
>
>* ```cpp
>	if (listen(server_fd, 3) <  0) {
>	  	perror("In listen");
>	  	exit(EXIT_FAILURE);
>	}
>	if ((new_socket = accept(server_fd, (struct sockaddr*)&address, (socklen_t *)&addrlen)) < 0) { // 返回的是客户端的 socket_fd
>	  	perror("In Accept");
>	  	exit(EXIT_FAILURE);
>	}
>	```

* Step 4 : 接收和发送数据:

	> * 最终我们的在客户端和服务端之间连接了一个 socket。
	>
	> * 通信的方式是非常简单的，我们使用read 和 write 系统调用，进行数据流的读写操作，就是传输的操作
	>
	> * ```cpp
	> 	char buffer[1024] = {0};
	> 	int valued = read(new_socket, buffer, 1024);
	> 	printf("%s\n", buffer);
	> 	if (valued < 0) {
	> 	  	printf("No bytes are read\n");
	> 	}
	> 	char* hello = "Hello from the server";
	> 	write(new_socket, hello, strlen(hello));
	> 	```

* Step 5: 关闭连接（close 系统调用）

	> * 当通信完成之后，我们直接使用close 对 socket 进行关闭，close 是系统调用，就和关闭文件的操作是一样的。
	>
	> * ```cpp
	> 	close(new_socket)
	> 	```
	>
	> * 

* 

* socket 是最流行的操作系统提供给程序访问网络的机制，它允许不同联网机器之间的进程可以进行通信操作。

![基于 TCP 协议的客户端和服务器工作](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzM0LmpwZw?x-oss-process=image/format,png)



* 基本流程：

	> * 服务端和客户端初始化 socket, 得到文件描述符。
	> * 服务端调用 bind ,将绑定在 IP 地址和端口。将socket 创建的 文件描述符和本机的 IP 地址以及端口号进行绑定相连操作。**将套接字绑定到服务器的 IP 地址和端口上**
	> * 服务端调用 listen ，进行监听。
	> * 服务端调用 accept, 等待客户端连接。
	> * 客户端调用 connect, 向服务器端的地址和端口发送连接请求。
	> * 服务端 accept 返回用于传输的 socket 文件描述符。（后续用来传输数据用的）
	> * 客户端调用 write 写入数据，服务端调用 read 读取数据。
	> * 客户端断开连接的时候，会调用 close， 那么服务端 read 数据的时候，就会读取到 EOF，等待服务端处理完数据之后，会调用 close 断开连接。

* 服务端调用 accept 时，连接成功了会返回一个已经连接的 socket, 后续用来传输数据.

* 监听使用的 socket 和真正用来传输数据的 socket 是两个 socket, 一个叫做**监听 socket**， 一个叫做：**已完成连接socket**。

* 客户端首先调用 connect 之后 connect 阻塞一直等服务端完成握手，第一次握手服务端收到了，调用 accpet 表示接收请求，两次握手结束之后，服务端 connect 返回，所以 客户端 connect连接成功，服务端在三次握手之后，accept 成功返回，返回的是用于传输 socket 文件描述符.

* listen 时候的参数： backlog 的意义?

	> * Linux 内核会维护两个队列：
	> * 半连接队列（SYN 队列）： 接收到一个 SYN 连接请求（第一次连接），结束之后处于：SYN_RCVD 状态
	> * 全连接队列（Accept 队列）： 三次握手完成之后，处于 established 状态
	>
	> ![ SYN 队列 与 Accpet 队列 ](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzM1LmpwZw?x-oss-process=image/format,png)

```cpp
int listen(int socketfd, int backlog)
// socketfd: socket 文件描述符
// backlog ：已完成连接的队列长度.
```

#### TCP socket 代码：

```cpp
// server:
#include <stdio.h>
#include <sys/socket.h>
#include <unistd.h>
#include <stdlib.h>
#include <netinet/in.h>
#include <string.h>

#define PORT 8081

int main(int argc, char const *argv[])
{
    int server_fd, new_socket; long valread;
    struct sockaddr_in address;
    int addrlen = sizeof(address);

    char *hello = "Hello from server";

    // Creating socket file descriptor
    if ((server_fd = socket(AF_INET, SOCK_STREAM, 0)) == 0)
    {
        perror("In socket");
        exit(EXIT_FAILURE);
    }


    address.sin_family = AF_INET;
    address.sin_addr.s_addr = INADDR_ANY;
    address.sin_port = htons( PORT );

    memset(address.sin_zero, '\0', sizeof address.sin_zero);


    if (bind(server_fd, (struct sockaddr *)&address, sizeof(address))<0)
    {
        perror("In bind");
        exit(EXIT_FAILURE);
    }
    if (listen(server_fd, 10) < 0)
    {
        perror("In listen");
        exit(EXIT_FAILURE);
    }
    if ((new_socket = accept(server_fd, (struct sockaddr *)&address, (socklen_t*)&addrlen))<0)
    {
        perror("In accept");
        exit(EXIT_FAILURE);
    }
    while(1) // 一直用于接收和发送数据
    {
        printf("\n+++++++ Waiting for new connection ++++++++\n\n");
        char buffer[30000] = {0};
        valread = read( new_socket , buffer, 30000);
        printf("%s\n",buffer );
        write(new_socket , hello , strlen(hello));
        printf("------------------Hello message sent-------------------\n");
        // close(new_socket);
    }
    close(new_socket);
    return 0;
}
// Client:
#include <stdio.h>
#include <sys/socket.h>
#include <stdlib.h>
#include <unistd.h>
#include <netinet/in.h>
#include <string.h>
#include <arpa/inet.h>

#define PORT 8081

int main(int argc, char const *argv[])
{
    int sock = 0; long valread;
    struct sockaddr_in serv_addr;
    char *hello;
    char buffer[1024] = {0};
    if ((sock = socket(AF_INET, SOCK_STREAM, 0)) < 0)
    {
        printf("\n Socket creation error \n");
        return -1;
    }

    memset(&serv_addr, '0', sizeof(serv_addr));

    serv_addr.sin_family = AF_INET;
    serv_addr.sin_port = htons(PORT);

    // Convert IPv4 and IPv6 addresses from text to binary form
    if(inet_pton(AF_INET, "10.19.125.114", &serv_addr.sin_addr)<=0)
    {
        printf("\nInvalid address/ Address not supported \n");
        return -1;
    }

    if (connect(sock, (struct sockaddr *)&serv_addr, sizeof(serv_addr)) < 0)
    {
        printf("\nConnection Failed \n");
        return -1;
    }
    while (1) { // 一直传输接收数据
        printf("input: ");
        gets(hello);
        send(sock , hello , strlen(hello) , 0 );
        printf("Hello message sent\n");
        valread = read( sock , buffer, 1024);
        printf("%s\n",buffer );
    }

    return 0;
}
```



#### UDP socket代码

```cpp
// server: 
#include <stdio.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <stdlib.h>

#define SERV_PORT   8000

int main()
{
  /* sock_fd --- socket文件描述符 创建udp套接字*/
  int sock_fd = socket(AF_INET, SOCK_DGRAM, 0);
  if(sock_fd < 0)
  {
    perror("socket");
    exit(1);
  }

  /* 将套接字和IP、端口绑定 */
  struct sockaddr_in addr_serv;
  int len;
  memset(&addr_serv, 0, sizeof(struct sockaddr_in));  //每个字节都用0填充
  addr_serv.sin_family = AF_INET;  　　　　　　　　　　　 //使用IPV4地址
  addr_serv.sin_port = htons(SERV_PORT);  　　　　　　　 //端口
  /* INADDR_ANY表示不管是哪个网卡接收到数据，只要目的端口是SERV_PORT，就会被该应用程序接收到 */
  addr_serv.sin_addr.s_addr = htonl(INADDR_ANY);  //自动获取IP地址
  len = sizeof(addr_serv);

  /* 绑定socket */
  if(bind(sock_fd, (struct sockaddr *)&addr_serv, sizeof(addr_serv)) < 0)
  {
    perror("bind error:");
    exit(1);
  }


  int recv_num;
  int send_num;
  char send_buf[20] = "i am server!";
  char recv_buf[20];
  struct sockaddr_in addr_client;

  while(1)
  {
    printf("server wait:\n");

    recv_num = recvfrom(sock_fd, recv_buf, sizeof(recv_buf), 0, (struct sockaddr *)&addr_client, (socklen_t *)&len);

    if(recv_num < 0)
    {
      perror("recvfrom error:");
      exit(1);
    }

    recv_buf[recv_num] = '\0';
    printf("server receive %d bytes: %s\n", recv_num, recv_buf);

    send_num = sendto(sock_fd, send_buf, recv_num, 0, (struct sockaddr *)&addr_client, len);

    if(send_num < 0)
    {
      perror("sendto error:");
      exit(1);
    }
  }

  close(sock_fd);

  return 0;
}

// client:
#include <stdio.h>
#include <string.h>
#include <errno.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>


#define DEST_PORT 8000
#define DSET_IP_ADDRESS  "127.0.0.1"


int main()
{
  /* socket文件描述符 */
  int sock_fd;

  /* 建立udp socket */
  sock_fd = socket(AF_INET, SOCK_DGRAM, 0);
  if(sock_fd < 0)
  {
    perror("socket");
    exit(1);
  }

  /* 设置address */
  struct sockaddr_in addr_serv;
  int len;
  memset(&addr_serv, 0, sizeof(addr_serv));
  addr_serv.sin_family = AF_INET;
  addr_serv.sin_addr.s_addr = inet_addr(DSET_IP_ADDRESS);
  addr_serv.sin_port = htons(DEST_PORT);
  len = sizeof(addr_serv);


  int send_num;
  int recv_num;
  char send_buf[20] = "hey, who are you?";
  char recv_buf[20];

  printf("client send: %s\n", send_buf);

  send_num = sendto(sock_fd, send_buf, strlen(send_buf), 0, (struct sockaddr *)&addr_serv, len);

  if(send_num < 0)
  {
    perror("sendto error:");
    exit(1);
  }

  recv_num = recvfrom(sock_fd, recv_buf, sizeof(recv_buf), 0, (struct sockaddr *)&addr_serv, (socklen_t *)&len);

  if(recv_num < 0)
  {
    perror("recvfrom error:");
    exit(1);
  }

  recv_buf[recv_num] = '\0';
  printf("client receive %d bytes: %s\n", recv_num, recv_buf);

  close(sock_fd);

  return 0;
}
```









### 设计模式

#### 单例模式

##### 使用场景

* 







* 单例模式指在整个系统生命周期里，**保证一个类只能产生一个实例**，确保该类的**唯一性**。

* > * 构造函数和析构函数为**private**类型，目的**禁止**外部构造和析构
  > * 拷贝构造和赋值构造函数为**delete**，目的是**禁止**外部拷贝和赋值，确保实例的唯一性
  > * 类里有个获取实例的**静态函数**，可以全局访问

* 懒汉式：指系统运行中，实例并不存在，**只有当我们需要这个实例的时候，我们才会考虑创建同时使用这个实例**（这个时候需要考虑线程安全的问题）
* 饿汉式：指这个**系统一运行，就初始化创建这个实例**，当需要的时候，直接调用就可以了（**本身就是线程安全的，不用考虑线程安全的问题**）
* 利用 c++11 static 是线程安全的特性写的这个代码, 只创建了一次实例。（懒汉式）

##### 懒汉

```cpp
#include<bits/stdc++.h>
using namespace std;

class Singleton{
public:
    ~Singleton() {
        cout << "Deconstructor this singleton" << endl;
    }
    Singleton(const Singleton&) = delete;
    Singleton& operator = (const Singleton& ) = delete;
    static Singleton& get_instance() { // 返回的是引用而不是指针，因为需要避免被外部对象调用 delete 的风险，导致对象被提前销毁。
        static Singleton instance; // c++ 11 之后 static 是一个线程安全的关键字
        return instance;
    }
  	void PrintAddr() {
        cout << "this instance addr is : " << this << endl;
    }
private:
    Singleton() {
        cout << "constructor this singleton" << endl;
    }
};

/*
    打印结果：
    Singleton()
    0x55be333b8152
    0x55be333b8152
    0x55be333b8152
    ~Singleton()
*/

int main() {
    Singleton& instance1 = Singleton::get_instance();
    Singleton& instance2 = Singleton::get_instance();
    Singleton& instance3 = Singleton::get_instance();
  	instance1.PrintAddr();
    instance2.PrintAddr();
    instance3.PrintAddr();
  	// 以上三个地址打印出来的应该是一样的，因为只有一个实例存在.
    cin.get();
    return 0;
}
```

* 使用指针加锁的形式实现线程安全

```cpp
#include<bits/stdc++.h>

using namespace std;

class Singleton{
public:
    typedef shared_ptr<Singleton> Ptr;
    ~Singleton() {
        cout << "Deconstructor this singleton" << endl;
    }
    Singleton(Singleton& ) = delete;
    Singleton& operator = (const Singleton& ) = delete;
    static Ptr get_instance() {
        if (m_instance == nullptr) {
            // 两层循环，是为了防止每次调用 get_instance 都加锁，因为加锁是有一定代价的，消耗的，所以先判断再加锁，减少加锁的次数
            lock_guard<mutex> lock(m_mutex); // RAII 特性
            if (m_instance == nullptr) {
                m_instance = shared_ptr<Singleton>(new Singleton);
                // 使用共享指针的 RAII 特性，当离开作用域，这个 Singleton 自动调用析构函数，使用 共享指针的 RAII 特性
            }
        }
        return m_instance;
    }
  	void PrintAddr() {
        cout << "this instance addr is: " << this << endl;
        cout << m_instance << endl;
    }
private:
    Singleton() {
        cout << "Constructor this singleton" << endl;
    }
    static mutex m_mutex;
    static Ptr m_instance;
};

// static 变量需要在类外面进行初始化，因为 static 变量属于整个 class
mutex Singleton::m_mutex;
Singleton::Ptr Singleton::m_instance = nullptr;

class Singleton1 {
public:
    ~Singleton1() {
        cout << "Deconstructor this singleton1!!!" << endl;
    }
    Singleton1(Singleton1& ) = delete; // 拷贝构造函数 传入的是引用，避免无限次的调用
    Singleton1& operator = (const Singleton1& ) = delete;
    static Singleton1& get_instance() {
        if (m_instance == nullptr) {
            lock_guard<mutex>lock(m_mutex);
            if (m_instance == nullptr) {
                m_instance = new Singleton1;
            }
        }
        return *m_instance;
    }
    static void deleteinstance() {
        lock_guard<mutex>lock(m_mutex);
        if (m_instance) {
            delete m_instance;
            m_instance = NULL;
        }
    }
  	void PrintAddr() {
        cout << "this instance addr is: " << this << endl;
        cout << m_instance << endl;
    }
private:
    Singleton1() {
        cout << "Constuct this singleton1!!!" << endl;
    }
    static mutex m_mutex;
    static Singleton1* m_instance;
};

mutex Singleton1::m_mutex;
Singleton1* Singleton1::m_instance = nullptr;

int main() {
    Singleton::Ptr instance1 = Singleton::get_instance();
    Singleton::Ptr instance2 = Singleton::get_instance();
    Singleton::Ptr instance3 = Singleton::get_instance();
    // Singleton1& instance1 = Singleton1::get_instance();
    // Singleton1& instance2 = Singleton1::get_instance();
    // Singleton1& instance3 = Singleton1::get_instance();
    instance1 -> PrintAddr();
    instance2 -> PrintAddr();
    instance3 -> PrintAddr();
    // 以上三个地址应该是一样的，因为只存在一个实例
    instance1.deleteinstance();
    instance1 -> PrintAddr();
    instance2 -> PrintAddr();
    instance3 -> PrintAddr();
    /*
    Singleton1()
    0x561a224eaeb0
    0x561a224eaeb0
    0x561a224eaeb0
    0x561a224eaeb0
    0x561a224eaeb0
    0x561a224eaeb0
    ~Singleton1()
    0x561a224eaeb0
    0
    0x561a224eaeb0
    0
    0x561a224eaeb0
    0
    
    */
    return 0;
}
```



##### 恶汉

```cpp

class Singleton2 {
public:
    ~Singleton2() {
        std::cout << "~Singleton2()" << std::endl;
    }
    Singleton2(const Singleton2&) = delete;
    Singleton2& operator = (const Singleton2& ) = delete;
    static Singleton2& getInstance() {
        return *m_instance;
    }
    void printAddr() {
        std::cout << this << std::endl;
        std::cout << m_instance << std::endl;
    }
    static void deleteInstance() {
        if (m_instance) {
            delete m_instance;
            m_instance = nullptr;
        }
    }
private:
    Singleton2() {
        std::cout << "Singleton2()" << std::endl;
    }
    static Singleton2* m_instance;
};

// 代码一运行就初始化创建实例 ，本身就线程安全,
// 所以 Singleton2() 一定是最先打印出来的，因为初始化是要早于类成员实例化的

Singleton2* Singleton2::m_instance = new Singleton2;

void TestCaseHungry() {
    Singleton2& instance1 = Singleton2::getInstance();
    Singleton2& instance2 = Singleton2::getInstance();
    Singleton2& instance3 = Singleton2::getInstance();
    instance1.printAddr();
    instance2.printAddr();
    instance3.printAddr();
    instance1.deleteInstance();
    instance1.printAddr();
    instance2.printAddr();
    instance3.printAddr();
}


int main() {
    std::cout << "ok" << std::endl;
    // TestCase();
    // TestCaseLock();
    TestCaseHungry();
    return 0;
}
/*
打印结果：
Singleton2()
ok
0x55d0c1f43eb0
0x55d0c1f43eb0
0x55d0c1f43eb0
0x55d0c1f43eb0
0x55d0c1f43eb0
0x55d0c1f43eb0
~Singleton2()
0x55d0c1f43eb0
0
0x55d0c1f43eb0
0
0x55d0c1f43eb0
0
*/
```





#### 工厂模式





### Linux

#### Page Cache

##### 如何用数据观测 Page Cache

* 最简单的理解方式，**buffer是缓存裸设备的读写，cache是缓存文件系统的读写**。

* page cache 可能造成的问题

* > * 服务器的 load 飙高；
  > * 服务器的 I/O 吞吐飙高；
  > * 业务响应时延出现大的毛刺；
  > * 业务平均访问时延明显增加。

* 什么是 page cache

* <img src="https://static001.geekbang.org/resource/image/f3/1b/f344917f3cacd5bc06ae7c743a217f1b.png?wh=2860*2440" alt="img" style="zoom:50%;" />

* 很明显，**Page Cache 是内核管理的内存，也就是说，它属于内核不属于用户**。

* ```shell
  
  $ cat /proc/meminfo
  ...
  Buffers:            1224 kB
  Cached:           111472 kB
  SwapCached:        36364 kB
  Active:          6224232 kB
  Inactive:         979432 kB
  Active(anon):    6173036 kB
  Inactive(anon):   927932 kB
  Active(file):      51196 kB
  Inactive(file):    51500 kB
  ...
  Shmem:             10000 kB
  ...
  SReclaimable:      43532 kB
  ...
  ```

* Buffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached

* 在 Page Cache 中，**Active(file)+Inactive(file) 是 File-backed page（与文件对应的内存页）**，是你最需要关注的部分。因为你平时用的 **mmap() 内存映射方式和 buffered I/O 来消耗的内存就属于这部分**，最重要的是，这部分在真实的生产环境上也最容易产生问题

* **Page Cache 存在的意义：减少 I/O，提升应用的 I/O 速度。**

* Page Cache 对应用提升 I/O 效率而言是一个投入产出比较高的方案，所以它的存在还是有必要的。

* Q:mmap 产生的pageCache 也是内核态的吗

* A： **内核态和用户态是指进程的运行状态** 并不能用来表示内存。 应用程序通过mmap系统调用返回的地址空间属于“用户空间”。 mmap的用户空间的地址对应的物理内存则是page cache，这是不同的概念。



##### Page cache 怎样产生和释放的

* Page Cache 是**如何产生和释放的，通俗一点的说就是它的“生”（分配）与“死”（释放）**，即 Page Cache 的生命周期

###### 如何产生？

* Buffered I/O（标准 I/O）；
* Memory-Mapped I/O（存储映射 I/O）。
* mmap （存储映射 I/O）与标准io的选择要看具体的场景。很多情况下内存拷贝不会是瓶颈，比如说只写几个或者几百字节的情况下，所以使用哪种都可以；只有在内存拷贝成为瓶颈，比如读写大量文件内容的情况下，比如一次要读写几十上百M，mmap的优势才会提现出来。
* **标准io相对而言更方便些，在数据量不大的情况下可以使用**；如果数据量较大，此时使用存储映射io会更好些。另外一个考虑因为是对内存的精细管理，如果需要管理，比如说把某些数据锁定在内存中，这个时候使用存储映射io更好些。
* ![img](https://static001.geekbang.org/resource/image/4e/52/4eb820e15a5560dee4b847227ee75752.jpg?wh=3957*2600)
* 标准 I/O 是写的 (write(2)) **用户缓冲区** (Userpace Page 对应的内存)，然后**再将用户缓冲区里的数据拷贝到内核缓冲区** (Pagecache Page 对应的内存)；如果是读的 (read(2)) 话则是先从内核缓冲区拷贝到用户缓冲区，再从用户缓冲区读数据，也就是 buffer 和文件内容不存在任何映射关系。
* 对于存储映射 I/O 而言，则是直接**将 Pagecache Page 给映射到用户地址空间**，用户直接读写 Pagecache Page 中内容。
* **存储映射 I/O 要比标准 I/O 效率高一些，毕竟少了“用户空间到内核空间互相拷贝”的过程**。
* ![img](https://static001.geekbang.org/resource/image/c7/5e/c728b8a189fb4e35e536cf131c4d9b5e.jpg?wh=3400*3000)
* 可以将 Alloc Page 理解为 Page Cache 的“诞生”，将 Dirty Page 理解为 Page Cache 的婴幼儿时期（最容易生病的时期），将 Clean Page 理解为 Page Cache 的成年时期（在这个时期就很少会生病了）。



###### Page Cache 的死亡

* 可以把 Page Cache 的回收行为 (Page Reclaim) 理解为 Page Cache 的“自然死亡”。
* free 命令中的 buff/cache 中的这些就是“活着”的 Page Cache，那它们什么时候会“死亡”（被回收）呢？
* ![img](https://static001.geekbang.org/resource/image/1d/bb/1d430c93e397e23d67d12e28827c4bbb.jpg?wh=3658*2138)
* 应用在申请内存的时候，即使没有 free 内存，**只要还有足够可回收的 Page Cache，就可以通过回收 Page Cache 的方式来申请到内存**，回收的方式主要是两种：**直接回收和后台回收**。
* 总的来说，**Page Cache 的生命周期对于应用程序而言是相对比较透明的**，即它的分配与回收都是由操作系统来进行管理的。
* 

##### 如何处理 PAGE Cache 难以回收产生的 load 标高问题

###### 直接内存回收

* **系统很卡顿，敲命令响应非常慢**；应用程序的 RT 变得很高，或者抖动得很厉害。在发生这些问题时，很有可能也伴随着系统 load 飙得很高。

* 可能的原因：

* > * 直接内存回收引起的 load 飙高；
  > * 系统中脏页积压过多引起的 load 飙高；
  > * 系统 NUMA 策略配置不当引起的 load 飙高。

* 直接内存回收是指在**进程上下文同步进行内存回收**，那么它具体是怎么引起 load 飙高的呢？

* 因为直接内存回收是在**进程申请内存的过程中同步进行的回收，而这个回收过程可能会消耗很多时间**，进而导致进程的后续行为都被迫等待，这样就会造成很长时间的延迟，以及系统的 CPU 利用率会升高，最终引起 load 飙高。

* ![img](https://static001.geekbang.org/resource/image/fe/00/fe84eb2bd4956bbbdd5b0259df8c9400.jpg?wh=3000*2912)

* 在开始内存回收后，首先进行后台异步回收（上图中蓝色标记的地方），这不会引起进程的延迟；如果后台异步回收跟不上进程内存申请的速度，就会开始同步阻塞回收，导致延迟（上图中红色和粉色标记的地方，这就是引起 load 高的地址）。

* 针对直接内存回收引起 load 飙高或者业务 RT 抖动的问题，**一个解决方案就是及早地触发后台回收来避免应用程序进行直接内存回收**.

* ![img](https://static001.geekbang.org/resource/image/44/72/44d471fdae7376eb13e6e6bfc70b3172.jpg?wh=2440*1300)

* 当内存水位低于 watermark low 时，就会**唤醒 kswapd 进行后台回收，然后 kswapd 会一直回收到 watermark high**。

* 我们可以**增大 min_free_kbytes 这个配置选项来及早地触发后台回收**，该选项最终控制的是内存回收水位

* 你可以渐进式地增大该值，比如先调整为 1G，观察 sar -B 中 pgscand 是否还有不为 0 的情况；如果存在不为 0 的情况，继续增加到 2G，再次观察是否还有不为 0 的情况来决定是否增大，以此类推。

* **设置vm.min_free_kbytes，尽早开始后台回收。**

* 



###### 系统中脏页积压过多

* 直接回收过程中，如果**存在较多脏页就可能涉及在回收过程中进行回写，这可能会造成非常大的延迟**.
* 而且因为这个过程本身是阻塞式的，所以又可能进一步导致**系统中处于 D 状态的进程数增多**，最终的表现就是系统的 load 值很高。
* ![img](https://static001.geekbang.org/resource/image/90/75/90c693c95d67cfaf89b86edbd1228d75.jpg?wh=3200*2500)
* 那如何解决这类问题呢？一个比较省事的解决方案是**控制好系统中积压的脏页数据。**
* 脏页控制的少了可能会影响系统整体的效率，脏页控制的多了还是会触发问题，所以我们接下来看下如何来衡量好这个“度”。
* 调整这些配置项有利有弊，**调大这些值会导致脏页的积压，但是同时也可能减少了 I/O 的次数**，从而提升单次刷盘的效率；调小这些值可以减少脏页的积压，但是同时**也增加了 I/O 的次数，降低了 I/O 的效率**。
* 解决方案：根据业务调整。





###### 系统 NUMA 策略配置不当

* 因为相比内存回收的危害而言，NUMA 带来的性能提升几乎可以忽略，所以配置为 0，利远大于弊。
* 





###### 总结

* **直接内存回收引起 load 飙高时，就去调整内存水位设置；脏页积压引起 load 飙高时，就需要去调整脏页的水位**
* NUMA 策略配置不当引起 load 飙高时，就去检查是否需要关闭该策略。
* 同时我们在做这些调整的时候，**一定要边调整边观察业务的服务质量**，确保 SLA 是可以接受的。



* Q： 我们都说剩余内存达到low水位时，就会进行内存回收。但每个zone都有水位，剩余内存是指某个zone里的剩余内存，还是node，还是整个系统的剩余内存？还有内存回收只针对当前zone，还是node，还是整个系统？
* 这是三个不同的维度：系统 -> node -> zone。我们调整的水位最终是提现在zone上，回收也是针对zone的。**进程在申请内存时其实是会指定zone的**，只是应用程序并不感知这个细节，对应用程序而言，它申请的内存一般都是normal zone，那么当normal zone中的剩余内存不足时就会触发回收；这里面又会涉及到node概念，假设有2个node，node 0的normal zone里剩余内存不足了就会唤醒kswapd0来回收node 0里normal zone的内存，但是此时**应用是可以去继续申请node 1的normal zone的内存的**（如果没有配置numa策略），如果node 1的normal zone剩余内存也不足了，那会唤醒kswapd1来回收node 1的normal zone。



##### Page Cache 容易回收引起的问题

* 误操作而导致 Page Cache 被回收掉，进而导致业务**性能**下降明显；
* 内核的一些机制导致业务 Page Cache 被回收，从而引起**性能**下降。
* 比如说这些 Page Cache 被清理掉后可能会引起系统性能下降。为什么？
* 其实这和 inode 有关，那 inode 是什么意思呢？**inode 是内存中对磁盘文件的索引**，进程在查找或者读取文件时就是通过 inode 来进行操作的，
* ![img](https://static001.geekbang.org/resource/image/7e/99/7ef7747f71ef236e8cf5f9378d80da99.jpg?wh=3500*2564)
* **进程会通过 inode 来找到文件的地址空间（address_space）**，然后结合文件偏移（会转换成 page index）来找具体的 Page
* 如果该 **Page 存在**，那就说明文件内容已经被读取到了内存；如果该 **Page 不存在**那就说明不在内存中，需要到磁盘中去读取。

* 如果 inode 不存在了，那么 PageCache Page 也就不存在了。







###### 如何解决

* 从应用代码层面来优化；

* > * 从**应用程序代码层面来解决是相对比较彻底的方案**，因为应用更清楚哪些 Page Cache 是重要的，哪些是不重要的，所以就可以明确地来对读写文件过程中产生的 Page Cache 区别对待。
  > * 比如说，对于重要的数据，可以通过 **mlock(2) 来保护它**，防止被回收以及被 drop；
  > * 对于不重要的数据（比如日志），那可以通过 **madvise(2)** 告诉内核来立即释放这些 Page Cache。

* 从系统层面来调整。

* 



#### 内存泄露

##### 进程哪些内容容易引发内存泄露

可能出现的场景： 

* **伴随着服务器中的后台任务持续地运行，系统中可用内存越来越少**；
* 应用程序正在运行时忽然被 **OOM kill 掉了**；
* 进程看起来没有消耗多少内存，但是系统内存就是不够用了；
* **内存泄漏指的是内存被分配出去后一直没有被释放，导致这部分内存无法被再次使用**，甚至更加严重的是，指向这块内存空间的指针都不存在了，进而再也无法访问这块内存空间。
* 内存泄漏可能是**应用程序的内存泄漏，也可能是内核（操作系统）的内存泄漏**
* 应用程序的内存泄漏可能是**堆内存（heap）的泄漏**，也可能是**内存映射区（Memory Mapping Region）的泄漏**
* ![img](https://static001.geekbang.org/resource/image/c3/32/c321c56a7b719bf14b0b5133d0a66132.jpg?wh=2948*1964)
* 应用程序首先会调用内存申请释放相关的函数，比如 glibc 提供的 malloc(3)、 free(3)、calloc(3) 等；或者是直接使用系统调用 mmap(2)、munmap(2)、 brk(2)、sbrk(2) 等
* 所以你可以把它理解为是进程申请内存的类型大汇总：
* ![img](https://static001.geekbang.org/resource/image/85/0f/85e7da0e15587c6a1d31f7e60e1ab00f.jpg?wh=3002*2208)
* 进程调用 **malloc() 或者 mmap() 来申请的内存都是虚拟内存**，只有往这些内存中写入数据后（比如通过 **memset**），才会真正地分配物理内存 。
* Q： 如果进程只是调用 malloc() 或者 mmap() 而不去写这些地址，即不去给它分配物理内存，是不是就不用担心内存泄漏了？
* A: 不是， 因为这可能导致进程虚拟地址空间耗尽，即虚拟地址空间同样存在内存泄露的问题。
* 分页：
* ![img](https://static001.geekbang.org/resource/image/5e/68/5e2dacf3890cd9d508d3e0181a8ac868.jpg?wh=3000*1526)
* Paging 的大致过程是，CPU 将要请求的虚拟地址传给 MMU（Memory Management Unit，内存管理单元），然后 **MMU 先在高速缓存 TLB（Translation Lookaside Buffer，页表缓存）中查找转换关系，如果找到了相应的物理地址则直接访问**；如果找不到则在地址转换表（Page Table）里查找计算。最终进程访问的虚拟地址就对应到了实际的物理地址。
* 通过指令的方式判断是否存在内存泄露： **如果地址空间中[heap]太大，那有可能是堆内存产生了泄漏**；再比如说，如果进程地址空间包含太多的 vma（可以把 maps 中的每一行理解为一个 vma），那很可能是应用程序调用了很多 mmap 而没有 munmap
* 查看进程内存时，可以**先使用 top 来看系统中各个进程的内存使用概况**，**再使用 pmap 去观察某个进程的内存细节**。
* Q： 平常看到的RSS / RES （进程实际占用的物理内存）会包含shared memory吗？
* A： 会包含的。RSS / RES是指**进程地址空间已映射的物理内存**，这个**物理内存有可能是Shared方式来映射的**，那这块物理内存就是Shared memory。也就是说RSS的本质是物理内存是否映射到了进程的地址空间，而和映射属性无关。







##### 如何预防内存泄露

```cpp

#include <stdlib.h>
#include <string.h>

#define SIZE (1024 * 1024 * 1024) /* 1G */
int main()
{
    char *p = malloc(SIZE);
    if (!p)
      return -1;
    
    memset(p, 1, SIZE);
    /* 然后就再也不使用这块内存空间 */
    /* 没有释放p所指向的内存进程就退出了 */
    /* free(p); */
    return 0;
}
```

* 使用一个简单的内存泄漏检查工具 (valgrind) 来看看。

* ```cpp
  
  $ valgrind --leak-check=full  ./a.out 
  ==20146== HEAP SUMMARY:
  ==20146==     in use at exit: 1,073,741,824 bytes in 1 blocks
  ==20146==   total heap usage: 1 allocs, 0 frees, 1,073,741,824 bytes allocated
  ==20146== 
  ==20146== 1,073,741,824 bytes in 1 blocks are possibly lost in loss record 1 of 1
  ==20146==    at 0x4C29F73: malloc (vg_replace_malloc.c:309)
  ==20146==    by 0x400543: main (in /home/yafang/test/mmleak/a.out)
  ==20146== 
  ==20146== LEAK SUMMARY:
  ==20146==    definitely lost: 0 bytes in 0 blocks
  ==20146==    indirectly lost: 0 bytes in 0 blocks
  ==20146==      possibly lost: 1,073,741,824 bytes in 1 blocks
  ==20146==    still reachable: 0 bytes in 0 blocks
  ==20146==         suppressed: 0 bytes in 0 blocks
  ```

* 从 valgrind 的检查结果里我们可以清楚地看到，申请的内存只被使用了一次（memset）就再没被使用，但是在使用完后却没有把这段内存空间给释放掉，这就是典型的内存泄漏。

* Q : 那这个内存泄漏是有危害的吗？

* 进程地址空间的分配和销毁来说起，下面是一个简单的示意图：

* ![img](https://static001.geekbang.org/resource/image/e0/64/e0e227529ba7f2fcab1ab445c4634764.jpg?wh=3300*1950)

* 进程在退出的时候，会把它建立的映射都给解除掉。换句话说，进程退出时，会把它申请的内存都给释放掉，**这个内存泄漏就是没危害的。**

* 总之，如果进程不是长时间运行，那么即使存在内存泄漏（比如这个例子中的只有 malloc 没有 free），它的危害也不大，因为进程退出时，内核会把进程申请的内存都给释放掉。

* 下面这段程序就是会对应用程序造成危害的内存泄露：

* ```cpp
  
  #include <stdlib.h>
  #include <string.h>
  #include <unistd.h>
  
  #define SIZE (1024 * 1024 * 1024) /* 1G */
  
  void process_memory()
  {
          char *p; 
          p = malloc(SIZE);
          if (!p)
                  return;
          memset(p, 1, SIZE);
          /* Forget to free this memory */
  }
  
  /* 处理其他事务，为了简便起见，我们就以sleep为例 */
  void process_others()
  {
          sleep(1);
  }
  
  
  int main()
  {
          /* 这部分内存只处理一次，以后再也不会用到 */
          process_memory();
  
  
          /* 进程会长时间运行 */
          while (1) {
                  process_others();
          }   
          return 0;
  }
  ```

* 这是一个长时间运行的程序，process_memory() 中我们申请了 1G 的内存去使用，然后就再也不用它了，**由于这部分内存不会再被利用，这就造成了内存的浪费，如果这样的程序多了，被泄漏出去的内存就会越来越多，然后系统中的可用内存就会越来越少**。

* 

###### 如何预防

* 系统内存不足时会唤醒 OOM killer 来选择一个进程给杀掉，在我们这个例子中它杀掉了这个正在内存泄漏的程序，该进程被杀掉后，整个系统也就变得安全了。

* ```cpp
  
  $ dmesg
  [944835.029319] a.out invoked oom-killer: gfp_mask=0x100dca(GFP_HIGHUSER_MOVABLE|__GFP_ZERO), order=0, oom_score_adj=0
  [...]
  [944835.052448] Out of memory: Killed process 1426 (a.out) total-vm:8392864kB, anon-rss:7551936kB, file-rss:4kB, shmem-rss:0kB, UID:0 pgtables:14832kB oom_score_adj:0
      
  ```

* **OOM killer 选择进程是有策略的，它未必一定会杀掉正在内存泄漏的进程，很有可能是一个无辜的进程被杀掉**。而且，OOM 本身也会带来一些副作用。

* OOM 日志可以理解为是一个单生产者多消费者的模型，如下图所示：
* ![img](https://static001.geekbang.org/resource/image/b3/a7/b39503a3fb39e731d2d4c51687db70a7.jpg?wh=2758*1591)

* 即使有泄漏了也不要让它给系统带来很大的危害。长时间的内存泄漏问题最后基本都会以 OOM 结束.
* 如果你的服务器有慢速的串口设备，那你一定要防止它接收太多的日志，尤其是 **OOM 产生的日志，因为 OOM 的日志量是很大的，打印完整个 OOM 信息 kennel 会很耗时，进而导致阻塞申请内存的进程**，甚至会严重到让整个系统假死。
* 我们的运维人员发现某几台机器 used（已使用的）内存越来越多，但是通过 top 以及其他一些命令，却检查不出来到底是谁在占用内存。随着可用内存变得越来越少，**业务进程也被 OOM killer 给杀掉，这给业务带来了比较严重的影响。**



##### shmem:进程没有消耗内存，内存去哪了

* 但是在我们生产环境上遇到的问题，各个进程的 RES 都不大，看起来和 /proc/meminfo 中的 Shmem 完全对应不起来，这又是为什么呢？
* 这跟一种特殊的 Shmem 有关。我们知道，磁盘的速度是远远低于内存的，有些应用程序为了提升性能，会**避免将一些无需持续化存储的数据写入到磁盘，而是把这部分临时数据写入到内存中**，然后定期或者在不需要这部分数据时，清理掉这部分内容来释放出内存。
* 在这种需求下，就产生了一种特殊的 Shmem：tmpfs。tmpfs 如下图所示：（mpfs 作为一种特殊的 Shmem （是共享内存的一部分， 不同进程之间都可以访问这段内存））
* ![img](https://static001.geekbang.org/resource/image/24/eb/248b083e0c263b096c66f8078e3a3aeb.jpg?wh=2800*1065)
* 它是一种内存文件系统，**只存在于内存中**，它无需应用程序去申请和释放内存，而是操作系统自动来规划好一部分空间，应用程序只需要往这里面写入数据就可以了，这样会很方便。
* 就像进程往磁盘写文件一样，进程写完文件之后就把文件给关闭掉了，这些文件和进程也就不再有关联，所以这些磁盘文件的大小不会体现在进程中。
* 同样地，**tmpfs 中的文件也一样，它也不会体现在进程的内存占用上**。
* tmpfs 是属于文件系统的一种。对于文件系统，我们都可以通过 df 来查看它的使用情况。
* 我们在生产环境上还遇到过这样一个问题：**systemd 不停地往 tmpfs 中写入日志但是没有去及时清理**，而 tmpfs 配置的初始值又太大，这就导致 systemd 产生的日志量越来越多，最终可用内存越来越少。
* 针对这个问题，解决方案就是限制 systemd 所使用的 tmpfs 的大小，在日志量达到 tmpfs 大小限制时，自动地清理掉临时日志，或者定期清理掉这部分日志，这都可以通过 systemd 的配置文件来做到
* **tmpfs是存储在内存中，但是它是使用文件方式来管理的**，所以也会有block的概念，它的block size跟page大小是一致的，也是4k，但是tmpfs不是块设备。





###### OOM 危害

* ![img](https://static001.geekbang.org/resource/image/15/ee/150863953f090f09179e87814322a5ee.jpg?wh=3766*2545)
* OOM killer 在杀进程的时候，会把系统中可以被杀掉的进程扫描一遍，**根据进程占用的内存以及配置的 oom_score_adj 来计算出进程最终的得分，然后把得分（oom_score）最大的进程给杀掉**，如果得分最大的进程有多个，那就把先扫描到的那个给杀掉。
* 如果你不想这个进程被首先杀掉，那你可以调整该进程的 oom_score_adj 改变这个 oom_score；如果你的进程无论如何都不能被杀掉，那你可以将 oom_score_adj 配置为 -1000。
* 我们都需要将一些很重要的系统服务的 oom_score_adj 配置为 -1000，比如 sshd，因为这些系统服务一旦被杀掉，我们就很难再登陆进系统了。
* OOM killer 的作用之一，就是找到系统中不停泄漏内存的进程然后把它给杀掉，**如果没有找对，那就会误杀其他进程，甚至是误杀了更为重要的业务进程。**
* 



##### 如何分析内核内存泄露

* **内核内存泄漏往往都会是很严重的问题，这通常意味着要重启服务器来解决了**

* 在发生了内存泄漏后，能够**判断出来是不是内核引起的问题**，以及能够找到引起问题的根因，或者是向更专业的内核开发者求助来找到问题根因，从而彻底解决掉它，以免再次重启服务器。

* 进程的虚拟地址空间（address space）既包括用户地址空间，也包括内核地址空间。这可以简单地理解为，**进程运行在用户态申请的内存，对应的是用户地址空间，进程运行在内核态申请的内存，对应的是内核地址空间**

* ![img](https://static001.geekbang.org/resource/image/41/d9/41909181c0f6aa0958c33df52cd626d9.jpg?wh=3700*2809)

* **应用程序**可以通过 malloc() 和 free() 在**用户态**申请和释放内存，与之对应，**可以通过 kmalloc()/kfree() 以及 vmalloc()/vfree() 在内核态申请和释放内存。**

* 从最右侧的物理内存中你可以看出这两类内存申请方式的主要区别，**kmalloc() 内存的物理地址是连续的，而 vmalloc() 内存的物理地址则是不连续的**。

* 其中 vmalloc 申请的内存会体现在 VmallocUsed 这一项中，即已使用的 Vmalloc 区大小；而 **kmalloc 申请的内存则是体现在 Slab** 这一项中，它又分为两部分，其中 SReclaimable 是指在内存紧张的时候可以被回收的内存，而 SUnreclaim 则是不可以被回收只能主动释放的内存。

* ```shell
  Slab:            1056712 kB
  SReclaimable:     578916 kB
  SUnreclaim:       477796 kB
  
  VmallocTotal:   34359738367 kB
  VmallocUsed:      177336 kB
  VmallocChunk:          0 kB
  
  ```

* 内核空间的内存泄漏与用户空间的内存泄漏有什么不同呢？

* 我们知道，用户空间内存的生命周期与用户进程是一致的，**进程退出后**这部分内存就会自动释放掉。

* 但是，**内核空间内存的生命周期是与内核一致的**，却不是跟内核模块一致的，也就是说，在内核模块退出时，不会自动释放掉该内核模块申请的内存，只有在**内核重启（即服务器重启）时才会释放掉这部分内存**。

* 生产环境上就遇到过很多起这类**第三方驱动引发的内存泄漏问题**，排查起来往往也比较费时。

* 当你发现**内核内存泄漏时，首先需要去质疑的就是你们系统中的第三方驱动程序**，以及你们自己开发的驱动程序。

###### 如何观察内核泄露

* 如果 /proc/meminfo 中**内核内存（比如 VmallocUsed 和 SUnreclaim）太大**，那很有可能发生了内核内存泄漏；
* 另外，你也可以**周期性地观察 VmallocUsed 和 SUnreclaim 的变化**，如果它们持续增长而不下降，也可能是发生了内核内存泄漏。
* 你可以通过 /proc/meminfo 里面的信息来看内核内存的使用情况，然后根据这里面的信息来做一些基本的判断：**如果内核太大那就值得怀疑；**
* kmemleak 是**内核内存分析的利器**，但是一般只在测试环境上使用它，因为它对性能会有比较明显的影响；
* 在生产环境中**可以使用 tracepoint 或者 kprobe**，来追踪特定类型内核内存的申请和释放，从而帮助我们判断是否存在内存泄漏。但这往往需要专业的知识，你在不明白的时候可以去请教一些内核专家；
* 内核内存泄漏通常都是第三方驱动或者自己写的一些内核模块导致的，在出现内核内存泄漏时，你可以优先去排查它们。
* 



##### 如何定位内存泄露

* ![img](https://static001.geekbang.org/resource/image/a4/30/a48d1c573d19e30ecee8dc6f6fdd3930.jpg?wh=4215*3139)

* 如果进程的内存有问题，那使用 top 就可以观察出来；如果进程的内存没有问题，那你可以从 /proc/meminfo 入手来一步步地去深入分析。

* **为了防止栈溢出问题，当线程栈溢出后，就会写这个保护页，进而出发segfault。**

* Q： 案例讲的是虚拟内存泄露，我想问一下:虚拟内存分配后在使用前不会真正的分配物理内存，那这个案例是不是仅仅是进程的虚拟内存存在问题？整个Linux系统的内存应该没有异常对不对？

* A： 对的 只是虚拟地址空间产生了问题 实际的物理内存并没有问题 整个系统的内存并没有异常

* 基本流程：

* > * top 找内存占用太大的进程
  >
  > * **pidstat 命令**（关于该命令，你可以man pidstat）来**追踪下该进程的内存行为**，看看能够发现什么现象。
  >
  > * ```cpp
  >               
  >   $ pidstat -r -p 31108  1
  >   
  >   
  >   04:47:00 PM     31108    353.00      0.00 299029776 4182152  12.73  app_server
  >   ...
  >   04:47:59 PM     31108    149.00      0.00 299029776 4181052  12.73  app_server
  >   04:48:00 PM     31108    191.00      0.00 299040020 4181188  12.73  app_server
  >   ...
  >   04:48:59 PM     31108    179.00      0.00 299040020 4181400  12.73  app_server
  >   04:49:00 PM     31108    183.00      0.00 299050264 4181524  12.73  app_server
  >   ...
  >   04:49:59 PM     31108    157.00      0.00 299050264 4181456  12.73  app_server
  >   04:50:00 PM     31108    207.00      0.00 299060508 4181560  12.73  app_server
  >   ...
  >   04:50:59 PM     31108    127.00      0.00 299060508 4180816  12.73  app_server
  >   04:51:00 PM     31108    172.00      0.00 299070752 4180956  12.73  app_server
  >      
  >   ```
  >
  > * 在每个整分钟的时候，VSZ 会增大 10244KB，这看起来是一个很有规律的现象。然后，我们再来看下增大的这个内存区域到底是什么，**你可以通过 /proc/PID/smaps 来看**
  >
  > * ```cpp
  >   
  >   $ cat /proc/31108/smaps 
  >   ...
  >   7faae0e49000-7faae1849000 rw-p 00000000 00:00 0 
  >   Size:              10240 kB
  >   Rss:                  80 kB
  >   Pss:                  80 kB
  >   Shared_Clean:          0 kB
  >   Shared_Dirty:          0 kB
  >   Private_Clean:         0 kB
  >   Private_Dirty:        80 kB
  >   Referenced:           60 kB
  >   Anonymous:            80 kB
  >   AnonHugePages:         0 kB
  >   Swap:                  0 kB
  >   KernelPageSize:        4 kB
  >   MMUPageSize:           4 kB
  >   7faae1849000-7faae184a000 ---p 00000000 00:00 0 
  >   Size:                  4 kB
  >   Rss:                   0 kB
  >   Pss:                   0 kB
  >   Shared_Clean:          0 kB
  >   Shared_Dirty:          0 kB
  >   Private_Clean:         0 kB
  >   Private_Dirty:         0 kB
  >   Referenced:            0 kB
  >   Anonymous:             0 kB
  >   AnonHugePages:         0 kB
  >   Swap:                  0 kB
  >   KernelPageSize:        4 kB
  >   MMUPageSize:           4 kB
  >   ```
  >
  > * 一个私有地址空间，这从 rw-p 这个属性中的 private 可以看出来；以及一个保护页 ，这从—p 这个属性可以看出来，即进程无法访问。
  >
  > * 对于有经验的开发者而言，从这个 **4K 的保护页就可以猜测出应该跟线程栈有关了**。
  >
  > * 然后我们**跟踪下进程申请这部分地址空间的目的是什么，通过 strace 命令来跟踪系统调用**就可以了。
  >
  > * **VIRT 的增加，它的系统调用函数无非是 mmap 或者 brk**，那么我们只需要 strace 的结果来看下 mmap 或 brk 就可以了。
  >
  > * ```cpp
  >   
  >   $ strace -t -f -p 31108 -o 31108.strace
  >   ```
  >
  > * 如果使用 **-f 来跟踪线程，跟踪的信息量也很大**，逐个搜索日志里面的 mmap 或者 brk 真是眼花缭乱， 所以我们来 grep 一下这个大小 (10489856 即 10244KB)，然后过滤下就好了：
  >
  > * ```cpp
  >   
  >   $ cat 31108.strace | grep 10489856    
  >   31152 23:00:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31151 23:01:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31157 23:02:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31158 23:03:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31165 23:04:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31163 23:05:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31153 23:06:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31155 23:07:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31149 23:08:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31147 23:09:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31159 23:10:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31157 23:11:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31148 23:12:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31150 23:13:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   31173 23:14:00 mmap(NULL, 10489856, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0 <unfinished ...>
  >   ```
  >
  > * 从这个日志我们可以看到，**出错的是 mmap() 这个系统调用**，那我们再来看下 mmap 这个内存的目的：
  >
  > * 找到了具体原因，然后解决就可以了。





#### TCP 重传

##### TCP 连接和断开受哪些系统配置的影响

可能出现的问题：

* Client 为什么无法和 Server 建立连接呢？
* 三次握手都完成了，为什么会收到 Server 的 reset 呢？
* 建立 TCP 连接怎么会消耗这么多时间？
* 系统中为什么会有这么多处于 time-wait 的连接？
* 该这么处理？系统中为什么会有这么多 close-wait 的连接？
* 针对我的业务场景，这么多的网络配置项，应该要怎么配置呢？



###### TCP  连接受哪些配置影响

![img](https://static001.geekbang.org/resource/image/af/44/afc841ee3822fyye3ec186b28ee93744.jpg?wh=3478*2074)

* **Client 调用 connect() 后，Linux 内核就开始进行三次握手**。

* 在生产环境上就遇到过这种情况，**Server 因为某些原因被下线，但是 Client 没有被通知到**，所以 Client 的 connect() 被阻塞 127s 才去**尝试连接一个新的 Server**， 这么长的超时等待时间对于应用程序而言是很难接受的。

* 所以通常情况下，我们都会将数据中心内部服务器的 tcp_syn_retries 给调小，这里推荐设置为 2，来**减少阻塞的时间**。

* 有些情况下 1s 的阻塞时间可能都很久，所以有的时候也会将**三次握手的初始超时时间从默认值 1s 调整为一个较小的值，比如 100ms**，这样**整体的阻塞时间就会小很多**。这也是数据中心内部经常进行一些网络优化的原因。

* syn queue 的长度就是 **tcp_max_syn_backlog** 这个配置项来决定的，当系统中积压的半连接个数超过了该值后，新的 SYN 包就会被丢弃。对于服务器而言，可能瞬间会有非常多的新建连接，所以我们可以适当地调大该值，以免 SYN 包被丢弃而导致 Client 收不到 SYNACK：

* ```cpp
  net.ipv4.tcp_max_syn_backlog = 16384
  ```

* 就像半连接队列（syn queue）的长度有限制一样，全连接队列（accept queue）的长度也有限制，目的就是**为了防止 Server 不能及时调用 accept() 而浪费太多的系统资源**。

* **全连接队列（accept queue）的长度是由 listen(sockfd, backlog) 这个函数里的 backlog 控制的**，而该 backlog 的最大值则是 somaxconn。

* ```cpp
  net.core.somaxconn = 16384
  ```

* Server 在将新连接丢弃时，有的时候需要发送 reset(RST 报文) 来通知 Client，这样 Client 就不会再次重试了。

* tcp_abort_on_overflow 这个配置项来控制的，该值默认为 0，即不发送 reset 给 Client。推荐也是将该值配置为 0:

* ```cpp
  net.ipv4.tcp_abort_on_overflow = 0
  ```

* 这是因为，Server 如果来不及 accept() 而导致全连接队列满，这往往是由瞬间有大量新建连接请求导致的，正常情况下 Server 很快就能恢复，然后 Client 再次重试后就可以建连成功了。也就是说，将 tcp_abort_on_overflow 配置为 0，给了 Client 一个重试的机会。

* 



###### TCP 断开受哪些因素影响



![img](https://static001.geekbang.org/resource/image/1c/cf/1cf68d3eb4f07113ba13d84124f447cf.jpg?wh=3000*1929)

* 当应用程序调用 close() 时，会向对端发送 FIN 包，然后会接收 ACK；对端也会调用 close() 来发送 FIN，然后本端也会向对端回 ACK，这就是 TCP 的四次挥手过程

* 我们首先来看 **FIN_WAIT_2 状态**，TCP 进入到这个状态后，如果本端迟迟收不到对端的 FIN 包，那就会一直处于这个状态，于是就会一直消耗系统资源。Linux 为了防止这种资源的开销，设置了这个状态的超时时间 tcp_fin_timeout，默认为 60s，**超过这个时间后就会自动销毁该连接。**

* 本端为何迟迟收不到对端的 FIN 包，通常情况下都是因为对端机器出了问题，或者是因为太繁忙而不能及时 close()。

* 通常我们都建议将 tcp_fin_timeout 调小一些，以尽量避免这种状态下的资源开销。对于数据中心内部的机器而言，将它调整为 2s 足以：

* ```cpp
  net.ipv4.tcp_fin_timeout = 2
  ```

* TIME_WAIT 状态存在这么长时间，也是对系统资源的一个浪费，所以系统也有配置项来限制该状态的最大个数，该配置选项就是 tcp_max_tw_buckets。对于数据中心而言，**网络是相对很稳定的，基本不会存在 FIN 包的异常，所以建议将该值调小一些**：

* ```cpp
  net.ipv4.tcp_max_tw_buckets = 10000
  ```

* Client 关闭跟 Server 的连接后，也有可能很快再次跟 Server 之间建立一个新的连接，而由于 TCP 端口最多只有 65536 个，如果不去复用处于 TIME_WAIT 状态的连接，就可能在快速重启应用程序时，出现端口被占用而无法创建新连接的情况。所以**建议你打开复用 TIME_WAIT 的选项**：

* ```cpp
  net.ipv4.tcp_tw_reuse = 1
  ```

* ![img](https://static001.geekbang.org/resource/image/3d/de/3d60be2523528f511dec0fbc88ce1ede.jpg?wh=3854*2416)

* Q : tcp成功连接后很占资源，有没有具体的解说？

* A :  TCP连接的开销主要是内存开销，也就是**TCP相关结构体的开销**，每个TCP结构体的大小可以通过/proc/slabinfo来查看，例如： cat /proc/slabinfo | grep -i TCP

* 这里面会有全连接和半连接TCP结构体的大小，**全连接较大，一个有几K字节**。

* 除了内存开销外，另外就是**软中断的开销，tcp连接会用到很多timer**，特别是time-wait连接，这些timer会消耗CPU。

* 





##### TCP  收发包过程受哪些影响

* TCP 收包和发包的过程也是容易引起问题的地方。收包是指**数据到达网卡再到被应用程序开始处理的过程**。**发包则是应用程序调用发包函数到数据包从网卡发出的过程**。

* 可能存在的问题：

* > * 网卡中断太多，占用太多 CPU，导致业务频繁被打断；
  > * 应用程序调用 write() 或者 send() 发包，怎么会发不出去呢；
  > * 数据包明明已经被网卡收到了，可是应用程序为什么没收到呢；
  > * 我想要调整缓冲区的大小，可是为什么不生效呢；
  > * 是不是内核缓冲区满了从而引起丢包，我该怎么观察呢；

###### TCP 数据包发送过程参数调整

* ![img](https://static001.geekbang.org/resource/image/5c/5e/5ce5d202b7a179829f4c9b3863b0b15e.jpg?wh=4500*3274)

* 应用程序调用 write(2) 或者 send(2) 系列系统调用开始往外发包时，这些系统调用会把数据包**从用户缓冲区拷贝到 TCP 发送缓冲区（TCP Send Buffer）**(还在机器中），这个 TCP 发送缓冲区的大小是受限制的，这里也是容易引起问题的地方。

* TCP 发送缓冲区的大小默认是受 net.ipv4.tcp_wmem 来控制：

* ```cpp
  net.ipv4.tcp_wmem = 8192 65536 16777216
  ```

* 我上面列出的 tcp_wmem 的 min、default、max 这几组数值就是调大后的值，也是我们在生产环境中配置的值。

* 之所以将这几个值给调大，是因为我们在**生产环境中遇到过 TCP 发送缓冲区太小，导致业务延迟很大的问题**.

* 应用程序有的时候会很**明确地知道自己发送多大的数据，需要多大的 TCP 发送缓冲区**，这个时候就**可以通过 setsockopt(2) 里的 SO_SNDBUF 来设置固定的缓冲区大小。**一旦进行了这种设置后，tcp_wmem 就会失效，而且这个缓冲区大小设置的是固定值，内核也不会对它进行动态调整。

* **tcp_wmem 以及 wmem_max 的大小设置都是针对单个 TCP 连接的**，这两个值的单位都是 Byte（字节）。**系统中可能会存在非常多的 TCP 连接，如果 TCP 连接太多，就可能导致内存耗尽。**因此，所有 TCP 连接消耗的总内存也有限制：

* ```cpp
  net.ipv4.tcp_mem = 8388608 12582912 16777216
  ```

* **经过 IP 层后，数据包再往下就会进入到网卡了**，然后通过网卡发送出去。至此，**你需要发送出去的数据就走完了 TCP/IP 协议栈**，然后正常地发送给对端了。



###### TCP 数据包接受参数调整

* ![img](https://static001.geekbang.org/resource/image/9c/56/9ca34a53abf57125334e0278edd10356.jpg?wh=4500*3193)
* TCP 数据包的接收流程在整体上与发送流程类似，只是方向是相反的。数据包到达网卡后，**就会触发中断（IRQ）来告诉 CPU 读取这个数据包。**
* 但是在高性能网络场景下，**数据包的数量会非常大，如果每来一个数据包都要产生一个中断，那 CPU 的处理效率就会大打折扣**，所以就产生了 NAPI（New API）这种机制让 CPU 一次性地去轮询（poll）多个数据包，以批量处理的方式来提升效率，降低网卡中断带来的性能开销。
* 这个 poll 的个数可以通过 sysctl 选项来控制： ` net.core.netdev_budget = 600`
* 比如增大到 600。增大该值可以一次性地处理更多的数据包。但是这种调整也是有缺陷的，因为这会导致 **CPU 在这里 poll 的时间增加**，**如果系统中运行的任务很多的话，其他任务的调度延迟就会增加**。
* TCP 接收缓冲区大小也是在 min 和 max 之间动态调整 ，不过跟发送缓冲区不同的是，这个**动态调整是可以通过控制选项来关闭的**，这个选项是 tcp_moderate_rcvbuf 。通常我们都是打开它，这也是它的默认值：`net.ipv4.tcp_moderate_rcvbuf = 1`
* 之所以接收缓冲区有选项可以控制自动调节，而发送缓冲区没有，那是因为 **TCP 接收缓冲区会直接影响 TCP 拥塞控制，进而影响到对端的发包**，所以使用该控制选项可以更加灵活地控制对端的发包行为。
* ![img](https://static001.geekbang.org/resource/image/8d/9b/8d4ba95a95684004f271677f600cda9b.jpg?wh=3946*2779)
* Q : 在 TCP 发送过程中使用到了 qdisc，但是在接收过程中没有使用它，请问是为什么？我们可以在接收过程中也使用 qdisc 吗？
* A :  qdisc的主要目的是流控，**流控一般都是在发送端进行控制**；对于接收端而言，**它已经收到这个包了，再进行流控的话，也就只有选择性的丢包**。如果在接收端也使用qdisc之类的流控机制，也需要将它模拟为发送端，也就是增加一个中间设备来做。
* 





##### TCP 拥塞控制的原因

* 可能导致的现象：

* > * 等电梯时和别人聊着微信，进入电梯后微信消息就发不出去了；
  > * 和室友共享同一个网络，当玩网络游戏玩得正开心时，游戏忽然卡得很厉害，原来是室友在下载电影；
  > * 使用 ftp 上传一个文件到服务器上，没想到要上传很久；



###### TCP 拥塞控制对业务网络性能产生的影响

* TCP 拥塞控制的大致原理。

* ![img](https://static001.geekbang.org/resource/image/5c/3c/5c4504d70ce3abc939yyca54780dd43c.jpg?wh=3055*2029)
* **初始发送数据包的数量是由 init_cwnd（初始拥塞窗口）来决定的**，该值在 Linux 内核中被设置为 10（TCP_INIT_CWND），这是由 Google 的研究人员总结出的一个经验值
* **增大 init_cwnd 可以显著地提升网络性能，因为这样在初始阶段就可以一次性发送很多 TCP Segments**
* 如果**初始拥塞窗口设置得过大的话，可能会引起很高的 TCP 重传率**。当然，你也可以通过 ip route 的方式来更加灵活地调整该值。
* **增大 init_cwnd 的值对于提升短连接的网络性能会很有效**，特别是数据量在慢启动阶段就能发送完的短连接，比如针对 http 这种服务，**http 的短连接请求数据量一般不大，通常在慢启动阶段就能传输完**





* **拥塞避免：**

* ![img](https://static001.geekbang.org/resource/image/0c/99/0c2ce093d74a1dc76f39b7cbdd386699.jpg?wh=2716*1817)

* 发送端**一次性发送了 4 个 TCP segments，但是第 2 个 segment 在传输过程中被丢弃掉了**，那么接收方就接收不到该 segment 了。

* 然而第 3 个 TCP segment 和第 4 个 TCP segment 能够被接收到，**此时 3 和 4 就属于乱序报文，它们会被加入到接收端的 ofo queue（乱序队列）里**。

* 丢包这类问题在移动网络环境中比较容易出现，特别是在一个**网络状况不好的环境中，比如在电梯里丢包率就会很高，而丢包率高就会导致网络响应特别慢**。

* **连续出现了 3 个响应的 ack 后，发送端会据此判断数据包出现了丢失**，于是就进入了下一个阶段：快速重传。

* 除了快速重传外，还有一种重传机制是超时重传。不过，这是非常糟糕的一种情况。**如果发送出去一个数据包，超过一段时间（RTO）都收不到它的 ack，那就认为是网络出现了拥塞。**这个时候就需要将 cwnd 恢复为初始值，再次从慢启动开始调整 cwnd 的大小。

* RTO 一般发生在网络链路有拥塞的情况下，如果**某一个连接数据量太大，就可能会导致其他连接的数据包排队，从而出现较大的延迟。**我们在开头提到的，下载电影影响到别人玩网络游戏的例子就是这个原因。

* 关于 RTO，它也是一个优化点。如果 RTO 过大的话，那么业务就可能要阻塞很久，所以在 3.1 版本的内核里引入了一种改进来将 RTO 的初始值从 3s 调整为 1s。

* 不过，RTO=1s 在某些场景下还是有些大了，特别是在**数据中心内部这种网络质量相对比较稳定的环境中**。

* 创建 TCP 连接，使用 SO_SNDTIMEO 来设置发送超时时间，以防止应用在发包的时候阻塞在发送端太久。

* ```cpp
  ret = setsockopt(sockfd, SOL_SOCKET, SO_SNDTIMEO, &timeout, len);
  ```

  当业务发现该 TCP 连接超时后，就会主动断开该连接，然后尝试去使用其他的连接。





###### 流量控制：接收方如何影响发送方发送数据

* **除了网络状况外，发送方还需要知道接收方的处理能力**。
* ![img](https://static001.geekbang.org/resource/image/e9/27/e920b93740d9677c5419dee332086827.jpg?wh=2877*1843)
* 接收方在收到数据包后，会给发送方回一个 ack，然后把自己的 rwnd 大小写入到 TCP 头部的 win 这个字段，这样发送方就能根据这个字段来知道接收方的 rwnd 了。
* 发送方在发送下一个 TCP segment 的时候，会先对比发送方的 cwnd 和接收方的 rwnd，得出这二者之间的较小值，然后控制发送的 TCP segment 个数不能超过这个较小值。
* 



##### TCP 端到端时延变大，可能的原因

* 可能出现的问题：

* > * 如果客户端收到的响应时间变大了，那么这是客户端自身的问题呢，还是因为服务端处理得慢呢，又或者是因为网络有抖动呢？
  > * 即使我们已经明确了是服务端或者客户端的问题，那么究竟是**应用程序自身引起的问题呢，还是内核导致的问题呢**？

###### C/S 架构网络抖动

* ![img](https://static001.geekbang.org/resource/image/bf/37/bf1cd8873fff3a7528f194658753e837.jpg?wh=2172x948)
* 中间网络可以理解为是一个黑盒，很难去获取这些网络的详细信息，更不用说到这些网络设备上去做 debug 了。所以，我在这里把它们都简化为了一个 Router（路由器），然后 **Client 和 Server 通过这个路由器来相互通信**
* 





#### 内核 CPU 飙高













### Linux c++

#### [实现写时复制 fork](#实现写时复制 fork)

#### 获取某个文件总数

```cpp
// 递归求解所有可能的文件夹中的文件
/*
    // 打开一个目录
    #include <sys/types.h>
    #include <dirent.h>
    DIR *opendir(const char *name);
        参数：
            - name: 需要打开的目录的名称
        返回值：
            DIR * 类型，理解为目录流
            错误返回NULL


    // 读取目录中的数据
    #include <dirent.h>
    struct dirent *readdir(DIR *dirp);
        - 参数：dirp是opendir返回的结果
        - 返回值：
            struct dirent，代表读取到的文件的信息
            读取到了末尾或者失败了，返回NULL

    // 关闭目录
    #include <sys/types.h>
    #include <dirent.h>
    int closedir(DIR *dirp);

*/

int getTotalNum(const char* filePath) {
    // 1. 打开目录
    DIR* dir = opendir(filePath);
    if (dir == NULL) {
        perror("opendir");
        exit(0); // 结束当前的进行，直接终止这个进程，结束所有的递归
    }

    struct dirent* ptr;
    int total = 0;
    // 2. 获取普通文件的个数，使用递归求解
    while ((ptr = readdir(dir)) != NULL) {
        // 获取名称
        char* dname = ptr -> d_name;
        if (strcmp(dname, ".") == 0 || strcmp(dname, "..") == 0) {
            // 忽略掉，因为每一个文件夹中都存在这个数据，表示的是当前的文件夹，或者是上一个文件夹
            continue;
        }
        if (ptr -> d_type == DT_REG) {
            total++;
            continue;
        }
        if (ptr -> d_type == DT_DIR) {
            char newPath[1024];
            sprintf(newPath, "%s/%s", filePath, dname);
            total += getTotalNum(newPath);  // 递归求解所有可能的文件
        }
    }
    // 关闭文件
    closedir(dir);
    // 返回 文件总数
    return total;
}

int main(int argc, char* argv[]) {

    if (argc < 2) {
        printf("%s and the directory\n", argv[0]);
        return -1;
    }

    int total = getTotalNum(argv[1]);
    printf("当前总共拥有的文件个数是 : %d\n", total);
    return 0;
}
```

#### 信号-signal

* 信号的捕获过程
* ![image-20220507032757828](/Users/zhoubing/Library/Application Support/typora-user-images/image-20220507032757828.png)

```cpp
// 信号的阻塞表示的是：信号被阻塞，这个信号虽然有但是不会实际的执行，被阻塞了，例如：原本 crtl + C 终止一个进程，但是这个信号被阻塞了，那么这就不会终止一个进程，除非这个信号被释放出来。被阻塞的信号也是没有执行的信号。

// PCB 的信号集有两种，一种阻塞信号集，一种是未决信号集
// 阻塞信号集： 是一个开关动作，是阻止信号的执行，不是阻止信号的产生，这个时候信号已近产生了，
// 未决信号集： 是一种状态，指的是从信号的产生到信号被处理的这段时间。
// 当未决信号有一个信号过来，但是发现当前信号是阻塞的信号，那么这个信号位的未决状态也变成 1


// 信号的捕捉行为指的是什么？
// 信号的捕获，指得是存在一个信号，执行某个任务，现在捕获这个信号，一直执行某个自定义的任务，防止信号直接结束了

// wait 和 waitpid 的区别是什么？
// waitpid 输入的参数更多，这样每一次可以处理的 进程的类型也更多，根据输入的参数不同，可以设置需要处理的 进程
// 但是 wait() 和 waitpid() 两个都是一次调用只能回收一个进程
// wait（） 调用会出现阻塞，父进程没法接着往下执行，如果子进程没有死亡的话，但是 waitpid() 即使子进程没有死亡，父进程也是可以接着执行下去，通过设置参数：WNOHANG(非阻塞)

// 信号的捕获过程是什么？？？

int signal() {

    sigset_t set;
    sigemptyset(&set);
    sigaddset(&set, SIGINT);
    sigaddset(&set, SIGQUIT);

    // 修改内核中的阻塞信号集
    // 注意这个不是未决信号集，注意需要和未决信号集进行比较，因为只有来了一个未决信号想执行，但是发现被阻塞了，所以未决信号就变成 1 了
    int ret = sigprocmask(SIG_BLOCK, &set, NULL);
    if (ret == -1) {
        perror("sigprocmask");
        return -1;
    }
    int num = 0;

    while (1) {
        num++;
        sigset_t set1;
        sigemptyset(&set1);
        // 获取内核中的未决信号集
        sigpending(&set1);
        printf("The num is : %d\n", num);
        for (int i = 1; i <= 31; i++) {
            if (sigismember(&set1, i) == 1) {
                printf("1");
            } else if (sigismember(&set1, i) == 0) {
                printf("0");
            } else {
                perror("sigpending");
                return -1;
            }
        }
        printf("\n");
        sleep(1);
        if (num == 10) {
            sigprocmask(SIG_UNBLOCK, &set, NULL); // 解除阻塞，这样原来的未决的信号就会执行，这里有一个 SIGINT 命令就会执行下去
        }
    }
    return 0;
}
```



#### 线程同步 & 互斥

* 共享资源和临界资源是同一个意思
* 读写锁 & 互斥锁： 互斥锁是遇到加锁一定是互斥的，读写锁，当多个线程都希望读取某一个资源，即使加了读锁，也不会阻塞，但是读写会阻塞，写写也会阻塞。读是共享的是并发执行的。
* 条件变量：条件变量不是一个锁，可以引起线程阻塞，某个条件满足以后阻塞线程，或者某个条件满足以后解除线程. 配合我们的互斥锁使用，解决同步的问题.
* 条件变量： **当这个函数调用条件变量阻塞的时候，会对互斥锁进行解锁， 让别的线程去执行任务，当不阻塞的，继续向下执行，会重新加锁。**
* 信号量： 信号量实现进程/线程间的同步，互斥锁实现对共享资源的互斥访问



#### detach() & join()

* detach()

```cpp
/*
    #include <pthread.h>
    int pthread_detach(pthread_t thread);
        - 功能：分离一个线程。被分离的线程在终止的时候，会自动释放资源返回给系统。
          1.不能多次分离，会产生不可预料的行为。
          2.不能去连接一个已经分离的线程，会报错。
        - 参数：需要分离的线程的ID
        - 返回值：
            成功：0
            失败：返回错误号
*/


void * callback(void * arg) {
    printf("chid thread id : %ld\n", pthread_self());
    return NULL;
}

int main() {

    // 创建一个子线程
    pthread_t tid;

    int ret = pthread_create(&tid, NULL, callback, NULL);
    if(ret != 0) {
        char * errstr = strerror(ret);
        printf("error1 : %s\n", errstr);
    }

    // 输出主线程和子线程的id
    printf("tid : %ld, main thread id : %ld\n", tid, pthread_self());

    // 设置子线程分离,子线程分离后，子线程结束时对应的资源就不需要主线程释放
    ret = pthread_detach(tid);
    if(ret != 0) {
        char * errstr = strerror(ret);
        printf("error2 : %s\n", errstr);
    }

    // 设置分离后，对分离的子线程进行连接 pthread_join()
    // ret = pthread_join(tid, NULL);
    // if(ret != 0) {
    //     char * errstr = strerror(ret);
    //     printf("error3 : %s\n", errstr);
    // }

    pthread_exit(NULL);

    return 0;
}
```



* join() 

```cpp
/*
    #include <pthread.h>
    int pthread_join(pthread_t thread, void **retval);
        - 功能：和一个已经终止的线程进行连接
                回收子线程的资源
                这个函数是阻塞函数，调用一次只能回收一个子线程，是个阻塞的函数，所以子线程没有结束，这个主线程就不会结束
                一般在主线程中使用
        - 参数：
            - thread：需要回收的子线程的ID
            - retval: 接收子线程退出时的返回值， 为什么需要返回值是二级指针呢？因为返回值是一个 一级的 void* 指针，我们需要对这个 一级指针修改
            就需要传入的是二级指针
        - 返回值：
            0 : 成功
            非0 : 失败，返回的错误号
*/


int value = 10;

void* callback (void* arg) {
    printf("The child tid is : %lld\n", pthread_self());
    sleep(3);
    // return NULL; // 这个就是执行 pthread_exit(NULL) 当所有的线程退出了，那么这个进程就会退出
    pthread_exit((void *)&value); // 返回的是 void* 一级指针，需要对其修改需要指定的是二级指针
}

int main() {
    pthread_t tid;
    int ret = pthread_create(&tid, NULL, callback, NULL);
    if (ret != NULL) {
        char* errstr = strerror(ret);
        printf("The error is %s\n", errstr);
        return -1;
    }
    // 主线程执行的任务
    for (int i = 0; i < 5; i++) {
        printf("%d\n", i);
    }
    printf("The main id is %lld, the child id is : %lld\n", pthread_self(), tid);
    int *retval;
    ret = pthread_join(tid, &retval);
    // pthread_join() 函数在完成子线程资源回收的同时要把子线程返回值赋给第二个参数,使用 int* retval 去接收这个子线程的返回值
    printf("The return value is %d\n", *retval);  // 注意这个是阻塞式函数，所以不需要后面的 pthread_exit(NULL)来结束函数了，因为阻塞在哪里了
    // 主线程退出，这样不会影响子线程的执行，这样就不需要 sleep(1)了
    // pthread_exit(NULL);
    return 0;
}
```





#### IO 端口复用

![image-20220511022853258](/Users/zhoubing/Library/Application Support/typora-user-images/image-20220511022853258.png)

* 当端口 9999 处于 **Time_wait 状态**（Linux 1 min) 的时候，表示的是 服务器 还处于拥有端口 9999。这个时候虽然端口不再被使用，但是别的进程想使用 9999 这个端口号的时候还是不行的，因为显示这个端口号是别别的进程占用的。

* 端口复用表示的是，复用这个处于 Time_Wait 的端口号，或者复用某些端口号

* 端口复用最常见的用途：

	> * 防止服务器重启的时候，之前绑定的端口还没有释放
	> * 程序突然退出而系统没有释放端口

* ```cpp
	int setsockopt(int sockfd, int level, int optname, const void* optval, socklen_t optlen);
	参数：
	  - sockfd : 需要操作的文件描述符
	  - level : 级别 - SOL_SOCKET (端口复用的级别)
	  - optname : 选项的名称
	    - SO_REUSEADDR
	    - SO_REUSEPORT
	  - optval : 端口复用的值（整型）
	    - 1 : 可以复用
	    - 0 : 不可以复用
	  - optlen : optval 参数的大小
	端口复用，设置的时机是在服务器绑定端口之前
	setsockopt();
	bind();
	  
	```









####  IO 多路复用



* int fd = Socket(), 这个 fd  是文件描述符，一开始在用户态，之后需要拷贝到内核态中，表示的是一块内核缓冲区，这个缓冲区可以进行读以及写操作，之后，可以通过向缓冲区进行读写操作，进行数据的发送.

* 这个io指的还是内存到内存的数据传输，从内核缓存区到用户内存是读，从用户内存到内核缓冲区是写。

* IO 多路复用可以使得**程序可以同时监听多个文件描述符**. Linux : select poll epoll 这三种

* 原理就是：原来我们需要使用  ``read(fd, buf, sizeof(buf)), recv(fd, buf,sizeof(buf), 0)`` 判断当前 文件描述符是否存在数据到达，当有大量的文件描述符的时候，我们需要轮训的遍历所有的 文件描述符判断所有的可能，但是 使用 IO 多路复用，可以避免这种情况，我们可以委托内核处理哪些文件描述符（epoll) 内核中发来数据了，我们可以根据内核收集的数据去读取，直接一次读取就可以了。

* Select:

	> * 内核进行 fd_set 的遍历：内核是全遍历 fd_set（指定大小的位） ,然后判断值为1的位，才会去检测这个文件描述符是否有数据。告诉内核需要检测这个文件描述符对应的数据缓冲区有没有变化，有变化保持1，没变化设置为0。
	>
	> * ```cpp
	> 	#include <sys/time.h>
	> 	#include <sys/types.h>
	> 	#include <unistd.h>
	> 	#include <sys/select.h>
	> 	int select(int nfds, fd_set *readfds, fd_set *writefds,
	> 	           fd_set *exceptfds, struct timeval *timeout);
	> 	    - 参数：
	> 	            - nfds : 委托内核检测的最大文件描述符的值 + 1
	> 	            - readfds : 要检测的文件描述符的读的集合，委托内核检测哪些文件描述符的读的属性
	> 	                      - 一般检测读操作
	> 	                      - 对应的是对方发送过来的数据，因为读是被动的接收数据，检测的就是读缓冲
	> 	区
	> 	                      - 是一个传入传出参数
	> 	            - writefds : 要检测的文件描述符的写的集合，委托内核检测哪些文件描述符的写的属性
	> 	                      - 委托内核检测写缓冲区是不是还可以写数据（不满的就可以写）
	> 	            - exceptfds : 检测发生异常的文件描述符的集合
	> 	            - timeout : 设置的超时时间
	> 	                struct timeval {
	> 	                    long    tv_sec;         /* seconds */
	> 	                    long    tv_usec;        /* microseconds */
	> 	                };
	> 	                - NULL : 永久阻塞，直到检测到了文件描述符有变化
	> 	                - tv_sec = 0 tv_usec = 0， 不阻塞
	> 	                - tv_sec > 0 tv_usec > 0， 阻塞对应的时间
	> 	      - 返回值
	> 	              -1 : 失败
	> 	              0(n) : 检测的集合中有n个文件描述符发生了变化
	> 	// 将参数文件描述符fd对应的标志位设置为0
	> 	void FD_CLR(iselect是非阻塞IO对吧，原本while(1) 循环里面没有判断就是直接accept，也就是阻塞的操作，降低了CPU利用率。现在while(1)循环里增加了委托人，进行了一次判断即select返回值，select是非阻塞的，等于就是while(1)循环并不会阻塞太久，只有存在IO需求才会有accept这个阻塞操作。但是这种方式也占用 了更多CPU资源，因为while循环转地更快了要占用更多的CPU和系统资源
	> 											
	> 	select 缺点：
	> 											
	> 	每次调用 select， 都需要把 fd 集合从用户态拷贝到内核态中，这个开销在 fd 很多的时候很大。
	> 											
	> 	同时每次调用 select 都需要在内核遍历传递进来的所有 fd, 这个开销在 fd 很多的时候也是非常的大。
	> 											
	> 	select 支持的文件描述符数量太小了，默认是 1024
	> 											
	> 	fds 集合不nt fd, fd_set *set);
	> 	// 判断fd对应的标志位是0还是1， 返回值 ： fd对应的标志位的值，0，返回0， 1，返回1
	> 	int  FD_ISSET(int fd, fd_set *set);
	> 	// 将参数文件描述符fd 对应的标志位，设置为1
	> 	void FD_SET(int fd, fd_set *set);
	> 	// fd_set 一共有 1024 bit ，全部初始化为 0
	> 	void FD_ZERO(fd_set *set)
	> 	
	>



>* select是非阻塞IO对吧，原本while(1) 循环里面没有判断就是直接accept，也就是阻塞的操作，降低了CPU利用率。现在while(1)循环里增加了委托人，进行了一次判断即select返回值，select是非阻塞的，等于就是while(1)循环并不会阻塞太久，只有存在IO需求才会有accept这个阻塞操作。但是这种方式也占用 了更多CPU资源，因为while循环转地更快了要占用更多的CPU和系统资源
>
>* select 缺点：
>
>* 1. 每次调用 select， 都需要把 fd 集合从用户态拷贝到内核态中，这个开销在 fd 很多的时候很大。
>	2. 同时每次调用 select 都需要在内核遍历传递进来的所有 fd, 这个开销在 fd 很多的时候也是非常的大。
>	3. select 支持的文件描述符数量太小了，默认是 1024
>	4. fds 集合不能重用，每次都需要重置



* poll

```cpp
#include <poll.h>
struct pollfd {
int fd; /* 委托内核检测的文件描述符 */
short events; /* 委托内核检测文件描述符的什么事件 */
short revents; /* 文件描述符实际发生的事件 */
};
struct pollfd myfd;
myfd.fd = 5;
myfd.events = POLLIN | POLLOUT;
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
    - 参数：
        - fds : 是一个struct pollfd 结构体数组，这是一个需要检测的文件描述符的集合
        - nfds : 这个是第一个参数数组中最后一个有效元素的下标 + 1
        - timeout : 阻塞时长
            0 : 不阻塞
            -1 : 阻塞，当检测到需要检测的文件描述符有变化，解除阻塞
            >0 : 阻塞的时长
    - 返回值：
        -1 : 失败
        >0（n） : 成功,n表示检测到集合中有n个文件描述符发生变化
#include <sys/epoll.h>
// 创建一个新的epoll实例。在内核中创建了一个数据，这个数据中有两个比较重要的数据，一个是需要检
测的文件描述符的信息（红黑树），还有一个是就绪列表，存放检测到数据发送改变的文件描述符信息（双向
链表）。
int epoll_create(int size);
    - 参数：
    	size : 目前没有意义了。随便写一个数，必须大于0
    - 返回值：
        -1 : 失败
        > 0 : 文件描述符，操作epoll实例的

```



* epoll

```cpp
#include <sys/epoll.h>
// 创建一个新的epoll实例。在内核中创建了一个数据，这个数据中有两个比较重要的数据，一个是需要检
测的文件描述符的信息（红黑树），还有一个是就绪列表，存放检测到数据发送改变的文件描述符信息（双向
链表）。
int epoll_create(int size);
    - 参数：
    	size : 目前没有意义了。随便写一个数，必须大于0
        - 返回值：
        	-1 : 失败
            > 0 : 文件描述符，操作epoll实例的
typedef union epoll_data {
    void *ptr;
    int fd;
    uint32_t u32;
    uint64_t u64;
} epoll_data_t;
struct epoll_event {
    uint32_t events; /* Epoll events */
    epoll_data_t data; /* User data variable */
};
常见的Epoll检测事件：
    - EPOLLIN
    - EPOLLOUT
    - EPOLLERR
// 对epoll实例进行管理：添加文件描述符信息，删除信息，修改信息
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
    - 参数：
        - epfd : epoll实例对应的文件描述符
        - op : 要进行什么操作
        EPOLL_CTL_ADD: 添加
        EPOLL_CTL_MOD: 修改
        EPOLL_CTL_DEL: 删除
        - fd : 要检测的文件描述符
        - event : 检测文件描述符什么事情
// 检测函数
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int
timeout);
    - 参数：
        - epfd : epoll实例对应的文件描述符
        - events : 传出参数，保存了发送了变化的文件描述符的信息
        - maxevents : 第二个参数结构体数组的大小
        - timeout : 阻塞时间
            - 0 : 不阻塞
            - -1 : 阻塞，直到检测到fd数据发生变化，解除阻塞
            - > 0 : 阻塞的时长（毫秒）
     - 返回值：
            - 成功，返回发送变化的文件描述符的个数 > 0
            - 失败 -1

```



* LT 模式：（水平触发） （默认的）

  假设委托内核检测读事件 -> 检测fd的读缓冲区
  	读缓冲区有数据 - > epoll检测到了会给用户通知
  		a.用户不读数据，数据一直在缓冲区，epoll 会一直通知
  		b.用户只读了一部分数据，epoll会通知
  		c.缓冲区的数据读完了，不通知

  > LT (level-triggered) 是缺省的工作方式，并且同时支持 block 和 no-block socket。 这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 IO 操作，如果你不做任何的操作，内核还是会继续的通知你

* ET 模式（边沿触发）

  假设委托内核检测读事件 -> 检测fd的读缓冲区
  	读缓冲区有数据 - > epoll检测到了会给用户通知
  		a.用户不读数据，数据一直在缓冲区中，epoll下次检测的时候就不通知了
  		b.用户只读了一部分数据，epoll不通知
  		c.缓冲区的数据读完了，不通知



> ET（edge - triggered）是高速工作方式，只支持 no-block socket。在这种模式下，当描述
> 符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，
> 并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述
> 符不再为就绪状态了。但是请注意，如果一直不对这个 fd 作 IO 操作（从而导致它再次变成
> 未就绪），内核不会发送更多的通知（only once）。
> ET 模式在很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。epoll
> 工作在 ET 模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写
> 操作把处理多个文件描述符的任务饿死。

![image-20220807170355322](/home/zhoubing/.config/Typora/typora-user-images/image-20220807170355322.png)



#### UDP 广播 组播

* 广播组播一定是使用 UDP 的，因为TCP 只能 一对一

* 广播 ： 同一个服务器广播到不同的主机，注意不是同一台机器！！！ (UDP)

  > a.只能在局域网中使用。
  > b.客户端需要绑定服务器广播使用的端口，才可以接收到广播消息。
  >
  > c. server只发送数据，要先开启广播设置，指定发送数据目的地client是局域网内的广播地址，并指定广播端口，但它本身不需要bind，因为它自己的端口号是啥(不bind就内核分配)不影响client的端口
  >
  > client的IP地址是局域网内的即可(所以可以使用INADDR_ANY)，但端口必须是server广播使用的端口(所以一定要绑定广播端口)
  >
  > d. server **发送**数据者，不需要端口号，但是client 是**接受者**，需要绑定端口号和IP。

* 组播：(UDP)

  > 单播地址标识单个 IP 接口，广播地址标识某个子网的所有 IP 接口，多播地址标识一组 IP 接口。
  > 单播和广播是寻址方案的两个极端（要么单个要么全部），多播则意在两者之间提供一种折中方
  > 案。多播数据报只应该由对它感兴趣的接口接收，也就是说由运行相应多播会话应用系统的主机上
  > 的接口接收。另外，广播一般局限于局域网内使用，而多播则既可以用于局域网，也可以跨广域网
  > 使用。
  > a.组播既可以用于局域网，也可以用于广域网
  > b.客户端需要加入多播组，才能接收到多播的数据
  > 组播地址
  > IP 多播通信必须依赖于 IP 多播地址，在 IPv4 中它的范围从 224.0.0.0 到 239.255.255.255 ，
  > 并被划分为局部链接多播地址、预留多播地址和管理权限多播地址三类:

* 本地套接字通信 (TCP)

  > 本地套接字的作用：本地的进程间通信
  > 	有关系的进程间的通信 (eg: 父子进程)
  > 	没有关系的进程间的通信
  > 本地套接字实现流程和网络套接字类似，一般采用TCP的通信流程。















### GO

#### 垃圾回收

* Go 这门有 GC 的语言中，集成了逃逸分析功能来帮助我们自动判断对象应该在堆上还是栈上，我们可以使用 go build -gcflags="-m" 来观察逃逸分析的结果。
* 若对象被**分配在栈上**，它的管理成本就比较低，我们通过**挪动栈顶寄存器就可以实现对象的分配和释放**。
* 对象被分配在堆上，我们就要**经历层层的内存申请过程**。

##### 内存管理的三个参与者

###### mutator

* mutator 指的是我们的应用(业务线程），也就是 **application**，我们将堆上的对象看作一个图，跳出应用来看的话，**应用的代码就是在不停地修改这张堆对象图里的指向关系**。下面的图可以帮我们理解 mutator 对堆上的对象的影响：
* ![img](https://static001.geekbang.org/resource/image/13/56/131e1a3e08e0c2fcac022c197c433556.gif?wh=1228x676)
* 





###### allocator

* allocator 就很好理解了，指的是**内存分配器**，应用需要内存的时候都要向 allocator 申请。
* allocator 要维护好内存分配的数据结构，在多线程场景下工作的内存分配器还需要考虑**高并发场景下锁的影响**，并针对性地进行设计以降低锁冲突。



###### garbage collector

* collector 是**垃圾回收器**。
* **死掉的堆对象、不用的堆内存都要由 collector 回收**，最终归还给操作系统。
* 当 GC 扫描流程开始执行时，collector 需要扫描内存中存活的堆对象，扫描完成后，未被扫描到的对象就是无法访问的堆上垃圾，需要将其占用内存回收掉。

<img src="https://static001.geekbang.org/resource/image/09/yy/093e3db4643e289d0803943124115dyy.png?wh=1920x1268" alt="img" style="zoom:40%;" />

* 应用需要在**堆上申请内存**时，会由编译器帮程序员自动调用 runtime.newobject，这时 allocator 会使用 mmap 这个系统调用从**操作系统中申请内存**，**若 allocator 发现之前申请的内存还有富余，会从本地预先分配的数据结构中划分出一块内存，**并把它以指针的形式返回给应用。
* 在内存分配的过程中，**allocator 要负责维护内存管理对应的数据结构**。
* collector 要扫描的就是 allocator 管理的这些数据结构，应用不再使用的部分便应该被回收，**通过 madvise 这个系统调用返还给操作系统**。



##### 内存分配

* 目标： 为对象在 heap 上分配内存

* 提前将内存**分块**：

* > * 调用系统调用 mmap() 向 OS 申请一块大的内存，例如 4MB 
  > * 再将内存划分成大块，例如 8 KB  称作 mspan
  > * 再将大块继续划分成特定大小的小块，用于对象的分配
  > * noscan mspan: 分配不包含指针的对象 - GC 不需要扫描
  > * scan mspan : 分配包含指针的对象 - GC需要扫描
  > * ![image-20220824220048733](/home/zhoubing/.config/Typora/typora-user-images/image-20220824220048733.png)

* 对象的分配：根据对象的大小，选择最合适的块返回

* **缓存操作**：

* > * TCMalloc: thread caching
  > * 每一个 p 包含一个 mcache 用于快速的分配，用于为绑定于 p 上的 g 分配对象
  > * mcache 管理一组 mspan
  > * 当 mcache 中的 mspan 分配完毕，向 mcentral 申请带有未分配块的 mspan.
  > * 当 mspan 中没有分配的对象， mspan 会被缓存在 mcentral 中，而不是立刻释放并归还给 OS。
  > * ![image-20220824220516111](/home/zhoubing/.config/Typora/typora-user-images/image-20220824220516111.png)

* ![image-20221029212104684](/home/zhoubing/.config/Typora/typora-user-images/image-20221029212104684.png)

* 应用程序使用 mmap 向 OS 申请内存，操作系统提供的接口比较简单，mmap 返回的结果是连续的内存区域。

* ![img](https://static001.geekbang.org/resource/image/92/76/926bc8690be714ed9140f25c0e03c276.png?wh=1556x782)

* 现代 CPU 上，除了内存分配的正确性以外，我们还要考虑分配过程的效率问题，应用执行期间小对象会不断地生成与销毁，如果**每一次对象的分配与释放都需要与操作系统交互，那么成本是很高的**。

* 这就需要我们在**应用层**设计好内存分配的多级缓存，**尽量减少小对象高频创建与销毁时的锁竞争**，这个问题在传统的 C/C++ 语言中已经有了解法，那就是 tcmalloc：

* ![img](https://static001.geekbang.org/resource/image/fd/60/fd5c558298850305b5dae88b856d1c60.png?wh=833x353)

* tcmalloc 通过维护一套多级缓存结构，降低了**应用内存分配过程中对全局锁的使用频率**，使小对象的内存分配做到了**尽量无锁**。

* 我们可以将内存分配的路径与 CPU 的多级缓存作类比，这里 mcache 内部的 tiny 可以类比为 L1 cache，而 **alloc 数组中的元素可以类比为 L2 cache**，全局的 **mheap.mcentral 结构为 L3 cache**，**mheap.arenas 是 L4**，L4 是以页为单位将内存向下派发的，由 pageAlloc 来管理 arena 中的空闲内存。

* > * tiny ：size < 16 bytes && has no pointer(noscan)；
  > * small ：has pointer(scan) || (size >= 16 bytes && size <= 32 KB)；
  > * large ：size > 32 KB。

* ![img](https://static001.geekbang.org/resource/image/13/8c/13be38850045c513f9393da115fdc18c.jpg?wh=856x268)

* 如果 L4 也没法满足我们的内存分配需求，那我们就需要向**操作系统去要内存了**。

* **small 类型**的内存没有本地的 mcache.tiny 缓存，其余的与 tiny 分配路径完全一致.

* large 内存分配稍微特殊一些，没有前面这两类这样复杂的缓存流程，而是**直接从 mheap.arenas 中要内存**，**直接走 pageAlloc 页分配器**。



###### 内存分配的数据结构之间的关系

* **arenas 是 Go 向操作系统申请内存时的最小单位**，**每个 arena 为 64MB 大小**，在内存中可以部分连续，但整体是个稀疏结构。
* 单个 arena 会被**切分成以 8KB 为单位的 page**，由 **page allocator 管理**，一个或多个 page 可以组成一个 mspan，每个 mspan 可以按照 sizeclass 再划分成多个 element。同样大小的 **mspan 又分为 scan 和 noscan 两种**，分别对应内部有指针的 object 和内部没有指针的 object。
* ![img](https://static001.geekbang.org/resource/image/0b/9a/0be6329b8a9a40d3eaa693c2e6b3ed9a.png?wh=1920x1500)
* **每一个 mspan 都有一个 allocBits 结构**，从 mspan 里分配 element 时，我们只要将 mspan 中对应该 element 位置的 bit 位置一就可以了，其实就是将 mspan 对应 allocBits 中的对应 bit 位置一。
* ![img](https://static001.geekbang.org/resource/image/e9/5f/e9c7b25f9fc787c7317a07aac648d65f.png?wh=1664x726)



##### 垃圾回收

* 基本流程
* ![image-20221029214935854](/home/zhoubing/.config/Typora/typora-user-images/image-20221029214935854.png)

* 

* Go 语言使用了**并发标记与清扫算法**作为它的 GC 实现。

* 标记、清扫算法是一种古老的 GC 算法，是指将**内存中正在使用的对象进行标记**，之后清扫掉那些未被标记的对象的一种垃圾回收算法。

* 标记根对象：

* > * 静态变量、全局变量、常量、线程栈等
  > * 标记清除可能出现内存碎片。

* 标记： 找到可达的对象

* > * 求指针指向的关系的传递闭包：从根对象出发，找到所有可达的对象.

* 清理：清理不可达的对象

* > * 将**存活对象复制到另外的内存空间**（copying GC）
  > * ![image-20220824211247457](/home/zhoubing/.config/Typora/typora-user-images/image-20220824211247457.png)
  > * 将死亡的对象的内存标记为 "可分配" (Mark-swap GC) 使用 free list 管理空闲的内存
  > * ![image-20220824211335463](/home/zhoubing/.config/Typora/typora-user-images/image-20220824211335463.png)
  > * 移动并整理**存活对象** (Mark-compact GC):**原地整理**对象
  > * **![image-20220824211422874](/home/zhoubing/.config/Typora/typora-user-images/image-20220824211422874.png)**

* **根据生命周期的不同，使用不同的标记和清理策略。**

* **分代 GC：**

* > * 每一个对象都有年龄：经历过的 GC 次数
  >
  > * 目的： 对年轻代和老年的对象，指定不同的 GC 策略，降低整体内存管理的开销
  >
  > * 不同年龄的对象处于 heap 的不同的区域
  >
  > * ![image-20220824212443924](/home/zhoubing/.config/Typora/typora-user-images/image-20220824212443924.png)
  >
  > * 年轻代：
  >
  > * > * 常规的对象分配
  >   > * 由于存活的对象很少，可以采用 copying collection
  >   > * GC 的吞吐率很高
  >
  > * 老年代：
  >
  > * > * 对象一直趋向于活着，反复的复制开销非常的大。
  >   > * 可以采用 mark-sweep collection

* ![image-20220824211213373](/home/zhoubing/.config/Typora/typora-user-images/image-20220824211213373.png)

* 

* 并发标记与清扫重点在**并发**，是指垃圾回收的**标记和清扫过程能够与应用代码并发执行**。

* **并发标记清扫算法的一大缺陷是无法解决内存碎片问题**，而 tcmalloc 恰好一定程度上缓解了内存碎片问题，两者配合使用相得益彰。

* 引用技术清除的方式的缺陷：

* > * 额外空间
  > * 自依赖（循环引用）
  > * 并发性（加锁之类的）

###### 垃圾分类

* 进行垃圾回收之前，我们要先对**内存垃圾进行分类**，主要可以分为**语义垃圾和语法垃圾**两类，但并不是所有垃圾都可以被垃圾回收器回收。
* **语义垃圾（semantic garbage）**，有些场景也被称为**内存泄露**，指的是从语法上可达（可以通过局部、全局变量被引用）的对象，但从语义上来讲他们是垃圾，垃圾回收器对此无能为力。
* ![img](https://static001.geekbang.org/resource/image/8e/d5/8eb4fc5be983a6821b0581eab2a3aed5.png?wh=1900x1072)
* 我们初始化了一个 slice，元素均为指针，每个指针都指向了堆上 10MB 大小的一个对象。
* ![img](https://static001.geekbang.org/resource/image/a4/87/a45499dab31de5e0888d3002e3529387.png?wh=1870x1174)
* 当这个 **slice 缩容时**，底层数组的**后两个元素已经无法再访问了**，但它关联的堆上内存依然是无法释放的。
* 碰到类似的场景，你可能需要在缩容前，先将数组元素置为 nil。
* **语法垃圾（syntactic garbage）**，讲的是那些从**语法上无法到达的对象**，这些才是垃圾收集器主要的收集目标。
* ![img](https://static001.geekbang.org/resource/image/f6/f3/f687c68dbf35ee45b208e561352493f3.png?wh=1152x978)
* 在 allocOnHeap **返回后**（这个函数执行结束了），堆上的 **a 无法访问，便成为了语法垃圾**。



##### GC 流程（三色标记）

###### 需要多少标记协程进行工作？

* ![image-20221029215356367](/home/zhoubing/.config/Typora/typora-user-images/image-20221029215356367.png)
* 25% 协程执行标记
* ![image-20221029215506763](/home/zhoubing/.config/Typora/typora-user-images/image-20221029215506763.png)
* 

###### 如何进行调度使标记协程运行







###### 什么是 STW

* 在gc事件发生过程中，会产生应用的停顿，**停顿产生时整个应用程序线程都会被暂停**，没有任何响应，有点像卡死的感觉，这个停顿称为STW

* 为什么需要 STW 

  > * 可达性分析算法中，枚举根节点(GC ROOT)会导致所有java程序执行线程停顿。
  > * 分析工作必须在一个能确保一致性的快照中进行
  > * 一致性指整个分析期间整个执行系统像被冻结在某个时间点上
  > * 如果出现**分析过程中对象引用关系还在不断地变化，则分析结果的准确性无法保证**。
  > * 举个例子，如果分析时判断该对象已经没有引用，而分析后下一秒该对象又有了引用，则gc时会将该对象清理，导致错误发生。

![img](https://static001.geekbang.org/resource/image/64/7b/64d7433118dc109f6616c07681bc127b.png?wh=1920x738)



* 在**并发标记开始前和并发标记终止时**，有两个短暂的 stw，该 stw 可以使用 pprof 的 pauseNs 来观测，也可以直接采集到监控系统中

###### 标记流程

* Go 语言使用**三色抽象作为其并发标记的实现**。所以这里我们首先要理解三种颜色的抽象：

* > * 黑表示已经扫描完毕，子节点扫描完毕（gcmarkbits = 1，且在队列外）；（一般将根对象理解成黑色对象）
  > * 灰表示已经扫描完毕，子节点未扫描完毕（gcmarkbits = 1, 在队列内）；（根对象引用的就是灰对象）
  > * 白表示未扫描，collector 不知道任何相关信息。

* ![image-20221029220634188](/home/zhoubing/.config/Typora/typora-user-images/image-20221029220634188.png)

* span 中标记为灰色对象，一直重复这个操作.

![img](https://static001.geekbang.org/resource/image/e8/11/e87f2cc71ea90d0e2a084211d33cf311.jpeg?wh=1920x1080)

* 使用三色抽象，主要是为了**能让垃圾回收流程与应用流程并发执行**，这样**将对象扫描过程拆分为多个阶段**，不需要一次性完成整个扫描流程。
* GC 扫描的起点是根对象，忽略掉那些不重要的（finalizer 相关的先省略）
* ![img](https://static001.geekbang.org/resource/image/db/f5/dbc5ab091c2ac0f026ebddc97926fef5.png?wh=1920x1301)
* 在 Go 语言中，从根开始扫描的含义是**从 .bss 段，.data 段以及 goroutine 的栈开始扫描**，最终遍历**整个堆上的对象树**。
* 标记过程是一个**广度优先的遍历过程**。它是扫描节点，将**节点的子节点推到任务队列中**，然后递归扫描子节点的子节点，直到所有工作队列都被排空为止。
* ![img](https://static001.geekbang.org/resource/image/39/df/3942dd9c208f7841d0f5d0310fa2f0df.gif?wh=1222x678)
* 标记过程会将**白色对象标记，并推进队列中变成灰色对象**。我们可以看看 scanobject 的具体过程：
* ![img](https://static001.geekbang.org/resource/image/94/4c/94ddaa38fc4923d3be06de7b47f3964c.gif?wh=1226x678)
* 在标记过程中，gc mark worker 会一边从工作队列（gcw）中弹出对象，一边把它的子对象 push 到工作队列（gcw）中，如果工作队列满了，则要将一部分元素向全局队列转移。
* **堆上对象本质上是图**，会存储引用关系互相交叉的时候，在标记过程中也有简单的剪枝逻辑：
* ![img](https://static001.geekbang.org/resource/image/8d/5c/8d9066cdfd55f0cc3a6e23216dd42f5c.png?wh=1920x1287)
* D 是 A 和 B 的共同子节点，**在标记过程中自然会减枝**，防止**重复标记**浪费计算资源：
* ![img](https://static001.geekbang.org/resource/image/0a/b5/0a139bd980ff1e88ec529e4eb29849b5.png?wh=808x306)
* 如果多个后台 mark worker 确实产生了并发，**标记时使用的是 atomic.Or8**，也是并发安全的：
* ![img](https://static001.geekbang.org/resource/image/16/f2/168d6ba157a7ac43c699873d3ccbf7f2.png?wh=1428x402)



###### 协助标记

* 当应用分配内存过快时，后台的 mark worker 无法及时完成标记工作，这时应用本身需要进行堆内存分配时，会判断是否需要适当协助 GC 的标记过程，防止应用因为分配过快发生 OOM。



###### 对象丢失问题

* 
* 前面我们提到了 GC 线程 / 协程与应用线程 / 协程是并发执行的
* **在 GC 标记 worker 工作期间**，应用还会**不断地修改堆上对象的引用关系**，这就可能**导致对象丢失**问题。
* 下面是一个典型的应用与 GC 同时执行时，由于**应用对指针的变更导致对象漏标记，从而被 GC 误回收的情况**。
* ![img](https://static001.geekbang.org/resource/image/19/a5/193926b4yydde270f88c62d2a8dc21a5.gif?wh=1224x674)
* 在这张图表现的 GC 标记过程中，应用动态地修改了 A 和 C 的指针，让 A 对象的内部指针指向了 B，C 的内部指针指向了 D。如果标记过程垃圾收集器无法感知到这种变化，**最终 B 对象在标记完成后是白色**，会**被错误地认作内存垃圾被回收**。
* **定义“三色不变性”**，如果我们的堆上对象的引用关系不管怎么修改，都能满足三色不变性，那么也不会发生对象丢失问题。
* **强三色不变性（strong tricolor invariant）**，**禁止黑色对象指向白色对象**：
* <img src="https://static001.geekbang.org/resource/image/e5/7e/e5bc46c9a0d590dc7e4baa8b6391527e.png?wh=866x1188" alt="img" style="zoom:50%;" />
* **弱三色不变性（weak tricolor invariant）**，黑色对象可以指向白色对象，但**指向的白色对象，必须有能从灰色对象可达的路径**：
* <img src="https://static001.geekbang.org/resource/image/00/1a/00af8b225d3f5ca747bc6356d389671a.png?wh=964x1188" alt="img" style="zoom:50%;" />
* 无论应用在与 GC 并发执行期间如何修改堆上对象的关系，**只要修改之后，堆上对象能满足任意一种不变性，就不会发生对象的丢失问题**。

###### 并发困境

* ![image-20221029220753433](/home/zhoubing/.config/Typora/typora-user-images/image-20221029220753433.png)
* 

###### 写屏障- write barrier

* STW 存在的原因： **暂停的时候可以真正的关注到垃圾回收的生命周期，什么时候开始以及什么时候停止**。

* Go 1.8 使用混合写屏障技术将 STW 的时间压缩到 0.5ms 。

* Go1.14 使用抢占式协程调度和基数树内存管理。

* **实现强 / 弱三色不变性均需要引入屏障技术**

* 强： 不允许白色对象被黑色对象引用

* 弱：可以被引用，但是必须要保证能被扫描到

* > * Dijistra Insertion Barrier，指针修改时，**指向的新对象要标灰：**
  > * ![img](https://static001.geekbang.org/resource/image/ed/36/ed18f2d72c1411761d85645d726c2c36.png?wh=982x352)
  > * Yuasa Deletion Barrier，**指针修改时，修改前指向的对象要标灰：**
  > * ![img](https://static001.geekbang.org/resource/image/0a/ba/0a399de8c0764c53e9d765a3dd2b53ba.png?wh=942x346)
  > * 如果 Go 语言的所有对象都在**堆上**，使用上述两种屏障的任意一种，都不会发生对象丢失的问题。
  > * Go 是不给**栈上**对象开启屏障的。因为实现非常的复杂。

###### 回收流程

* 进程启动时会有两个特殊 goroutine：

* > * 一个叫 sweep.g，**主要负责清扫死对象，合并相关的空闲页**；
  > * 一个叫 scvg.g，主要负责**向操作系统归还内存**。

* 当 GC 的标记流程结束之后，**sweep goroutine 就会被唤醒，进行清扫工作**，其实就是**循环执行** sweepone -> sweep。针对每个 mspan，sweep.g 的工作是将**标记期间生成的 bitmap 替换掉分配时使用的 bitmap**：

* ![img](https://static001.geekbang.org/resource/image/b9/29/b9ec19a138f667df5730d7895d9e3d29.png?wh=1042x666)

* **mspan：用标记期间生成的 bitmap 替换掉分配内存时使用的 bitmap**

* 然后根据 mspan 中的槽位情况决定该 mspan 的去向：

* > * 如果 mspan 中存活对象数 = 0，也就是所有 element 都变成了内存垃圾，那执行 freeSpan -> 归还组成该 mspan 所使用的页，并更新全局的页分配器摘要信息；
  > * 如果 mspan 中没有空槽，说明所有对象都是存活的，将其放入 fullSwept 队列中；
  > * 如果 mspan 中有空槽，说明**这个 mspan 还可以拿来做内存分配**，将其放入 partialSweep 队列中。

* 之后“清道夫” **scvg goroutine 被唤醒**，执行线性流程，一路运行到将**页内存归还给操作系统**，也就是 bgscavenge -> pageAlloc.scavenge -> pageAlloc.scavengeOne -> pageAlloc.scavengeRangeLocked -> sysUnused -> madvise.





#### 垃圾回收 + 混合写屏障

<embed id="pdfPlayer" src="/home/zhoubing/Downloads/gc.pdf" type="application/pdf" width="100%" height="1000" >

* **栈对象不可能成为堆对象的下游，因为当栈对象被堆对象引用的时候，栈对象会从栈中逃逸到堆上变成堆对象**。
* 为了保证在栈上的速度，我们不在栈上实现插入屏障.(这就是插入屏障不足的地方，需要再次使用 STW 扫描栈).
* 栈对象可以引用堆对象，所以 GC 需要遍历栈对象。
* 删除屏障存在下一次才能 GC 的原因： 因为可能别的黑色对象指向它（因为之前满足弱三色一致性，可以被引用），但是这时候如果删除不置成灰色，可能就会被 GC 回收.
* 混合写屏障，并不是不需要STW：混合写屏障是去除整体的STW 的一个改进，转而并发一个一个栈处理的方式(每个栈单独暂停)，从而消除了整机 STW 的影响，带来了吞吐的提升。



#### GMP 

<embed id="pdfPlayer" src="/home/zhoubing/Downloads/gmp.pdf" type="application/pdf" width="100%" height="1000" >







#### 什么时候出发 GC

##### 系统触发

* 在系统触发的场景中，Go 源码的 `src/runtime/mgc.go` 文件，明确标识了 GC 系统触发的三种场景，分别如下：

* ```go
  const (
   gcTriggerHeap gcTriggerKind = iota
   gcTriggerTime
   gcTriggerCycle
  )
  
  ```

* gcTriggerHeap：当所分配的堆大小达到阈值（由控制器计算的触发堆的大小）时，将会触发。

* gcTriggerTime：当距离上一个 GC 周期的时间超过一定时间时，将会触发。-时间周期以 `runtime.forcegcperiod` 变量为准，默认 2 分钟。

* gcTriggerCycle：如果没有开启 GC，则启动 GC。

##### 手动触发

* 在手动触发的场景下，Go 语言中仅有 `runtime.GC` 方法可以触发，也就没什么额外的分类的。

* ![img](https://ask.qcloudimg.com/http-save/yehe-5709307/d1a1a0fb2059c4fe85be348deea3f283.png?imageView2/2/w/1620)

* 但我们要思考的是，一般我们在什么业务场景中，要涉及到手动干涉 GC，强制触发他呢？

  需要手动强制触发的场景极其少见，可能会是在某些业务方法执行完后，因其占用了过多的内存，需要人为释放。又或是 debug 程序所需。

##### 在哪触发

* .Go 是哪里实现的触发的机制，似乎在流程中完全没有看到？

###### 监控线程

* 实质上在 Go 运行时（runtime）初始化时，会启动一个 goroutine，用于处理 GC 机制的相关事项。

* ```go
  func init() {
   go forcegchelper()
  }
  
  func forcegchelper() {
   forcegc.g = getg()
   lockInit(&forcegc.lock, lockRankForcegc)
   for {
    lock(&forcegc.lock)
    if forcegc.idle != 0 {
     throw("forcegc: phase error")
    }
    atomic.Store(&forcegc.idle, 1)
    goparkunlock(&forcegc.lock, waitReasonForceGCIdle, traceEvGoBlock, 1)
      // this goroutine is explicitly resumed by sysmon
    if debug.gctrace > 0 {
     println("GC forced")
    }
  
    gcStart(gcTrigger{kind: gcTriggerTime, now: nanotime()})
   }
  }
  ```

* 在这段程序中，需要特别关注的是在 `forcegchelper` 方法中，会调用 `goparkunlock` 方法让该 goroutine 陷入休眠等待状态，以减少不必要的资源开销。

* 在休眠后，会由 `sysmon` 这一个系统监控线程来进行监控、唤醒等行为：

* ```go
  func sysmon() {
   ...
   for {
    ...
    // check if we need to force a GC
    if t := (gcTrigger{kind: gcTriggerTime, now: now}); t.test() && atomic.Load(&forcegc.idle) != 0 {
     lock(&forcegc.lock)
     forcegc.idle = 0
     var list gList
     list.push(forcegc.g)
     injectglist(&list)
     unlock(&forcegc.lock)
    }
    if debug.schedtrace > 0 && lasttrace+int64(debug.schedtrace)*1000000 <= now {
     lasttrace = now
     schedtrace(debug.scheddetail > 0)
    }
    unlock(&sched.sysmonlock)
   }
  }
  ```

* 这段代码核心的行为就是不断地在 for 循环中，对 `gcTriggerTime` 和 `now` 变量进行比较，判断是否达到一定的时间（默认为 2 分钟）。

* 若达到意味着满足条件，会将 `forcegc.g` 放到全局队列中接受新的一轮调度，再进行对上面 `forcegchelper` 的唤醒。

* 



#### 内存逃逸

##### 什么是内存逃逸

* `golang程序变量`会携带有一组校验数据，用来证明它的整个生命周期是否在运行时完全可知。如果变量通过了这些校验，它就可以在`栈上`分配。否则就说它 `逃逸` 了，必须在`堆上分配`。
* **编译器经过逃逸分析**后将其在堆上分配内存还是在栈中分配内存。
* 写过`C`语言的朋友应该都知道`C`语言函数是不能返回局部变量地址(特指存放于栈区的局部变量地址)，除非是**局部静态变量地址，字符串常量地址、动态分配地址**。
* 定义： 在一段程序中，每一个函数都会有自己的内存区域存放自己的局部变量、返回地址等，这些内存会由编译器在栈中进行分配，每一个函数都会分配一个栈桢，在函数运行结束后进行销毁，但是**有些变量我们想在函数运行结束后仍然使用它，那么就需要把这个变量在堆上分配，这种从"栈"上逃逸到"堆"上的现象就成为内存逃逸。**

##### 什么是逃逸分析

* 定义： 逃逸分析就是指**程序在编译阶段根据代码中的数据流**，对代码中**哪些变量需要在栈中分配，哪些变量需要在堆上分配进行静态分析**的方法。

* `Go`语言引入了`GC`机制，`GC`机制会对**位于堆上的对象进行自动管理**，当某个**对象不可达时**(即没有其对象引用它时)，他将会被回收并被重用。

* 虽然引入`GC`可以让开发人员降低对内存管理的心智负担，但是`GC`也会给程序带来性能损耗，当**堆内存中有大量待扫描的堆内存对象时**，将会给`GC`带来过大的压力。

* `Go`语言使用的是**标记清除算法**，并且在此基础上使用了**三色标记法和写屏障技术**，提高了效率。

* 但是如果我们的程序仍在堆上分配了大量内存，依赖会对`GC`造成不可忽视的压力。因此为了减少`GC`造成的压力，`Go`语言引入了逃逸分析，也就是想法设法**尽量减少在堆上的内存分配**，**可以在栈中分配的变量尽量留在栈中**。

* 堆栈区别：

* > * 堆和栈相比，**堆适合不可预知大小的内存分配**。但是为此付出的代价是**分配速度较慢，而且会形成内存碎片**。
  > * **栈内存分配则会非常快**。栈分配内存只需要两个CPU指令：“PUSH”和“RELEASE”，分配和释放
  > * 堆分配内存首先需要去找到一块大小合适的内存块，之后要通过垃圾回收才能释放。所以**逃逸分析更做到更好内存分配，提高程序的运行速度**。

* 逃逸分析原理如下：

* > * `pointers to stack objects cannot be stored in the heap`：指向**栈对象的指针不能存储在堆中**
  > * `pointers to a stack object cannot outlive that object`：指向栈对象的指针不能超过该对象的存活期，也就说指针不能在栈对象被销毁后依旧存活。（例子：声明的函数返回并销毁了对象的栈帧，或者它在循环迭代中被重复用于逻辑上不同的变量）

##### 逃逸分析案例

###### 函数返回值是局部变量：

```go
func Add(x,y int) *int {
    res := 0
    res = x + y
    return &res
}

func main()  {
    Add(1,2)
}

go build -gcflags="-m -m -l" ./test1.go
# command-line-arguments
./test1.go:6:9: &res escapes to heap
./test1.go:6:9:         from ~r2 (return) at ./test1.go:6:2
./test1.go:4:2: moved to heap: res
```

* 当函数`Add`执行结束后，对应的栈桢就会被销毁，但是引用已经返回到函数之外，如果我们在外部解引用地址，就会导致程序访问非法内存，就像上面的`C`语言的例子一样，编译器经过逃逸分析后将其在堆上分配内存。
* 

###### interface 类型逃逸

* ```go
  func main()  {
      str := "asong太帅了吧"
      fmt.Printf("%v",str)
  }
  
  go build -gcflags="-m -m -l" ./test2.go 
  # command-line-arguments
  ./test2.go:9:13: str escapes to heap
  ./test2.go:9:13:        from ... argument (arg to ...) at ./test2.go:9:13
  ./test2.go:9:13:        from *(... argument) (indirection) at ./test2.go:9:13
  ./test2.go:9:13:        from ... argument (passed to call[argument content escapes]) at ./test2.go:9:13
  ./test2.go:9:13: main ... argument does not escape
  ```

* `str`是`main`函数中的一个局部变量，传递给`fmt.Println()`函数后发生了**逃逸**

* 因为`fmt.Println()`函数的**入参是一个`interface{}`类型**，如果函数参数为`interface{}`，那么在**编译期间就很难确定其参数的具体类型**，也会发送逃逸。

* 我们可以看到没有`moved to heap: str`，这也就是**说明`str`变量并没有在堆上进行分配**，只是它**存储的值逃逸到堆上了**，也就说任何被`str`引用的对象必须分配在堆上。



###### 变量大小不确定或者栈空间不足

```go
package main

import (
    "math/rand"
)

func LessThan8192()  {
    nums := make([]int, 100) // = 64KB
    for i := 0; i < len(nums); i++ {
        nums[i] = rand.Int()
    }
}


func MoreThan8192(){
    nums := make([]int, 1000000) // = 64KB
    for i := 0; i < len(nums); i++ {
        nums[i] = rand.Int()
    }
}


func NonConstant() {
    number := 10
    s := make([]int, number)
    for i := 0; i < len(s); i++ {
        s[i] = i
    }
}

func main() {
    NonConstant()
    MoreThan8192()
    LessThan8192()
}


go build -gcflags="-m -m -l" ./test4.go
# command-line-arguments
./test4.go:8:14: LessThan8192 make([]int, 100) does not escape
./test4.go:16:14: make([]int, 1000000) escapes to heap
./test4.go:16:14:       from make([]int, 1000000) (non-constant size) at ./test4.go:16:14
./test4.go:25:11: make([]int, number) escapes to heap
./test4.go:25:11:       from make([]int, number) (non-constant size) at ./test4.go:25:11
```

* 当栈空间足够时，不会发生逃逸，但是当变量过大时，已经完全超过栈空间的大小时，将会发生逃逸到堆上分配内存。
* 同样当我们初始化切片时，**没有直接指定大小，而是填入的变量**，这种情况为了**保证内存的安全，编译器也会触发逃逸**，在堆上进行分配内存。









#### GMP

* goroutine 特点：

* > * 相比线程，其启动的代价很小，以很小栈空间启动（2Kb左右）.
  > * 能够**动态地伸缩栈的大小**，最大可以支持到Gb级别.
  > * 工作在用户态，切换成很小.
  > * 与线程关系是n:m，即可以在n个系统线程上多工调度m个Goroutine.

* **KSE是Kernel Scheduling Entity的缩写，其是可被操作系统内核调度器调度的对象实体，是操作系统内核的最小调度单元，可以简单理解为内核级线程**。
* **用户级线程即协程**，由应用程序创建与管理，协程必须与内核级线程绑定之后才能执行.
* **线程由 CPU 调度是抢占式的，协程由用户态调度是协作式的，一个协程让出 CPU 后，才执行下一个协程**。



##### 线程模型

* **内核线程是真正的线程。因为它会分配到 CPU 的执行资源。**

* **内核拥有较高权限**，因此可以在多个 CPU 核心上执行内核线程。

* 线程创建、管理、调度等采用的方式称为线程模型。线程模型一般分为以下三种：

* > - 内核级线程(Kernel Level Thread)模型
  > - 用户级线程(User Level Thread)模型
  > - 两级线程模型，也称混合型线程模型

* 三大线程模型最大差异就在于**用户级线程与内核调度实体KSE**（KSE，Kernel Scheduling Entity）之间的对应关系。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3014ebb137414b968ea0deed8c4196e2~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

* **内核级线程模型中用户线程与内核线程是一对一关系（1 : 1）**。**线程的创建、销毁、切换工作都是有内核完成的**，应用程序**不参与线程的管理工作**，只能调用内核级线程编程接口(应用程序创建一个新线程或撤销一个已有线程时，都会进行一个系统调用）

* 用户线程在其生命期内都会绑定到该内核线程。一旦用户线程终止，**两个线程都将离开系统**。

* 操作系统为**每个线程创建上下文**。进程的**每个线程在资源可用时都可以被指派到处理器内核**。

* 内核级线程模型的优点：

* > * 在多处理器系统中，**内核能够并行执行同一进程内的多个线程**
  > * 如果**进程中的一个线程被阻塞**，不会阻塞其他线程，是能够切换同一进程内的其他线程继续执行
  > * 当一个线程阻塞时，**内核根据选择可以运行另一个进程的线程**，而**用户空间实现的线程中，运行时系统始终运行自己进程中的线程**

* 缺点： 线程的创建与删除都需要CPU参与，**成本大**

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1273aa1b15604a66a008c140ddf4ee26~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

* **用户线程模型中的用户线程与内核线程KSE是多对一关系（N : 1）**。**线程的创建、销毁以及线程之间的协调、同步等工作都是在用户态完成**。（不是内核完成的）

* **内核对这些是无感知的，内核此时的调度都是基于进程的**。

* 线程的并发处理从宏观来看，任意时刻**每个进程只能够有一个线程在运行**，且**只有一个处理器内核会被分配给该进程**。

* 库调度器从进程的多个线程中选择一个线程，然后该线程和该进程允许的一个内核线程关联起来。内核线程将被操作系统调度器指派到处理器内核。**用户级线程是一种”多对一”的线程映射**

* 优点：

* > * 创建和销毁线程、线程切换代价等线程管理的代价比内核线程少得多, 因为**保存线程状态的过程和调用程序都只是本地过程**
  > * 线程能够利用的表空间和堆栈空间比内核级线程多

* 缺点：

* > * **线程发生I/O或页面故障引起的阻塞时**，如果**调用阻塞系统调用则内核由于不知道有多线程的存在**，而会阻塞整个进程从而阻塞所有线程, 因此同一进程中只能同时有一个线程在运行
  > * 资源调度按照进程进行，多个处理机下，同一个进程中的线程只能在同一个处理机下**分时复用**

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a1014ba5fb88450ba11028079fc5bd01~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

* **两级线程模型中用户线程与内核线程是一对一关系（N : M）**。
* 两级线程模型充分吸收上面两种模型的优点，尽量规避缺点。
* **其线程创建在用户空间中完成**，线程的调度和同步也在应用程序中进行。一个应用程序中的多个用户级线程被绑定到一些（小于或等于用户级线程的数目）内核级线程上。



##### Golang 线程模型

* **Golang在底层实现了混合型线程模型**。**M即系统线程**，由系统调用产生，**一个M关联一个KSE**，即两级线程模型中的系统线程。G为Groutine，即两级线程模型的的应用及线程。M与G的关系是N:M。
* ![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/96a6257a915441698e2f3f9123efbdec~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

* ![G-M-P模型概览图](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fbaf7adc2b3d4878834fcc8a27e7a2f3~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)
* G-M-P分别代表：
  - G - Goroutine，Go协程，是参与调度与执行的最小单位
  - M - Machine，指的是**系统级线程**
  - P - Processor，指的是**逻辑处理器**，P关联了的本地可运行**G的队列**(也称为LRQ)，最多可存放256个G。
* GMP调度流程大致如下：
  - 线程M想运行任务就需得获取 P，即与P(cpu 核心）关联。
  - 然从 P 的本地队列(LRQ)获取 G
  - 若LRQ中没有可运行的G，M 会尝试从全局队列(GRQ)拿一批G放到P的本地队列，
  - 若全局队列也未找到可运行的G时候，**M会随机从其他 P 的本地队列偷一半放到自己 P 的本地队列。**
  - 拿到可运行的G之后，M 运行 G，G 执行之后，M 会从 P 获取下一个 G，不断重复下去。

* GMP 数量

* > * G : 理论上没有数量上限限制的。查看当前G的数量可以使用`runtime. NumGoroutine()`
  > * P : 由启动时环境变量 `$GOMAXPROCS` 或者是由`runtime.GOMAXPROCS()` 决定。这意味着在程序执行的任意时刻都只有 $GOMAXPROCS 个 goroutine 在同时运行。
  > * M : go 语言本身的限制：go 程序启动时，会设置 M 的最大数量，默认 10000. 但是**内核很难支持这么多的线程数**，所以这个限制可以忽略。

* 

##### 调度流程

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ca4939a070a4425282b4ec008f69f0fb~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)



* 每个P有个局部队列，局部队列保存待执行的goroutine(流程2)，当M绑定的P的的局部队列已经满了之后就会把goroutine放到全局队列(流程2-1)
* 每个P和一个M绑定，M是真正的执行P中goroutine的实体(流程3)，M从绑定的P中的局部队列获取G来执行
* 当M绑定的P的局部队列为空时，M会从全局队列获取到本地队列来执行G(流程3.1)，当从全局队列中没有获取到可执行的G时候，M会从其他P的局部队列中偷取G来执行(流程3.2)，这种从其他P偷的方式称为**work stealing**
* 当**G因系统调用(syscall)阻塞时会阻塞M**，此时P会和M解绑即**hand off**，并寻找新的idle的M，若没有idle的M就会新建一个M(流程5.1)。
* 当**G因channel或者network I/O阻塞时**，不会阻塞M，M会寻找其他runnable的G；当阻塞的G恢复后会重新**进入runnable进入P队列等待执行**(流程5.3)



##### 总结

* Golang的线程模型采用的是**混合型线程模型**，线程与协程关系是N:M。

* Golang混合型线程模型实现采用GMP模型进行调度，G是goroutine，是golang实现的协程，M是OS线程，P是逻辑处理器。

* 每一个M都需要与一个P绑定，P拥有本地可运行G队列，M是执行G的单元，M获取可运行G流程是先从P的本地队列获取，若未获取到，则从其他P偷取过来（即work steal)，若其他的P也没有则从全局G队列获取，若都未获取到，则M将处于自旋状态，并不会销毁。

* 当执行G时候，发生通道阻塞等用户级别阻塞时候，此时M不会阻塞，M会继续寻找其他可运行的G，当阻塞的G恢复之后，重新进入P的队列等待执行，若G进行系统调用时候，会阻塞M，此时P会和M解绑(即hand off)，并寻找新的空闲的M。若没有空闲的就会创建一个新的M。

* Work Steal和Hand Off保证了线程的高效利用。



######  GMP 高效原因

* **M是可以复用的**，不需要反复创建与销毁，当没有可执行的Goroutine时候就处于自旋状态，等待唤醒

* **Work Stealing和Hand Off策略**保证了M的高效利用

* 内存分配状态(mcache)位于P，G可以跨M调度，不再存在跨M调度局部性差的问题

* **M从关联的P中获取G，不需要使用锁**，是lock free的













#### slice 扩容

```go
// 扩容的源码
newcap := old.cap
doublecap := newcap + newcap
if cap > doublecap {
    newcap = cap
} else {
    if old.cap < 1024 {
        newcap = doublecap
    } else {
        // Check 0 < newcap to detect overflow
        // and prevent an infinite loop.
        for 0 < newcap && newcap < cap {
            newcap += newcap / 4
        }
        // Set newcap to the requested cap when
        // the newcap calculation overflowed.
        if newcap <= 0 {
            newcap = cap
        }
    }
}
```

* append单个元素，或者append少量的多个元素，这里的少量指double之后的容量能容纳，这样就会走以下扩容流程，不足1024，双倍扩容，超过1024的，1.25倍扩容。

* 若是**append多个元素**，且double后的容量不能容纳，直接使用预估的容量。

* ```go
  func main() {
  	a := []byte{1, 0}
  	a = append(a, 1, 1, 1)
  	fmt.Println("cap of a is ", cap(a))
  
  	b := []int{23, 51}
  	b = append(b, 4, 5, 6)
  	fmt.Println("cap of b is ", cap(b))
  
  	c := []int32{1, 23}
  	c = append(c, 2, 5, 6)
  	fmt.Println("cap of c is ", cap(c))
  
  	type D struct {
  		age  byte
  		name string
  	}
  	d := []D{
  		{1, "123"},
  		{2, "234"},
  	}
  
  	d = append(d, D{4, "456"}, D{5, "567"}, D{6, "678"})
  	fmt.Println("cap of d is ", cap(d))
  }
  
  
  // 打印出的内容
  cap of a is  8
  cap of b is  6
  cap of c is  6
  cap of d is  5
  ```

* **以上两个分支得到新容量后，均需要根据slice的类型size，算出新的容量所需的内存情况`capmem`，然后再进行`capmem`向上取整，得到新的所需内存，除上类型size，得到真正的最终容量,作为新的slice的容量。**





#### map 扩容

```go
if !h.growing() && (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) {
    hashGrow(t, h)
    goto again // Growing the table invalidates everything, so try again
}
```

* map 的**负载因子超过 6.5，一种是溢出桶太多了出现扩容**
* 在 Go map 中采用的就是 **"链地址法 " 去解决哈希冲突**，又称 "拉链法"。



##### 如何扩容

* 使用的函数：

* > 1. hashGrow：负责初始化新的桶，以及设置扩容后的map的各个字段
  > 2. growWork：每调用一次growWork函数，都**至多会迁移两个桶的数据**
  > 3. evacuate：**真正负责迁移数据的函数，会负责迁移指定桶中的数据**
  > 4. advanceEvacuationMark：收尾工作，增加nevacuate，如果所有的oldbuckets都迁移完成了，会摘除oldbuckets

* **golang的map数据迁移是渐进式的迁移**，不是一次性的，在判断需要扩容之后，会调用hashGrow函数，hashGrow函数并没有做实际的迁移操作，它**只是根据扩容类型创建新的桶，并将原来的桶挂载在map的指定位置**，设置了hmap的各个字段，经过hashGrow函数处理后，新的hmap结构如下所示:

* ![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0c3ca87dfe1841218549f2f5078c486a~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

* **真正迁移数据的操作会散布在后续的map修改操作中，即mapassign和mapdelete函数**，在两个函数中都会有下列的代码片段，会判断当前map是否处在扩容阶段，如果是，则会调用growWork函数进行数据迁移:

* ![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/69413dbd31a44d5caffcafad8d11cd07~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

* g**rowWork函数并不会真正进行数据迁移**，它会调用**evacuate函数来完成迁移工作**，growWork函数每次会迁移至多两个桶的数据，一个是目前需要使用的桶，一个是h.nevacuate桶（这里很重要，在后面判断是否迁移过程中有很大的作用），h.nevacuate记录的是目前迁移的桶的个数

* ![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/26cf8da9511a42609fab00f64b52ed94~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

* evacuate是真正进行数据迁移的函数，它每次会迁移一个bmap中的数据，简单说，就是遍历旧有buckets中bmap中的数据，将其放到新bmap的对应位置即可



###### 总结

* 根据需扩容的原因不同（overLoadFactor/tooManyOverflowBuckets），分为两类容量规则方向，**为等量扩容（不改变原有大小）或双倍扩容**
* 新申请的扩容空间（newbuckets/newoverflow）都是预分配，等真正使用的时候才会初始化
* 扩容完毕后（预分配），不会马上就进行迁移。而是采取**增量扩容**的方式，当有**访问到具体 bukcet 时，才会逐渐的进行迁移**（将 oldbucket 迁移到 bucket）



#### Map 实现

```go
// A header for a Go map.
type hmap struct {
    // 元素个数，调用 len(map) 时，直接返回此值
	count     int
	flags     uint8
	// buckets 的对数 log_2
	B         uint8
	// overflow 的 bucket 近似数
	noverflow uint16
	// 计算 key 的哈希的时候会传入哈希函数
	hash0     uint32
    // 指向 buckets 数组，大小为 2^B
    // 如果元素个数为0，就为 nil
	buckets    unsafe.Pointer
	// 等量扩容的时候，buckets 长度和 oldbuckets 相等
	// 双倍扩容的时候，buckets 长度会是 oldbuckets 的两倍
	oldbuckets unsafe.Pointer
	// 指示扩容进度，小于此地址的 buckets 迁移完成
	nevacuate  uintptr
	extra *mapextra // optional fields
}
```

![hashmap bmap](https://golang.design/go-questions/map/assets/0.png)

















#### channel

![img](https://i6448038.github.io/img/channel/hchan.png)

* `buf`是有缓冲的channel所特有的结构，用来存储缓存数据。是个循环链表
* `sendx`和`recvx`用于**记录`buf`**这个循环链表中的~~发送或者接收的~~index
* `lock`是个互斥锁。
* `recvq`和`sendq`分别是接收(<-channel)或者发送(channel <- xxx)的goroutine抽象出来的结构体(sudog)的队列。是个双向链表



##### 创建一个通道

```go
ch := make(chan int, 3)
```

**![img](https://i6448038.github.io/img/channel/hchan1.png)**

* 创建channel实际上就是在**内存中实例化了一个`hchan`的结构体**，并返回一个ch指针，我们使用过程中channel在函数之间的传递都是用的这个指针，这就是为什么函数传递中无需使用channel的指针，而直接用channel就行了，因为**channel本身就是一个指针**。
* channel中发送send(ch <- xxx)和recv(<- ch)接收:
* Q : 如果你想让goroutine以先进先出(FIFO)的方式进入一个结构体中，你会怎么操作？
* A : 加锁！对的！channel就是用了一个锁。hchan本身包含一个互斥锁`mutex`.



###### channel 中队列如何实现的

* channel中有个缓存buf，是用来缓存数据的(假如实例化了带缓存的channel的话)队列。我们先来看看是如何实现“队列”的。
* ![img](https://i6448038.github.io/img/channel/hchan_gif1.png)
* 当使用`send (ch <- xx)`或者`recv ( <-ch)`的时候，首先要锁住`hchan`这个结构体。
* ![img](https://i6448038.github.io/img/channel/hchan_gif2.png)
* 然后开始`send (ch <- xx)`数据。
* ![img](https://i6448038.github.io/img/channel/send.gif)
* 然后是取`recv ( <-ch)`的过程，是个逆向的操作，也是需要加锁。
* 

![img](https://i6448038.github.io/img/channel/recv.gif)





* 注意以上两幅图中`buf`和`recvx`以及`sendx`的变化，`recvx`和`sendx`是根据循环链表`buf`的变动而改变的。
* 为什么channel会使用**循环链表作为缓存结构**，我个人认为是在缓存列表在动态的`send`和`recv`过程中，**定位当前`send`或者`recvx`的位置、选择`send`的和`recvx`的位置比较方便吧**，只要顺着链表顺序一直旋转操作就好。
* 缓存中按链表顺序存放，取数据的时候按链表顺序读取，符合FIFO的原则。



##### send/recv 的细化操作

* 缓存链表中以上每一步的操作，都是需要加锁操作的！

* > - 第一，加锁
  > - 第二，把数据从goroutine中copy到“队列”中(或者从队列中copy到goroutine中）。
  > - 第三，释放锁

* Go中那句经典的话：`Do not communicate by sharing memory; instead, share memory by communicating.`的具体实现就是利用channel把数据从一端copy到了另一端！

* ![img](https://i6448038.github.io/img/channel/hchan_channl.gif)

* Q : channel 满了如何处理？

* A : 当channel缓存满了，或者没有缓存的时候，我们继续send(ch <- xxx)或者recv(<- ch)会阻塞当前goroutine

* Go的goroutine是用户态的线程(`user-space threads`)，**用户态的线程是需要自己去调度的**，Go有运行时的scheduler去帮我们完成调度这件事情。

* ![img](https://i6448038.github.io/img/channel/hchan_block.png)

* ![img](https://i6448038.github.io/img/channel/hchan_block1.png)

* 这个时候G1正在正常运行,当再次进行send操作(ch<-1)的时候，**会主动调用Go的调度器,让G1等待，并从让出M，让其他G去使用**

* ![img](https://i6448038.github.io/img/channel/hchan_block2.png)

* 同时G1也会被抽象成含有G1指针和send元素的`sudog`结构体保存到hchan的`sendq`中等待被唤醒。

* ![img](https://i6448038.github.io/img/channel/hchan_blok3.gif)

* G1什么时候被唤醒呢？这个时候G2隆重登场。(就是从队列中获取数据的 goroutine)

* ![img](https://i6448038.github.io/img/channel/hchan_block4.png)

* G2执行了recv操作`p := <-ch`，于是会发生以下的操作：

* ![img](https://i6448038.github.io/img/channel/hchan_block5.gif)

* G2从缓存队列中取出数据，**channel会将等待队列中的 G1 推出**，将G1当时send的数据推到缓存中，然后调用Go的scheduler，唤醒G1，并把G1放到可运行的Goroutine队列中。

* 









#### Goroutine







#### Golang 的并发控制与通信

##### 全局共享变量

1. 声明一个全局变量；
2. 所有子goroutine共享这个变量，并不断轮询这个变量检查是否有更新；
3. 在主进程中变更该全局变量；
4. 子goroutine检测到全局变量更新，执行相应的逻辑。

```go
package main

import (
    "fmt"
    "time"
)

func main() {
    running := true
    f := func() {
        for running {
            fmt.Println("sub proc running...")
            time.Sleep(1 * time.Second)
        }
        fmt.Println("sub proc exit")
    }
    go f()
    go f()
    go f()
    time.Sleep(2 * time.Second)
    running = false
    time.Sleep(3 * time.Second)
    fmt.Println("main proc exit")
}
```

* **全局变量的优势是简单方便，不需要过多繁杂的操作，通过一个变量就可以控制所有子goroutine的开始和结束；**
* **缺点是功能有限，由于架构所致，该全局变量只能是多读一写，否则会出现数据同步问题，当然也可以通过给全局变量加锁来解决这个问题，但那就增加了复杂度，另外这种方式不适合用于子goroutine间的通信，因为全局变量可以传递的信息很小；**
* **还有就是主进程无法等待所有子goroutine退出，因为这种方式只能是单向通知，所以这种方法只适用于非常简单的逻辑且并发量不太大的场景，一旦逻辑稍微复杂一点，这种方法就有点捉襟见肘。**









##### channel 通信

* CSP 模型由并发执行的实体（线程或者进程）所组成，实体之间通过发送消息进行通信，这里**发送消息时使用的就是通道**，或者叫 channel。

* **CSP 模型的关键是关注 channel**，而不关注发送消息的实体。

* CSP 描述这样一种并发模型：多个Process 使用一个 Channel 进行通信, 这个 Channel 连结的 Process 通常是匿名的，消息传递通常是同步的.

* 这里可以实现优雅等待所有子goroutine完全结束之后主进程才结束退出，借助了标准库**sync里的Waitgroup**，这是一种控制并发的方式，可以实现对多goroutine的等待 : 简单来讲，它的源码里**实现了一个类似计数器的结构**，**记录每一个在它那里注册过的协程，然后每一个协程完成任务之后需要到它那里注销**，然后在**主进程那里可以等待直至所有协程完成任务退出**。

* > * 队列存储数据
  > * 阻塞和唤醒goroutine

* ```go
  type hchan struct {
      qcount   uint           // 队列中数据个数
      dataqsiz uint           // channel 大小
      buf      unsafe.Pointer // 存放数据的环形数组
      elemsize uint16         // channel 中数据类型的大小
      closed   uint32         // 表示 channel 是否关闭
      elemtype *_type // 元素数据类型
      sendx    uint   // send 的数组索引
      recvx    uint   // recv 的数组索引
      recvq    waitq  // 由 recv 行为（也就是 <-ch）阻塞在 channel 上的 goroutine 队列
      sendq    waitq  // 由 send 行为 (也就是 ch<-) 阻塞在 channel 上的 goroutine 队列
  
      // lock protects all fields in hchan, as well as several
      // fields in sudogs blocked on this channel.
      //
      // Do not change another G's status while holding this lock
      // (in particular, do not ready a G), as this can deadlock
      // with stack shrinking.
      lock mutex
  }
  ```

* 从源码可以看出它其实就是一个队列加一个锁（轻量）

* 从上面总结的两个功能点出发，一个是 ring buffer，用于存数据； 一个是存放操作（读写）该channel的goroutine 的队列。

* **buf是一个通用指针，用于存储数据**，看源码时重点关注对这个变量的读写

* recvq 是读操作阻塞在 channel 的 goroutine 列表，sendq 是写操作阻塞在 channel 的 goroutine 列表。列表的实现是 sudog，其实就是一个对 g 的结构的封装，看源码时重点关注，是怎样通过这两个变量阻塞和唤醒goroutine的.

* 







##### Context 包

* 每个Goroutine在执行之前，都要先知道程序当前的执行状态，通常将这些执行状态封装在一个Context变量中，传递给要执行的Goroutine中.
* 在网络编程下，当接收到一个网络请求Request，在处理这个Request的goroutine中，可能需要在当前gorutine继续开启多个新的Goroutine来获取数据与逻辑处理（例如访问数据库、RPC服务等），即一个请求Request，会需要多个Goroutine中处理。
* 





#### 字符串转成 byte 数组，会发生拷贝吗？

* 字符串转成切片， 会产生拷贝。**严格来说，只要是发生类型强转都会发生内存拷贝**。那么问题来了。

* 频繁的内存拷贝操作听起来对性能不大友好。**有没有什么办法可以在字符串转成切片的时候不用发生拷贝呢？**

* ```go
  package main
  
  import (
  	"fmt"
  	"reflect"
  	"unsafe"
  )
  
  func main() {
  	a := "aaa"
  	ssh := *(*reflect.StringHeader)(unsafe.Pointer(&a))
  	b := *(*[]byte)(unsafe.Pointer(&ssh))
  	fmt.Printf("%v\n", b)
  }
  
  ```

* `StringHeader` 是`字符串`在go的底层结构。

* ```go
  type StringHeader struct {
   Data uintptr
   Len  int
  }
  
  ```

* `SliceHeader` 是`切片`在go的底层结构。

* ```go
  type SliceHeader struct {
   Data uintptr
   Len  int
   Cap  int
  }
  ```

* 那么如果想要在底层转换二者，只需要把 `StringHeader` 的地址强转成 `SliceHeader` 就行。那么go有个很强的包叫 `unsafe` 。

* 1.`unsafe.Pointer(&a)`方法可以得到变量`a`的地址。

* 2.`(*reflect.StringHeader)(unsafe.Pointer(&a))` 可以把字符串a转成底层结构的形式。

* 3.`(*[]byte)(unsafe.Pointer(&ssh))` 可以把ssh底层结构体转成byte的切片的指针。

* 4.再通过 `*`转为指针指向的实际内容。



#### 拷贝大切片一定比小切片代价大吗？

* 并不是，所有切片的大小相同；**三个字段**（一个 uintptr，两个int）。切片中的第一个字是指向切片底层数组的指针，这是切片的存储空间，第二个字段是切片的长度，第三个字段是容量。将一个 slice 变量分配给另一个变量**只会复制三个机器字**。所以 **拷贝大切片跟小切片的代价应该是一样的**。

* ```go
  type SliceHeader struct {
   Data uintptr
   Len  int
   Cap  int
  }
  ```

* 





#### uintptr 和 unsafe.Pointer 区别

* unsafe.Pointer只是**单纯的通用指针类型，用于转换不同类型指针**，它不可以参与指针运算；

* 而uintptr是用于指针运算的，GC 不把 uintptr 当指针，也就是说 uintptr 无法持有对象， uintptr 类型的目标会被回收；

* unsafe.Pointer 可以和 普通指针 进行相互转换；

* unsafe.Pointer 可以和 uintptr 进行相互转换。

* ```go
  func main() {
  	var w *W = new(W)
  	fmt.Println(w.b, w.c)
      // 0 0
  	b := unsafe.Pointer(uintptr(unsafe.Pointer(w)) + unsafe.Offsetof(w.b))
  	*((*int)(b)) = 10
      // //现在我们通过指针运算给b变量赋值为10
  	fmt.Println(w.b, w.c)
      // 10 0
  }
  
  // uintptr(unsafe.Pointer(w)) 获取了 w 的指针起始值
  // unsafe.Offsetof(w.b) 获取 b 变量的偏移量
  // 两个相加就得到了 b 的地址值，
  // 将通用指针 Pointer 转换成具体指针 ((*int)(b))，通过 * 符号取值，然后赋值。*((*int)(b)) 相当于把 (*int)(b) 转换成 int 了，最后对变量重新赋值成 10，这样指针运算就完成了。
  ```

* 



#### Json 包变量不加 tag 会变成什么样？

* 如果变量`首字母小写`，则为`private`。无论如何`不能转`，因为取不到`反射信息`。(加不加 tag 都是不能转，效果是一样的)

* 如果变量`首字母大写`，则为`public`。

  * `不加tag`，可以正常转为`json`里的字段，`json`内字段名跟结构体内字段`原名一致`。

  * `加了tag`，从`struct`转`json`的时候，`json`的字段名就是`tag`里的字段名，**原字段名已经没用**。

* ```go
  type J struct {
  	a string // 小写无 tag
  	b string `json:"B"` // 小写 + tag
  	C string // 大写无 tag
  	D string `json:"DD"` // 大写 + tag
  }
  
  func main() {
  	j := J{
  		a: "1",
  		b: "2",
  		C: "3",
  		D: "4",
  	}
  	fmt.Println("没转 Json 之前的情况 : ", j)
  	// 没转 Json 之前的情况 :  {1 2 3 4}
  	jsonInfo, _ := json.Marshal(j)
  	fmt.Println("转 Json 之后的情况 : ", string(jsonInfo))
  	// 转 Json 之后的情况 :  {"C":"3","DD":"4"}
  }
  
  ```

* 





#### new & make 区别

* make **只能**用来分配及初始化类型为slice、map、chan 的数据, 返回的是引用类型，
* new 可以分配任意类型的数据； new 分配返回的是指针，即类型*Type







#### select & switch

* `select`只能应用于channel的操作，既可以用于channel的数据接收，也可以用于channel的数据发送。
* 如果`select`的多个分支都满足条件，则会随机的选取其中一个满足条件的分支
* `switch`可以为各种类型进行分支操作， 设置可以为接口类型进行分支判断(通过i.(type))。
* `switch` 分支是顺序执行的，这和`select`不同。





#### slice 和数组的区别

* **数组是定长的，长度定义好之后，不能再更改**。在 Go 中，数组是不常见的，因为其长度是类型的一部分，限制了它的表达能力，比如 [3]int 和 [4]int 就是不同的类型。
* slice 的底层数据是数组，slice 是对数组的封装，它描述一个数组的片段。两者都可以通过下标来访问单个元素。
* 而**切片则非常灵活，它可以动态地扩容**。切片的类型和长度无关。
* 数组就是一片连续的内存， slice 实际上是一个结构体，包含三个字段：长度、容量、底层数组。
* ![切片数据结构](https://golang.design/go-questions/slice/assets/0.png)
* 底层数组是可以被多个 slice 同时指向的，**因此对一个 slice 的元素进行操作是有可能影响到其他 slice 的**。



#### Slice 扩容

##### golang 1.18 之前

* 当原 slice 容量小于 `1024` 的时候，新 slice 容量变成原来的 `2` 倍；原 slice 容量超过 `1024`，新 slice 容量变成原来的`1.25`倍。

* ```go
  // go 1.9.5 src/runtime/slice.go:82
  func growslice(et *_type, old slice, cap int) slice {
      // ……
      newcap := old.cap
  	doublecap := newcap + newcap
  	if cap > doublecap {
  		newcap = cap
  	} else {
  		if old.len < 1024 {
  			newcap = doublecap
  		} else {
  			for newcap < cap {
  				newcap += newcap / 4
  			}
  		}
  	}
  	// ……
  	
  	capmem = roundupsize(uintptr(newcap) * ptrSize)
  	newcap = int(capmem / ptrSize)
  }
  ```

* 

##### 1.18 之后

* 当原slice容量(oldcap)小于256的时候，新slice(newcap)容量为原来的2倍；原slice容量超过256，新slice容量newcap = oldcap+(oldcap+3*256)/4

* ```go
  // go 1.18 src/runtime/slice.go:178
  func growslice(et *_type, old slice, cap int) slice {
      // ……
      newcap := old.cap
  	doublecap := newcap + newcap
  	if cap > doublecap {
  		newcap = cap
  	} else {
  		const threshold = 256
  		if old.cap < threshold {
  			newcap = doublecap
  		} else {
  			for 0 < newcap && newcap < cap {
                  // Transition from growing 2x for small slices
  				// to growing 1.25x for large slices. This formula
  				// gives a smooth-ish transition between the two.
  				newcap += (newcap + 3*threshold) / 4
  			}
  			if newcap <= 0 {
  				newcap = cap
  			}
  		}
  	}
  	// ……
      
  	capmem = roundupsize(uintptr(newcap) * ptrSize)
  	newcap = int(capmem / ptrSize)
  }
  ```

* 如果只看前半部分，现在网上各种文章里说的 `newcap` 的规律是对的。现实是，**后半部分还对 `newcap` 作了一个`内存对齐`**，这个和内存分配策略相关。进行内存对齐之后，**新 slice 的容量是要 `大于等于` 按照前半部分生成的`newcap`**。

* 之后，向 Go 内存管理器申请内存，**将老 slice 中的数据复制过去**，并且将 append 的元素添加到新的底层数组中。





#### Slice 作为切片参数

* Go 语言的函数参数传递，只有值传递，没有引用传递。

* ```go
  // runtime/slice.go
  type slice struct {
  	array unsafe.Pointer // 元素指针
  	len   int // 长度 
  	cap   int // 容量
  }
  ```

* 当 slice 作为函数参数时，就是一个普通的结构体。其实很好理解：若直接传 slice，在调用者看来，**实参 slice 并不会被函数中的操作改变；若传的是 slice 的指针，在调用者看来，是会被改变原 slice 的**。

* 值得注意的是，不管传的是 slice 还是 slice 指针，如果**改变了 slice 底层数组的数据，会反应到实参 slice 的底层数据**。

* 底层数据在 slice 结构体里是一个指针，尽管 slice 结构体自身不会被改变，也就是说底层数据地址不会被改变。 但是通过指向底层数据的指针，可以改变切片的底层数据，没有问题。

* ```go
  package main
  
  func main() {
  	s := []int{1, 1, 1}
  	f(s)
  	fmt.Println(s)
  }
  
  func f(s []int) {
  	// i只是一个副本，不能改变s中元素的值
  	/*for _, i := range s {
  		i++
  	}
  	*/
  
  	for i := range s {
  		s[i] += 1
  	}
  }
  [2, 2, 2]
  ```

* 要想真的改变外层 `slice`，只有将返回的新的 slice 赋值到原始 slice，或者向函数传递一个指向 slice 的指针。我们再来看一个例子：

* ```go
  package main
  
  import "fmt"
  
  func myAppend(s []int) []int {
  	// 这里 s 虽然改变了，但并不会影响外层函数的 s
  	s = append(s, 100)
  	return s
  }
  
  func myAppendPtr(s *[]int) {
  	// 会改变外层 s 本身
  	*s = append(*s, 100)
  	return
  }
  
  func main() {
  	s := []int{1, 1, 1}
  	newS := myAppend(s)
  
  	fmt.Println(s) // [1 1 1]
  	fmt.Println(newS) // [1 1 1 100]
  
  	s = newS
  
  	myAppendPtr(&s)
  	fmt.Println(s) // [1 1 1 100 100]
  }
  ```

* 



#### Map 常见问题

##### key 为什么无序

* map 在扩容后，会发生 key 的搬迁，**原来落在同一个 bucket 中的 key，搬迁后，有些 key 就要远走高飞了**（bucket 序号加上了 2^B）。
* 而遍历的过程，就是按顺序遍历 bucket，同时按顺序遍历 bucket 中的 key。
* 搬迁后，key 的位置发生了重大的变化，有些 key 飞上高枝，有些 key 则原地不动。这样，遍历 map 的结果就不可能按原来的顺序了。
* 当我们在遍历 map 时，并不是固定地从 0 号 bucket 开始遍历，**每次都是从一个随机值序号的 bucket 开始遍历**，并且是**从这个 bucket 的一个随机序号的 cell 开始遍历**。这样，即使你是一个写死的 map，仅仅只是遍历它，也不太可能会返回一个固定序列的 key/value 对了。
* 





##### float 可以作为 map 的 key 吗

* Go 语言中只要是可比较的类型都可以作为 key。**除开 slice，map，functions 这几种类型**，其他类型都是 OK 的。
* 具体包括：布尔值、数字、字符串、指针、通道、接口类型、结构体、只包含上述类型的数组。这些类型的共同特征是支持 `==` 和 `!=` 操作符，`k1 == k2` 时，可认为 k1 和 k2 是同一个 key。
* **float 型可以作为 key，但是由于精度的问题，会导致一些诡异的问题，慎用之**。







##### 可以边遍历边删除吗

* map 并不是一个线程安全的数据结构。**同时读写一个 map 是未定义的行为，如果被检测到，会直接 panic**。

* 上面说的是发生在多个协程同时读写同一个 map 的情况下。

* 如果在同一个协程内边遍历边删除，并不会检测到同时读写，理论上是可以这样做的。

* 但是，**遍历的结果就可能不会是相同的了**，有可能结果遍历结果集中包含了删除的 key，也有可能不包含，**这取决于删除 key 的时间**：是在遍历到 key 所在的 bucket 时刻前或者后。

* 读之前调用 `RLock()` 函数，读完之后调用 `RUnlock()` 函数解锁；写之前调用 `Lock()` 函数，写完之后，调用 `Unlock()` 解锁。

  另外，`sync.Map` 是线程安全的 map，也可以使用。

* 





##### 可以对 map 取地址吗

* 无法对 map 的 key 或 value 进行取址。以下代码不能通过编译：

* ```go
  package main
  
  import "fmt"
  
  func main() {
  	m := make(map[string]int)
  
  	fmt.Println(&m["qcrao"])
  }
  // 编译报错
  ./main.go:8:14: cannot take the address of m["qcrao"]
  ```

* 







##### 如何比较两个 map 相等

* map 深度相等的条件：

* ```shell
  1、都为 nil
  2、非空、长度相等，指向同一个 map 实体对象
  3、相应的 key 指向的 value “深度”相等
  ```

* 直接将使用 map1 == map2 是错误的。这种写法只能比较 map 是否为 nil。

* 因此只能是遍历map 的每个元素，比较元素是否都是深度相等。

* ```go
  // 不能通过编译
  //fmt.Println(m == n)
  ```

* 



##### map 是线程安全的吗

* map 不是线程安全的。

* 在查找、赋值、遍历、删除的过程中都会检测写标志，一旦发现写标志置位（等于1），**则直接 panic**。赋值和删除函数在检测完写标志是复位之后，先将写标志位置位，才会进行之后的操作。

* 检测写标志：

* ```go
  if h.flags&hashWriting == 0 {
      throw("concurrent map writes")
  }
  ```

* 设置写标志：

* ```go
  h.flags |= hashWriting
  ```

* 







#### Channel 常见问题

##### CSP

* 不要通过共享内存来通信，而要通过通信来实现内存共享。
* Go 的并发原则非常优秀，目标就是简单：**尽量使用 channel；把 goroutine 当作免费的资源，随便用。**
* 



##### channel 底层结构

```go
type hchan struct {
	// chan 里元素数量
	qcount   uint
	// chan 底层循环数组的长度
	dataqsiz uint
	// 指向底层循环数组的指针
	// 只针对有缓冲的 channel
	buf      unsafe.Pointer
	// chan 中元素大小
	elemsize uint16
	// chan 是否被关闭的标志
	closed   uint32
	// chan 中元素类型
	elemtype *_type // element type
	// 已发送元素在循环数组中的索引
	sendx    uint   // send index
	// 已接收元素在循环数组中的索引
	recvx    uint   // receive index
	// 等待接收的 goroutine 队列
	recvq    waitq  // list of recv waiters
	// 等待发送的 goroutine 队列
	sendq    waitq  // list of send waiters

	// 保护 hchan 中所有字段
	lock mutex
}
```

* ![chan data structure](https://golang.design/go-questions/channel/assets/0.png)
* `lock` 用来保证每个读 channel 或写 channel 的操作都是原子的。
* 



##### channel 关闭过程

* close 逻辑比较简单，对于一个 channel，recvq 和 sendq 中分别保存了阻塞的发送者和接收者。(的 goroutinue 协程)
* 关闭 channel 后，对于等待接收者而言，会收到一个相应类型的零值。
* 对于**等待发送者，会直接 panic**。所以，在不了解 channel 还有没有接收者的情况下，不能贸然关闭 channel。
* close 函数先上一把大锁，接着把所有挂在这个 channel 上的 sender 和 receiver 全都连成一个 sudog 链表，再解锁。最后，再将所有的 sudog 全都唤醒。
* 唤醒之后，该干嘛干嘛。**sender 会继续执行 chansend 函数里 goparkunlock 函数之后的代码，很不幸，检测到 channel 已经关闭了，panic。receiver 则比较幸运，进行一些扫尾工作后**，返回。
* 



##### 关闭通道还可以接受数据吗

* ```go
  func main() {
  	ch := make(chan int, 5)
  	ch <- 18
  	close(ch)
  	x, ok := <-ch
  	if ok {
  		fmt.Println("received: ", x)
  	}
  
  	x, ok = <-ch
  	if !ok {
  		fmt.Println("channel closed, data invalid.")
  	}
  }
  received:  18
  channel closed, data invalid.
  ```

* 先创建了一个有缓冲的 channel，向其发送一个元素，然后关闭此 channel。

* 之后两次尝试从 channel 中读取数据，第一次仍然能正常读出值。**第二次返回的 ok 为 false，说明 channel 已关闭，且通道里没有数据。**





##### 操作 channel 可能出现的情况

* ![image-20221101175032805](/home/zhoubing/.config/Typora/typora-user-images/image-20221101175032805.png)





##### 如何优雅的关闭 channel

###### 关闭通道可能的问题

* 在不改变 channel 自身状态的情况下，无法获知一个 channel 是否关闭。
* 关闭一个 closed channel 会导致 panic。所以，如果关闭 channel 的一方在**不知道 channel 是否处于关闭状态**时就去贸然关闭 channel 是很危险的事情。
* 向一个 closed channel 发送数据会导致 panic。所以，**如果向 channel 发送数据的一方不知道 channel 是否处于关闭状态时就去贸然向 channel 发送数据是很危险的事情。**

###### 不优雅的关闭方式

* 使用 defer-recover 机制，放心大胆地关闭 channel 或者向 channel 发送数据。**即使发生了 panic，有 defer-recover 在兜底。**

* 使用 sync.Once 来保证只关闭一次。



###### 优雅的方式

1. 一个 sender，一个 receiver
2. 一个 sender， M 个 receiver
3. N 个 sender，一个 reciver
4. N 个 sender， M 个 receiver

* 对于 1，2，只有一个 sender 的情况就不用说了，**直接从 sender 端关闭就好了**，没有问题。
* 第 3 种情形下，优雅关闭 channel 的方法是：the only receiver says “please stop sending more” by closing an additional signal channel。
* **解决方案就是增加一个传递关闭信号的 channe**l，receiver 通过信号 channel 下达关闭数据 channel 指令。senders 监听到关闭信号后，停止发送数据。
* 在 Go 语言中，对于一个 channel，如果**最终没有任何 goroutine 引用它，不管 channel 有没有被关闭，最终都会被 gc 回收。所以，在这种情形下，所谓的优雅地关闭 channel 就是不关闭 channel，让 gc 代劳。**
* **don’t close a channel from the receiver side and don’t close a channel if the channel has multiple concurrent senders.**







##### Channel 发送和接受元素的本质是什么

* 就是说 **channel 的发送和接收操作本质上都是 “值的拷贝”**，无论是从 sender goroutine 的栈到 chan buf，还是从 chan buf 到 receiver goroutine，或者是直接从 sender goroutine 到 receiver goroutine。

* ```go
  type user struct {
  	name string
  	age int8
  }
  
  var u = user{name: "Ankur", age: 25}
  var g = &u
  
  func modifyUser(pu *user) {
  	fmt.Println("modifyUser Received Vaule", pu)
  	pu.name = "Anand"
  }
  
  func printUser(u <-chan *user) {
  	time.Sleep(2 * time.Second)
  	fmt.Println("printUser goRoutine called", <-u)
  }
  
  func main() {
  	c := make(chan *user, 5)
  	c <- g  // 只是值拷贝，而不是指针的拷贝
  	fmt.Println(g) // &{Ankur 25}
  	// modify g
  	g = &user{name: "Ankur Anand", age: 100}
  	go printUser(c) // printUser goRoutine called &{Ankur 25}
  	go modifyUser(g) // modifyUser Received Vaule &{Ankur Anand 100}
  	time.Sleep(5 * time.Second)
  	fmt.Println(g) // &{Anand 100}
  }
  ```

* 这里并不是将指针 g “发送” 到了 channel 里，只是拷贝它的值而已。



##### Channel 在什么情况下内存泄露

* 泄漏的原因是 goroutine 操作 channel 后，**处于发送或接收阻塞状态，而 channel 处于满或空的状态，一直得不到改变。**同时，垃圾回收器也不会回收此类资源，进而导致 gouroutine 会一直处于等待队列中，不见天日。
* 程序运行过程中，对于一个 channel，如果没有任何 goroutine 引用了，gc 会对其进行回收操作，不会引起内存泄漏。
* 



##### Channel 的应用

###### 控制并发数

* 有时需要定时执行几百个任务，例如每天定时按城市来执行一些离线计算的任务。但是并发数又不能太高，因为任务执行过程依赖第三方的一些资源，对请求的速率有限制。这时就可以通过 channel 来控制并发数。

* ```go
  var limit = make(chan int, 3)
  
  func main() {
      // …………
      for _, w := range work {
          go func() {
              limit <- 1
              w()
              <-limit
          }()
      }
      // …………
  }
  ```

* 构建一个缓冲型的 channel，容量为 3。接着遍历任务列表，每个任务启动一个 goroutine 去完成。

* 真正执行任务，访问第三方的动作在 w() 中完成，在执行 w() 之前，先要从 limit 中拿“许可证”，拿到许可证之后，才能执行 w()，并且在执行完任务，要将“许可证”归还。**这样就可以控制同时运行的 goroutine 数**。

* 还有一点要注意的是，如果 w() 发生 panic，那“许可证”可能就还不回去了，因此**需要使用 defer 来保证**。

* 



#### 调度器相关知识





































### JAVA

#### 垃圾回收





























































### C++

#### 迭代器失效如何处理

##### 序列式容器 （vector deque）

* 迭代器的失效问题：**对容器的操作影响了元素的存放位置**，称为迭代器失效。

* 失效情况：

* > * 当容器调用`erase()`方法后，当前位置到容器末尾元素的所有迭代器全部失效。
  > * 当容器调用`insert()`方法后，当前位置到容器末尾元素的所有迭代器全部失效。
  > * 如果容器扩容，在其他地方重新又开辟了一块内存。原来容器底层的内存上所保存的迭代器全都失效了。

* ```cpp
  #include<iostream>
  #include<vector>
  
  using namespace std;
  
  int main() {
  
  	vector<int> q{ 1,2,3,4,5,6 };
  	// 在这里想把大于2的元素都删除
  	for (auto it = q.begin(); it != q.end(); it++) {
  		if (*it > 2)
  			q.erase(it); // 这里就会发生迭代器失效
  	}
  	// 打印结果
  	for (auto it = q.begin(); it != q.end(); it++) {
  		cout << *it << " ";
          // 1 2 4 6
  	}
  	cout << endl;
  
  	return 0;
  }
  
  ```

* 迭代器失效的原因是：因为 vetor、deque 使用了连续分配的内存，`erase`操作**删除一个元素导致后面所有的元素都会向前移动一个位置，这些元素的地址发生了变化**，所以当前位置到容器末尾元素的所有迭代器全部失效。

* 解决方法是**利用`erase`方法可以返回下一个有效的 iterator**: 

* ```cpp
  // 在这里想把大于2的元素都删除
  for(auto it=q.begin();it!=q.end();)
  {
      if(*it>2)
      {
      	it=q.erase(it); // 这里会返回指向下一个元素的迭代器，因此不需要再自加了
      }
      else
      {
      	it++;
      }
  }
  
  ```

* 

##### 链表式容器

* 对于链表式容器(如 list)，删除当前的 iterator，仅仅会使当前的 iterator 失效，这是因为 list 之类的容器，**使用了链表来实现，插入、删除一个结点不会对其他结点造成影响。**只要在 erase 时，**递增当前 iterator 即可**，并且 erase 方法可以返回下一个有效的 iterator。

* 解法一： 递增当前的 iterator

* ```cpp
  for (iter = cont.begin(); it != cont.end();)
  {
     (*iter)->doSomething();
     if (shouldDelete(*iter))
        cont.erase(iter++);  // 直接递增当前的 iter
     else
        iter++;
  }
  
  ```

* 解法二： 通过 erase 获取下一个有效的 iterator

* ```cpp
  for (iter = cont.begin(); iter != cont.end();)
  {
     (*it)->doSomething();
     if (shouldDelete(*iter))
        iter = cont.erase(iter);  //erase删除元素，返回下一个迭代器
     else
        ++iter;
  }
  ```

* 





##### 关联式容器

* 对于关联容器(如 map, set,multimap,multiset)，删除当前的 iterator，仅仅会使当前的 iterator 失效，只要在 erase 时，递增当前 iterator 即可。这是因为 map 之类的容器，使用了红黑树来实现，插入、删除一个结点不会对其他结点造成影响。erase 迭代器只是被删元素的迭代器失效，但是返回值为 void，所以要采用`erase(iter++)`的方式删除迭代器。

* `dataMap.erase(iter)`之后，iter 就已经失效了，所以 iter 无法自增，即`iter++`就会出bug。解决方案，就是在 iter 失效之前，先自增。

* ```cpp
  void mapTest()
  {
      map<int, string> dataMap;
  
  
      for (int i = 0; i < 100; i++)
      {
             string strValue = "Hello, World";
  
              stringstream ss;
              ss<<i;
              string tmpStrCount;
              ss>>tmpStrCount;
              strValue += tmpStrCount;
              dataMap.insert(make_pair(i, strValue));
      }
  
      cout<<"MAP元素内容为："<<endl;
      map<int, string>::iterator iter;
      for (iter = dataMap.begin(); iter != dataMap.end(); iter++)
      {
              int nKey = iter->first;
              string strValue = iter->second;
              cout<<strValue<<endl;
      }
  
      cout<<"内容开始删除："<<endl;
      for (iter = dataMap.begin(); iter != dataMap.end();)
      {
              int nKey = iter->first;
              string strValue = iter->second;
  
             if (nKey % 2 == 0)
             {
                  dataMap.erase(iter++);
                  auto a = iter;
  
             }
             else {
                 iter ++;
             }
      }
  }
  
  ```

* 









#### volatile

* volatile 关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素更改。比如：操作系统、硬件或者其它线程等。遇到这个关键字声明的变量，**编译器对访问该变量的代码就不再进行优化，从而可以提供对特殊地址的稳定访问**。

* 当要求使用 volatile 声明的变量的值的时候，**系统总是重新从它所在的内存读取数据**，即使它前面的指令刚刚从该处读取过数据。例如：

* ```cpp
  volatile int i=10;
  int a = i;
  int b = i; 
  ```

* volatile 指出 i 是随时可能发生变化的，每次使用它的时候必须从 i的地址中读取，因而**编译器生成的汇编代码会重新从i的地址读取数据放在 b 中**。

* 优化做法是，由于编译器发现两次从 i读数据的代码之间的代码没有对 i 进行过操作，它会自动把上次读的数据放在 b 中。而不是重新从 i 内存里面读。

* 这样以来，**如果 i是一个寄存器变量或者表示一个端口数据就容易出错**，所以说 **volatile 可以保证对特殊地址的稳定访问**。

* ```cpp
  #include <stdio.h>
  void main()
  {
      int i = 10;
      int a = i;
  
      printf("i = %d", a);
  
      // 下面汇编语句的作用就是改变内存中 i 的值
      // 但是又不让编译器知道
      __asm {
          mov dword ptr [ebp-4], 20h
      }
  
      int b = i;
      printf("i = %d", b);
  }
  ```

* 然后，在 Debug 版本模式运行程序，输出结果如下：

* ```cpp
  i = 10
  i = 32
  ```

* 然后，在 Release 版本模式运行程序，输出结果如下：

* ```cpp
  i = 10
  i = 10
  ```

* 输出的结果明显表明，Release 模式下，编译器对代码进行了优化，第二次没有输出正确的 i 值。下面，我们把 i 的声明加上 volatile 关键字，看看有什么变化：

* ```cpp
  #include <stdio.h>
  void main()
  {
      volatile int i = 10;
      int a = i;
  
      printf("i = %d", a);
  
      // 下面汇编语句的作用就是改变内存中 i 的值
      // 但是又不让编译器知道
      __asm {
          mov dword ptr [ebp-4], 20h
      }
  
      int b = i;
      printf("i = %d", b);
  }
  ```

* 分别在 Debug 和 Release 版本运行程序，输出都是：

* ```cpp
  i = 10
  i = 32
  ```

* volatile用在如下的几个地方：

* > * **中断服务程序中修改的供其它程序检测的变量需要加volatile；**
  > * **多任务环境下各任务间共享的标志应该加volatile；**
  > * **存储器映射的硬件寄存器通常也要加volatile说明，因为每次对它的读写都可能由不同意义；**



##### 多线程下的 volatile

* 有些变量是用volatile关键字声明的。当两个线程都要用到某一个变量且该变量的值会被改变时，应该用volatile声明，该关键字的作用是**防止优化编译器把变量从内存装入CPU寄存器中。**

* 如果变量被装入寄存器，那么两个线程有可能一个使用内存中的变量，一个使用寄存器中的变量，这会造成程序的错误执行。

* volatile的意思是**让编译器每次操作该变量时一定要从内存中真正取出，而不是使用已经存在寄存器中的值，**如下：

* volatile BOOL bStop = FALSE;

* (1)在一个线程中：

* ```cpp
  while( !bStop ) { ... } 
  bStop = FALSE; 
  return; 
  ```

* (2)在另外一个线程中，要终止上面的线程循环：

* ```cpp
  bStop  =  TRUE;  
  while(  bStop  ); 
  ```

* 等待上面的线程终止，如果bStop不使用volatile声明，那么这个循环将是一个死循环，因为bStop已经读取到了寄存器中，寄存器中bStop的值永远不会变成FALSE，加上volatile，程序在执行时，**每次均从内存中读出bStop的值，就不会死循环了。**

* 这个关键字是用来设定某个对象的存储位置在内存中，而不是寄存器中。**因为一般的对象编译器可能会将其的拷贝放在寄存器中用以加快指令的执行速度**，例如下段代码中：

* ```cpp
  ... 
  int nMyCounter = 0; 
  for(; nMyCounter<100;nMyCounter++) 
  { 
  ... 
  } 
  ... 
  ```

* nMyCounter的拷贝可能存放到某个寄存器中

* volatile 只在三种场合下是合适的。

* 1 和信号处理（signal handler）相关的场合；
  2 和内存映射硬件（memory mapped hardware）相关的场合；
  3 和非本地跳转（setjmp 和 longjmp）相关的场合。

* **volatile 关键字唯一的应用场景是与外部硬件交流的时候，比如 DMA 或 MMIO**

* 















#### GDB 调试指南

* **Coredump（核心存储）是进程异常终止或崩溃时的内存快照.**操作系统会在程序发生异常而异常在进程内部又没有被捕获的情况下，会把进程此刻内存、[寄存器](https://www.zhihu.com/search?q=寄存器&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2663700699})状态、堆栈指针、内存管理信息以及函数调用堆栈等信息转储保存在一个文件里（Corefile）.
* 键入 bt （backtrace）打印函数调用栈，第一行即为发生 Core 的最后调用处。
* 





#### 定义和声明

* 声明是告诉编译器“**这个函数或者变量可以在哪找到，它的模样像什么**”。而定义则是告诉编译器，“**在这里建立变量或函数”，并且为它们分配内存空间。**

  

##### 声明

```cpp
int Add(int, int);
```

* 变量的声明如：extern int i; 
* 在变量定义前**加extern关键字表示声明一个变量但不定义它**，这对函数同样有效，如：extern int Add(int a, int b);因为没有函数体，编译器必会把它视作声明而不是定义，extern关键字对于函数来说是多余的，可选的。
* extern 声明不是定义，不会为其分配内存空间



##### 定义

```cpp
int Add(int, int) {}
```

* int i; **如果在此之前没有对 i 的声明，那么这里既是对它的声明也是对它的定义**，编译器会为其分配对应的内存。

* **注意：**如果使用extern关键字时，对变量进行了初始化，那就是定义。

* ```cpp
  extern int b = 20;  //是定义
  ```



##### 定义等同声明

* extern 声明不是定义，定义就是声明，声明不会分配内存空间

* ```cpp
  int a = 10;    //定义就是声明
  extern int b;  //声明，不是定义
  ```



##### 仅仅是定义

```cpp
1:  在类定义之外，定义并初始化一个静态数据成员。如 int A::a = 0;
2:  在类外定义非内联成员函数。
```



##### 声明仅仅是声明

```cpp
1:  仅仅提供函数原型：void display();
2:  extern int a;
3:  class A;
4:  typedef 声明;
5:  在类中定义的静态数据成员的声明
```













#### 字符集

* 常见字符集名称：**ASCII字符集**、GB2312字符集、BIG5字符集、 GB18030字符集、**Unicode字符集**等。

* Unicode: 它的名字叫统一码， 也叫万国码，符号数量在不断增加，已超百万 。(这是一种所有符号的编码, 每一个符号都给予一个独一无二的编码)

* unicode字符存储的多种实现方式，比如**UTF-8,UTF-16**

* **UTF-16** 的 16 指的就是最小为 16 位一个单元，也即**两字节为一个单元**。早期，UTF16采用固定长度2字节的方式编码，两个字节可以表示65536种符号（其实真正能表示要比这个少）

* UTF-8使用1~4字节为每个字符编码：

* 其他语言的字符（包括中日韩文字、东南亚文字、中东文字等）包含了大部分常用字，使用3字节编码。

* UTF-8是一种编码格式，一个字节包含8个比特。

* 上述编码是单字节编码， 也就是只有8个比特。**如果想匹配多于256个字符的语言，一个字节显然不够，用两个字节的话，16比特，可以编码65536个字符**，BIG-5就是一个双字节编码方式，它包括大多数中文繁体字，[GB18030](https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/GB18030) 则包括繁体和简体。比如：

* ![img](https://pic3.zhimg.com/80/v2-5f4e506607e895e0fd3212c2c53a5246_720w.jpg)

* 对于一些语言单字节编码不够，所以采用双字节，双字节也不够的时候可以采用三字节，甚至四字节，字节是不是越多越好呢？并不是，**因为字节用的越多，那些用单字节就能表示的字符会增加很多个0**， 浪费很多容量。比如 A 可能就是00000000 00000000 00000000 01000001， 这样就没有必要了。

* **因为Unicode其实不是一种编码， 而是定义了一个表， 表中为世界上每种语言中的每个字符设定了统一并且唯一的码位** （code point），以满足跨语言、跨平台进行文本转换的要求。

* 如果是汉语，**在UTF-8 中三个字节才能代表一个字符**。如果我们同样使用截取[]:

* ```python
  >>> string = '汉字'
  >>> string[0]
  '\xe6'
  ```

* **只会给我们返回一个「汉」这个字的第一个字节， 也就是11100110**， 但是「汉」需要用11100110 10111100 10100010 才能表示。那我们要怎么才能截取汉字的第一个字符呢？

* ```python
  >>> string_u = string.decode('UTF-8')
  >>> string_u[0]
  u'\u6c49'
  >>> print(string_u[0].encode('UTF-8'))
  汉
  ```

* 将「汉字」解码到Unicode, 这时再截取第一个字符就是一个 u 开头的Unicode了，再用UTF-8 编码， 返回的就是「汉」 这个字符了。









#### Segmentation fault 原因

* Segmentation fault (core dumped)**多为内存不当操作造成**。

* 空指针、野指针的读写操作，数组越界访问，破坏常量等。

* ```cpp
  int main() {
      int array[5] = {1, 2, 3, 4, 5};
      for (int i= 0; ; i++) {
          cout << i << " " << array[i] << endl;
      }
      return 0;
  }
  // 对于前五个元素，打印结果是正常的，正如我们初始化的一样，array[i] 为 0 .
  // 对于第五个之后的元素，一般来说，会打印出一个不确定的整数，直到在打印某个元素的时候，出现了 “segmentation fault”.
  
  ```

* 可以看到，这里 i 的值已经相当大了，所以这里需要强调的就是，**数组越界不是必然导致段错误，有时候只是访问到了一些脏数据**，这会导致重现该段错误的重现可能会很困难，配置好环境以便在出现段错误的时候能够保存有检查错误用的 core dump 文件非常重要。

* 多线程同时访问的全局数据，应该注意加锁保护，否则很容易造成core dump。

* 对每个指针声明后进行**初始化为NULL**是避免这个问题的好办法。排除此问题的最好办法则是**调试**。

* **SIGSEGV是当一个进程执行了一个无效的内存引用**，或发生段错误时发送给它的信号

* eg : 

* ```cpp
  int main() {
      char* ch2 = "abcdefg";
      ch2[5] = 'h'; // 破坏常量,因为不能修改内部的内容
      char* p = nullptr;
      *p = 'a'; // 给空指针赋值,对空指针的写操作
      ch2 = "hello"; // 这个就可以，因为指针重新指向别的对象，所以不会出错
      printf("%s\n", ch2);
      return 0;
  }
  ```

* 









#### C++ 异常处理

* noexcept C++11关键字, 告诉编译器，**函数中不会发生异常,有利于编译器对程序做更多的优化**
* C++中的异常处理是在**运行时**而不是编译时检测的。为了实现运行时检测，**编译器创建额外的代码**，然而这会妨碍程序优化
* 如果确定代码不会出现异常，那么采用这种 noexpect 形式，告诉编译器不会出现异常，不需要生成额外的代码。
* 











#### c++ 基础知识

* c++ **不允许对两个基本类型元素，进行运算符重载操作**， eg : 两个 const char[] 不能相加操作， 两个 char 字符不能相加， 至少存在一个 string 就可以相加了.

* ```cpp
  int main() {
      auto n = 1000000;
      string s;
      for (int i = 0; i < n; i++) {
          s = s + "a"; // O(n^2) 时间复杂度, 因为这个生成的 s + "a" 需要拷贝一遍， 非常的耗时, 之后还需要 拷贝一遍给 s,
          // c++11 之后使用的是移动拷贝赋值操作，但是之前还是做了一次拷贝还是非常的慢
          s += "a"; // O(n) 时间复杂度， 这个类似 push_back 不需要拷贝操作, 推荐
      }
      return 0;
  }
  ```

* STL 所有容器都是**值语义**，不是指针语义，所以对一个容器修改，不会影响另外一个容器内部的数据.

#### 继承

继承的核心：

> * 每一个子类都拥有一个父类的对象
> * 父类的实现，子类不能干预，不能指手画脚（不能在子类中初始化父类的成员变量/ 修改父类的成员变量）



#### 尽量使用初始化列表

```cpp
class CExample {
public:
    int a;
    float b;
    //构造函数初始化列表
    CExample(): a(0),b(8.8)
    {}
    //构造函数内部赋值
    CExample()
    {
        a=0;
        b=8.8;
    }
};
```

* 什么时候只可以使用初始化列表不能使用内部赋值： 

* > * 1.成员类型是**没有默认构造函数的类**。若没有提供显示初始化式，则编译器隐式使用成员类型的默认构造函数，若类没有默认构造函数，则编译器尝试使用默认构造函数将会失败。
  > * 2.**const 成员**或**引用类型**的成员。因为 **const 对象或引用类型只能初始化**，不能对他们赋值。

* 初始化列表的性能优势：

* > * 1.**内置数据类型，复合类型（指针，引用）**- 在成员初始化列表和构造函数体内进行，在性能和结果上都是一样的
  > * 2.**用户定义类型（类类型）**- 结果上相同，但是性能上存在很大的差别。因为**类类型的数据成员对象在进入函数体前已经构造完成**，也就是说**在成员初始化列表处进行构造对象的工作，调用构造函数**，在进入函数体之后，进行的是对已经构造好的类对象的赋值，又调用个拷贝赋值操作符才能完成（如果并未提供，则使用编译器提供的默认按成员赋值行为）
  > * 对于**自定义类型**，**一定会在初始化列表处调用一次**, 即使**我们没有自己写初始化列表编译器也会自己调用一次初始化**。所以说如果我们都在初始化列表对变量进行初始化，这样就会更高效一些。



#### Swap 使用 std::move  OR not

##### 不使用 std::move

```cpp
template<class T>
void swap(T& a, T& b){
    T tmp = a; //调用复制构造函数
    a = b; //调用operatpr=
    b = tmp; //调用operatpr=
}
```

* 则上述转换中会调用一次复制构造函数，两次赋值运算符重载

```cpp
class vector{
private:
    int *_arr;
    int _size;
public:
    //仅考虑复制构造函数和赋值运算符重载
    vector(const vector& _right){
        _size = _right._size;
        _arr = new int[_size];
        for(int i = 0; i < _size; i++){
            _arr[i] = _right._arr[i]; //该步骤需要重复_size次
        }
    }

    vector& operator=(const vector& _right){
        if(this != &_right){
            if(_arr != NULL)
                delete[] _arr;
            _size = _right._size;
            _arr = new int[_size];
            for(int i = 0; i < _size; i++){
                _arr[i] = _right._arr[i]; //该步骤需要重复_size次
            }
        }
    }

    //析构函数
    ~vector(){
        if(_arr != NULL)
            delete[] _arr;
    }
};
```

* 复制构造函数和赋值运算符重载函数的**复杂度均为O(_size)**，因此swap函数的复杂度亦为O(_size)。因此，这并不是一个高效的交换方法。



##### 使用 std::move

```cpp
template<class T>
void swap(T& var1, T& var2){
    T tmp = std::move(var1); //调用移动构造函数
    var1 = std::move(var2); //调用operator=(T&&)
    var2 = std::move(tmp); //调用operator=(T&&)
}
```

* swap中三次调用的均为move类函数

```cpp
//以下函数均在vector类中
//移动（move）构造函数
vector(vector&& _right){
    _size = _right._size;
    _arr = _right._arr; //由于知道_right引用的是临时变量，不会被继续使用，因此可以把_right._arr直接拿过来（即移动）
    _right._arr = NULL; //这个语句是表示_right对_arr已经失去所有权，防止_right在析构时销毁_arr。
}
//移动赋值运算符
vector& operator=(vector&& _right){
    if(this != &_right){
        if(_arr != NULL)
            delete[] _arr;
        _size = _right._size;
        _arr = _right._arr; //同样地，知道_right是右值引用，就可以直接把_right._arr拿过来。
        _right._arr = NULL;
    }
}
```

* 与第1节的复制构造函数和赋值运算符相比，移动构造函数和移动赋值运算符实现了右值中一些动态对象的“废物利用“，**无需进行复制操作，从而可以极大地提高效率。**
* **时间复杂度均为O(1)，因此swap的整体时间复杂度为O(1)**。





#### 预处理阶段优化

* 预处理是源码级别的替换，效率非常的高

* **预处理指令都以符号“#”开头**，这个你应该很熟悉了。但同时你也应该意识到，虽然都在一个源文件里，但它不属于 C++ 语言，它走的是预处理器，**不受 C++ 语法规则的约束。**

* 预处理编程也就不用太遵守 C++ 代码的风格。一般来说，预处理指令不应该受 C++ 代码缩进层次的影响，不管是在函数、类里，还是在 if、for 等语句里，永远是**顶格写**。

* 先来说说最常用的预处理指令“#include”，它的作用是“包含文件”。注意，不是“包含头文件”，而是可以包含任意的文件。(包括图片音频视频等)

* ```cpp
  #include "a.out"      // 完全合法的预处理包含指令，你可以试试,      #include”其实是非常“弱”的,不做什么检查
  ```

* 非常大的数组，另存为一个“*.inc”文件，然后再用“#include”替换原来的大批数字。这样就节省了大量的空间，让代码更加整洁。

* ```cpp
  
  static uint32_t  calc_table[] = {
  #  include "calc_values.inc"        // 非常大的一个数组，细节被隐藏
  };
  ```

* 宏是没有作用域概念的，永远是全局生效。所以，对于一些用来简化代码、起临时作用的宏，最好是用完后尽快用“#undef”取消定义，避免冲突的风险。像下面这样：

* ```cpp
  
  #define CUBE(a) (a) * (a) * (a)  // 定义一个简单的求立方的宏
  
  cout << CUBE(10) << endl;        // 使用宏简化代码
  cout << CUBE(15) << endl;        // 使用宏简化代码
  
  #undef CUBE                      // 使用完毕后立即取消定义
  ```

* <img src="https://static001.geekbang.org/resource/image/b8/11/b8b819b1331736ebc40664cd878f7511.jpg?wh=2284*1973" alt="img" style="zoom:50%;" />

  

##### extern "C" 

* **extern C** 的本质原因是**C不支持函数重载**。**混合编程链接时候不兼容**，会报找不到函数的错误。**C++因为支持重载，编译会将所有函数重命名**。C调用C++函数实际是调用重命名后的。(C++ 程序保持和 C 兼容)
* 这个是为了兼容C语言，**因为C++编译生成的链接符号与C不一样**，用这个就会导出与C一样规则的符号，方便外部库调用。



##### define & inline 区别

* > * **处理阶段：**宏定义define在预处理阶段就换成了字符串的替换，而inline在编译阶段进行。
  > * **类型安全检查：**宏定义define是简单的字符串替换，不存在类型安全检查，而inline函数还是一个函数，编译器会进行类型安全检查(词法语法的分析），因此inline更加安全。
  > * **替换方式：**宏定义define只是单纯的字符串替换，而inline是代码嵌入，也就是说**编译器在函数调用的地方直接将inline函数代码写进去，这样就不会产生函数的调用跳转（无栈帧消耗）**，因此适用于短小的函数，并且安全可靠。
  > * **使用方式：**宏定义define只要定义了就会替换，而inline只是建议，**编译器可以拒绝替换**，在**函数较大的时候**，编译器可以选择不展开相应的函数。
  > * **inline关键字必须在函数定义处，位于函数声明处则无效**，因此一般将公共的inline函数的定义写在头文件中。（注意事项）



#### const_cast  static_cast  dynamic_cast reinterpret_cast

##### const_cast

* 1、**常量指针被转化成非常量的指针**，并且仍然指向原来的对象；
  2、**常量引用被转换成非常量的引用**，并且仍然指向原来的对象；
  3、const_cast一般用于修改指针。如const char *p形式。

* ```cpp
  #include<iostream>
  int main() {
      // 原始数组
      int ary[4] = { 1,2,3,4 };
  
      // 打印数据
      for (int i = 0; i < 4; i++)
          std::cout << ary[i] << "\t";
      std::cout << std::endl;
  
      // 常量化数组指针
      const int*c_ptr = ary;
      //c_ptr[1] = 233;   //error
  
      // 通过const_cast<Ty> 去常量
      int *ptr = const_cast<int*>(c_ptr);
  
      // 修改数据
      for (int i = 0; i < 4; i++)
          ptr[i] += 1;    //pass
  
      // 打印修改后的数据
      for (int i = 0; i < 4; i++)
          std::cout << ary[i] << "\t";
      std::cout << std::endl;
  
      return 0;
  }
  
  /*  out print
      1   2   3   4
      2   3   4   5
  */
  ```

* 

##### static_cast

* **static_cast 作用和C语言风格强制转换的效果基本一样**，由于**没有运行时类型检查来保证转换的安全性**，所以这类型的强制转换和C语言风格的强制转换都有安全隐患。

* 用于类层次结构中基类（父类）和派生类（子类）之间指针或引用的转换。注意：进行**上行转换（把派生类的指针或引用转换成基类表示）是安全的**；进行下行转换（把基类指针或引用转换成派生类表示）时，由于**没有动态类型检查，所以是不安全的**。

* 用于基本数据类型之间的转换，如把int转换成char，把int转换成enum。这种转换的安全性需要开发者来维护。

* static_cast不能转换掉原有类型的const、volatile、或者 __unaligned属性。(前两种可以使用const_cast 来去除)

* 在c++ primer 中说道：**c++ 的任何的隐式转换都是使用 static_cast 来实现**。

* ```cpp
  /* 常规的使用方法 */
  float f_pi=3.141592f
  int   i_pi=static_cast<int>(f_pi); /// i_pi 的值为 3
  
  /* class 的上下行转换 */
  class Base{
      // something
  };
  class Sub:public Base{
      // something
  }
  
  //  上行 Sub -> Base
  //编译通过，安全
  Sub sub;
  Base *base_ptr = static_cast<Base*>(&sub);  
  
  //  下行 Base -> Sub
  //编译通过，不安全
  Base base;
  Sub *sub_ptr = static_cast<Sub*>(&base);    
  ```

* 

##### dynamic_cast

* 用于动态类型转换。**只能用于含有虚函数的类**，用于类层次间的向上和向下转化。只能转指针或引用。向下转化时，如果是非法的对于指针返回NULL，对于引用抛异常。

* 向上转换：指的是子类向基类的转换

* 向下转换：指的是基类向子类的转换

* 它通过判断在执行到该语句的时候变量的运行时类型和要转换的类型是否相同来判断是否能够进行向下转换。

* ```cpp
  #include<iostream>
  using namespace std;
  
  class Base{
  public:
      Base() {}
      ~Base() {}
      void print() {
          std::cout << "I'm Base" << endl;
      }
  
      virtual void i_am_virtual_foo() {}
  };
  
  class Sub: public Base{
  public:
      Sub() {}
      ~Sub() {}
      void print() {
          std::cout << "I'm Sub" << endl;
      }
  
      virtual void i_am_virtual_foo() {}
  };
  int main() {
      cout << "Sub->Base" << endl;
      Sub * sub = new Sub();
      sub->print();
      Base* sub2base = dynamic_cast<Base*>(sub);
      if (sub2base != nullptr) {
          sub2base->print();
      }
      cout << "<sub->base> sub2base val is: " << sub2base << endl;
  
  
      cout << endl << "Base->Sub" << endl;
      Base *base = new Base();
      base->print();
      Sub  *base2sub = dynamic_cast<Sub*>(base);
      if (base2sub != nullptr) {
          base2sub->print();
      }
      cout <<"<base->sub> base2sub val is: "<< base2sub << endl;
  
      delete sub;
      delete base;
      return 0;
  }
  /* vs2017 输出为下
  Sub->Base
  I'm Sub
  I'm Base
  <sub->base> sub2base val is: 00B9E080   // 注:这个地址是系统分配的,每次不一定一样
  
  Base->Sub
  I'm Base
  <base->sub> base2sub val is: 00000000   // VS2017的C++编译器,对此类错误的转换赋值为nullptr
  */
  ```

* 



##### reinterpret_cast

* **几乎什么都可以转**，比如将int转指针，可能会出问题，尽量少用；
* 





#### 编译阶段

* 断言 assert

* assert 虽然是一个宏，但在预处理阶段不生效，而是在**运行阶段才起作用**，所以又叫“动态断言”。

* “static_assert”，不过它是一个专门的关键字，而不是宏。因为它只**在编译时生效，运行阶段看不见**，所以是“静态”的。

* 要想保证我们的程序只在 64 位系统上运行，可以用静态断言在编译阶段检查 long 的大小，必须是 8 个字节（当然，你也可以换个思路用预处理编程来实现）。

* ```cpp
  
  static_assert(
    sizeof(long) >= 8, "must run on x64");
    
  static_assert(
    sizeof(int)  == 4, "int must be 32bit");
  ```

* static_assert 运行在**编译阶段**，只能看到编译时的常数和类型，看不到**运行时的变量、指针、内存数据**等.

* ```cpp
  char* p = nullptr; // 在运行阶段才会出现，编译阶段不会出现
  static_assert(p == nullptr, "some error.");  // 错误用法
  ```

* <img src="https://static001.geekbang.org/resource/image/25/39/25232468a72b55a41bf7af90583ae239.jpg?wh=2284*3130" alt="img" style="zoom:33%;" />

* Q: 预处理阶段可以自定义宏，但编译阶段不能自定义属性标签，这是为什么呢？

* A : 因为属性标签都在编译器里内置，自定义的属性标签编译器无法识别。





#### 面向对象设计

```cpp

class Interface        // 接口类定义，没有final，可以被继承
{ ... };           

class Implement final : // 实现类，final禁止再被继承
      public Interface    // 只用public继承
{ ... };
```

* 构造函数、析构函数、拷贝构造函数、拷贝赋值函数。C++11 因为引入了右值（Rvalue）和转移（Move），又多出了两大函数：转移构造函数和转移赋值函数。所以，在现代 C++ 里，一个类总是会有六大基本函数：**三个构造、两个赋值、一个析构。**

##### 编码准则

* 对于比较重要的构造函数和析构函数，应该用“= default”的形式，明确地告诉编译器（和代码阅读者）：“**应该实现这个函数，但我不想自己写**。”这样编译器就得到了明确的指示，可以做更好的优化。

* ```cpp
  
  class DemoClass final 
  {
  public:
      DemoClass() = default;  // 明确告诉编译器，使用默认实现
     ~DemoClass() = default;  // 明确告诉编译器，使用默认实现
  };
  ```

* 还有一种“= delete”的形式。它表示**明确地禁用某个函数形式**，而且不限于构造 / 析构，可以用于任何函数（成员函数、自由函数）。

* ```cpp
  
  class DemoClass final 
  {
  public:
      DemoClass(const DemoClass&) = delete;              // 禁止拷贝构造
      DemoClass& operator=(const DemoClass&) = delete;  // 禁止拷贝赋值
  };
  ```

* 你的类里有单参数的构造函数，或者是转型操作符函数，为了**防止意外的类型转换**

* **一般来说单参才有这种隐式转换。多参是需要显示调用的构造传参的。**

* 有个**构造函数A(int x)，那么，A a = 1，这里就会有一个隐式构造**。大多数时候这个不是问题，但有的时候会导致意外的转换。

* ```cpp
  
  class DemoClass final 
  {
  public:
      explicit DemoClass(const string_type& str)  // 显式单参构造函数
      { ... }
  
      explicit operator bool()                  // 显式转型为bool
      { ... }
  };
  ```

* 

##### 常用技巧

* 在**类实例化之前，不存在成员变量的初始化，而仅仅是声明**，只有在**实例化一个类对象时，才会去做成员变量的初始化**， 所以只有实例化才会去开辟一段内存空间.
* 头文件里的类只是**定义/声明**，并不是实体，所以类的成员变量是没有任务问题的。

* 在 C++11 里，你就可以使用“委托构造”的新特性，一个**构造函数直接调用另一个构造函数**，把构造工作“委托”出去，既简单又高效。

* ```cpp
  
  class DemoDelegating final
  {
  private:
      int a;                              // 成员变量
  public:
      DemoDelegating(int x) : a(x)        // 基本的构造函数
      {}  
  
      DemoDelegating() :                 // 无参数的构造函数
          DemoDelegating(0)               // 给出默认值，委托给第一个构造函数
      {}  
  
      DemoDelegating(const string& s) : // 字符串参数构造函数
          DemoDelegating(stoi(s))        // 转换成整数，再委托给第一个构造函数
      {}  
  };
  ```

* C++11 里，你可以在类里**声明变量的同时给它赋值**，实现初始化，这样不但简单清晰，也消除了隐患。

* ```cpp
  
  class DemoInit final                  // 有很多成员变量的类
  {
  private:
      // 还可以像下面这样写，好家伙
      int                 a = 0;        // 整数成员，赋值初始化
      string              s = "hello";  // 字符串成员，赋值初始化
      vector<int>         v{1, 2, 3};   // 容器成员，使用花括号的初始化列表
  public:
      DemoInit() = default;             // 默认构造函数
     ~DemoInit() = default;             // 默认析构函数
  public:
      DemoInit(int x) : a(x) {}         // 可以单独初始化成员，其他用默认值
  };
  
  ```

* **类型别名**： C++11 扩展了**关键字 using 的用法**，增加了 typedef 的能力，可以定义类型别名。它的格式与 typedef 正好相反，别名在左边，原名在右边，是标准的赋值形式，所以易写易读。

* ```cpp
  
  class DemoClass final
  {
  public:
      using this_type         = DemoClass;          // 给自己也起个别名
      using kafka_conf_type   = KafkaConfig;        // 外部类起别名
  
  public:
      using string_type   = std::string;            // 字符串类型别名
      using uint32_type   = uint32_t;              // 整数类型别名
  
      using set_type      = std::set<int>;          // 集合类型别名
      using vector_type   = std::vector<std::string>;// 容器类型别名
  
  private:
      string_type     m_name  = "tom";              // 使用类型别名声明变量
      uint32_type     m_age   = 23;                  // 使用类型别名声明变量
      set_type        m_books;                      // 使用类型别名声明变量
  
  private:
      kafka_conf_type m_conf;                       // 使用类型别名声明变量
  };
  ```

* <img src="https://static001.geekbang.org/resource/image/67/b3/6739782414607164bdbe20fca7fd5fb3.jpg?wh=2000*2535" alt="img" style="zoom:33%;" />





#### auto/decltype 自动类型推导

* 问题： 泛型编程的时候，麻烦就来了。因为**泛型编程里会有很多模板参数，有的类型还有内部子类型**，一下子就把 C++ 原本简洁的类型体系给搞复杂了，这就迫使我们去和编译器“斗智斗勇”，只有写对了类型，编译器才会“放行”（编译通过）。


* 关键字 auto，在代码里的作用像是个“占位符”（placeholder）。写上它，你就可以让**编译器去自动“填上”正确的类型**，既省力又省心。

* 编译器一直是知道这个类型的，我们需要让编译器自动的填补上这个类型.

* ```cpp
  
  auto  i = 0;          // 自动推导为int类型
  auto  x = 1.0;        // 自动推导为double类型
  
  auto  str = "hello";  // 自动推导为const char [6]类型, 推导错误，我们希望的是 std::string 类型
  
  std::map<int, std::string> m = {{1,"a"}, {2,"b"}};  // 自动推导不出来
  
  auto  iter = m.begin();  // 自动推导为map内部的迭代器类型
  
  auto  f = bind1st(std::less<int>(), 2);  // 自动推导出类型，具体是啥不知道
  ```

* “自动类型推导”实际上和“attribute”一样，是**编译阶段的特殊指令**，指示编译器去计算类型。所以，它在泛型编程和模板元编程里还有更多的用处.



##### auto

* 如果不是初始化的形式，**只是“纯”变量声明**，那就无法使用 auto。因为这个时候**没有表达式可以让 auto 去推导**。

* ```cpp
  
  auto x = 0L;    // 自动推导为long
  auto y = &x;    // 自动推导为long*
  auto z {&x};    // 自动推导为long* 
  
  auto err;       // 错误，没有赋值表达式，不知道是什么类型
  ```

* 还有一个特殊情况，**在类成员变量初始化的时候**，目前的 **C++ 标准不允许使用 auto 推导类型**

* ```cpp
  class X final
  {
      auto a = 10;  // 错误，类里不能使用auto推导类型
  };
  ```

* auto 推导规则：

* > * auto 总是推导出“值类型”，绝不会是“引用”；
  >
  > * auto 可以附加上 const、volatile、*、& 这样的类型修饰符，得到新的类型。
  >
  > * ```cpp
  >                       
  >   auto        x = 10L;    // auto推导为long，x是long
  >                       
  >   auto&       x1 = x;      // auto推导为long，x1是long&
  >   auto*       x2 = &x;    // auto推导为long，x2是long*
  >   const auto& x3 = x;        // auto推导为long，x3是const long&
  >   auto        x4 = &x3;    // auto推导为const long*，x4是const long*
  >   ```



##### decltype

* Q : “自动类型推导”要求必须从表达式推导，那在没有表达式的时候，该怎么办呢？

* A : decltype 的形式很像函数，后面的圆括号里就是可用于计算类型的表达式.

* 因为它**已经自带表达式，所以不需要变量后面再有表达式，也就是说可以直接声明变量**。

* ```cpp
  
  int x = 0;          // 整型变量
  
  decltype(x)     x1;      // 推导为int，x1是int
  decltype(x)&    x2 = x;    // 推导为int，x2是int&，引用必须赋值
  decltype(x)*    x3;      // 推导为int，x3是int*
  decltype(&x)    x4;      // 推导为int*，x4是int*
  decltype(&x)*   x5;      // 推导为int*，x5是int**
  decltype(x2)    x6 = x2;  // 推导为int&，x6是int&，引用必须赋值
  ```

* **decltype 不仅能够推导出值类型，还能够推导出引用类型**，也就是表达式的“原始类型”。(auto 只能推导出值类型变量)

* C++14 就又增加了一个**“decltype(auto)”的形式，既可以精确推导类型，又能像 auto 一样方便使用。**(真行啊)

* ```cpp
  
  int x = 0;            // 整型变量
  
  decltype(auto)     x1 = (x);  // 推导为int&，因为(expr)是引用类型, 这里是C++一个比较奇怪的语法，x是值类型，加上括号就变成了引用类型，算是个“坑”吧。
  decltype(auto)     x2 = &x;   // 推导为int*
  decltype(auto)     x3 = x1;   // 推导为int&
  ```

* 在 C++14 里，**auto 还新增了一个应用场合，就是能够推导函数返回值**，这样在写复杂函数的时候，比如返回一个 pair、容器或者迭代器，就会很省事。

* ```cpp
  
  auto get_a_set()              // auto作为函数返回值的占位符
  {
      std::set<int> s = {1,2,3};
      return s;
  }
  ```

* <img src="https://static001.geekbang.org/resource/image/6e/14/6ec0c53ee9917795c0e2a494cfe70014.png?wh=3000*2994" alt="img" style="zoom:33%;" />









#### RAII

* RAII 依托**栈和析构函数**，来对**所有**的资源——包括**堆内存**在内——进行管理

* 程序通常需要牵涉到三个可能的内存管理器的操作：

* > * 1. 让内存管理器**分配**一个某个大小的内存块
  > * 2. 让内存管理器**释放**一个之前分配的内存块
  > * 3. 让内存管理器进行垃圾收集操作，**寻找不再使用的内存块**并予以释放
  > * C++ 通常会做上面的操作 1 和 2。Java 会做上面的操作 1 和 3。而 Python 会做上面的操作 1、2、3
  > * 释放内存不只是简单地把内存标记为未使用。**对于连续未使用的内存块，通常内存管理器需要将其合并成一块**，以便可以满足后续的较大内存分配要求。毕竟，目前的编程模式都要求申**请的内存块是连续的**。

* C++  支持将对象存储在栈中，但是经常无法存储在栈中：

* > * 对象很大；
  > * 对象的大小在**编译时不能确定**；
  > * 对象是函数的返回值，但由于特殊的原因，不应使用对象的值返回。

* RAII的基本用法： 将清理工作（删除指针，释放资源等等...）放在**析构函数**中，而不是手动的在代码中显式调用； 这样伴随着**栈变量无论如何在退出的时候都会调用析构函数的特性**，将清理这一操作成为必然，而不会因为人工遗漏导致内存泄漏； 一句话总结就是将清**理逻辑通过析构函数告诉编译器**，由**编译器**来帮助程序员完成清理工作。

* Q : 局静态和局部静态的变量是存储在哪个区域？看很多书是静态存储区，但静态存储区又是什么区？堆？

* A : 静态存储区既不是堆也不是栈，而是……静态的。意思是，它们是在程序编译、链接时完全确定下来的，具有**固定的存储位置**（暂不考虑某些系统的地址扰乱机制）。**堆和栈上的变量则都是动态的，地址无法确定**。

* Q : 工程的时候，具体怎么考虑在栈上分配还是在堆上分配，更合理些？

* A : 凡生命周期超出当前函数的，一般需要用堆（或者使用对象移动传递）。反之，生命周期在当前函数内的，就该用栈。





#### struct && class

##### C++ 

* C++ struct & class 区别不大

* > * 面向对象特性 struct 也有.
  >
  > * **struct可以包括成员函数**
  >
  > * 区别：
  >
  > * > * 1. **默认的继承访问权。class默认的是private,strcut默认的是public。**
  >   >   2. 对于默认的继承级别 class 是 private,  struct 是 public.
  >   > * 2. **“class”这个关键字还用于定义模板参数，就像“typename”。但关建字“struct”不用于定义模板参数**

##### C

* C  struct & class 区别很大

* > * **因为struct是一种数据类型，那么就肯定不能定义函数，所以在面向c的过程中，struct不能包含任何函数。否则编译器会报错**
  > * struct 是数据类型， 面向过程要求数据和数据操作是要分开的，所以不能定义函数。
  > * c++ 面向对象的特性 C 都没有



#### 实现 smart pointer

[shared pointer](#智能指针)

* 我们是不是可以考虑在拷贝智能指针时把对象拷贝一份？不行，通常人们不会这么用，因为使用智能指针的目的就是要**减少对象的拷贝啊**

* **c++默认会生成一个浅拷贝构造函数**，会将两个对象的指针变量指向堆中的同一块数据，此时调用析构函数，同一块地址的数据不可能被释放两次，所以报错奔溃。

* 1. 这里最简单的方式是删除（delete）了拷贝构造和赋值两个函数，直接让测试用例编译失败。 
  2. 如果不删除这两个函数，那就会存在 “multiple free” 的问题导致程序崩溃 - 浅拷贝问题（导致多个指针指向一个）原因是： **c++默认会生成一个浅拷贝构造函数**
  
* 根据 C++ 的规则，如果**我提供了移动构造函数而没有手动提供拷贝构造函数，那后者自动被禁用**（记住，C++ 里那些复杂的规则也是为方便编程而设立的）

* 赋值构造需要产生一个临时对象，没有拷贝构造函数，只能移动构造产生，因此必须传递std::move对象（因为，找不到对应的构造函数,因此编译出错）

* ![img](https://static001.geekbang.org/resource/image/07/c8/072fc41e503d22c3ab2bf6a3801903c8.png?wh=1200*500)

* unique_ptr**一个对象只能被一个只能指针拥有，多个智能指针同时拥有一个对象是shared_ptr.**

* unique_ptr 算是一种较为安全的智能指针了。但是，**一个对象只能被单个 unique_ptr 所拥有**，这显然不能满足所有使用场合的需求。

* 子类指针是可以向基类指针转换的

* **智能指针准确说是一个对象，通过调用对象的构造和析构函数创建一个智能指针和销毁一个指针**

* ```cpp
  // shared_point 循环引用
  #include <bits/stdc++.h>
  
  using namespace std;
  
  struct Node{
      // shared_ptr<Node> next; 可能出现循环引用
      // shared_ptr<Node> prev; 可能出现循环引用
      weak_ptr<Node> next;
      weak_ptr<Node> prev;
  };
  
  int main() {
      shared_ptr<Node> ptr = make_shared<Node>();
      shared_ptr<Node> ptr1 = make_shared<Node>();
      {
          ptr -> next = ptr1;
          cout << ptr.use_count() << endl;
          cout << ptr1.use_count() << endl;
          ptr1 -> next = ptr;
          cout << ptr.use_count() << endl;
          cout << ptr1.use_count() << endl;
      }
      cout << ptr.use_count() << endl;
      cout << ptr1.use_count() << endl;
      // delete ptr;
      // cout << ptr.use_count() << endl;
      // cout << ptr1.use_count() << endl;
      return 0;
  }
  
  ```

* 如果指针变量需要绑定资源的所有权，那么会选择unique_ptr或[shared_ptr](https://www.zhihu.com/search?q=shared_ptr&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A34428725})

* 如果**不需要拥有资源的所有权**，那么会选择[weak_ptr](https://www.zhihu.com/search?q=weak_ptr&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A34428725})和raw pointer，这两种指针变量在**离开作用域时不会对其所指向的资源产生任何影响**。

* **独占则使用unique_ptr**(人无我有，人有我丢)，否则使用shared_ptr(你有我有全都有)。这一点很好理解。

* Q： 如果不拥有资源的所有权(non-owning pointer)，那么指针变量是否需要在适当的时候感知到资源的有效性？

* A： 如果需要则使用weak_ptr，它可以在适当的时候通过weak_ptr::lock()获得所有权，当**拥有所有权后便可以得知资源的有效性**。

* Q： 为何unique_ptr不能和weak_ptr配合？

* A： 这是因为unique_ptr是独占所有权，也就是说**资源的生命期等于指针变量的生命期**，那么程序员可以很容易通过指针变量的生命期来判断资源是否有效，这样weak_ptr就不再有必要了。而相对来说，**shared_ptr则不好判断，特别是多线程环境下**。

* 很多人说weak_ptr的作用是可以破除循环引用，这个说法是对的，但没有抓住本质(祼指针也可以破除，那为何要用weak_ptr?)。**写出循环引用的原因是因为程序员自己没有理清楚资源的所有权问题**。

* 循环引用是所有权不明造成的，简单来说就是在所有权明晰的情况下不应该出现循环引用。

* 智能指针一个很重要的概念是“所有权”，**所有权意味着当这个智能指针被销毁的时候，它指向的内存（或其它资源）也要一并销毁**。

* 这技术可以利用智能指针的生命周期，来自动地处理程序员自己分配的内存，**避免显示地调用delete**，是自动资源管理的一种重要实现方式。

* 为什么要引入“弱引用”指针呢？弱引用指针就是没有“所有权”的指针。

* 有时候我只是想找个指向这块内存的指针，但我不想把这块内存的生命周期与这个指针关联。这种情况下，弱引用指针就代表“我指向这东西，但这东西什么时候释放不关我事儿……”

* 直接用原始指针（raw pointer）来表示弱引用。然后用这种原始指针，其弱引用的含义不够明确，万一别人写个delete xxxx，你就被坑了……而且弱引用指针还有其它一些方便你正确使用它的好处。







* <img src="https://static001.geekbang.org/resource/image/e5/51/e5298af2501d0156fcc50d50cdb82351.jpg?wh=1000*1677" alt="img" style="zoom:60%;" />
* 





#### 左值 & 右值

* 左值 lvalue 是**有标识符、可以取地址的表达式**，最常见的情况有：

* > * 变量、函数或数据成员的名字
  > * 返回左值引用的表达式，如 ++x、x = 1、cout << ' '
  > * 字符串字面量如 "hello world"

* 纯右值 prvalue 是**没有标识符、不可以取地址的表达式**，一般也称之为“临时对象”。最常见的情况有：

* > * 返回非引用类型的表达式，如 x++、x + 1、make_shared(42)
  > * 除字符串字面量之外的字面量，如 42、true

* ```cpp
  
  template <typename U>
  smart_ptr(const smart_ptr<U>& other) noexcept
  {
    ptr_ = other.ptr_;
    if (ptr_) {
      other.shared_count_->add_count();
      shared_count_ =
        other.shared_count_;
    }
  }
  template <typename U>
  smart_ptr(smart_ptr<U>&& other) noexcept // other 是个变量的名字，变量有标识符、有地址，所以它还是一个左值——虽然它的类型是右值引用。
  // 类型是右值引用，变量是一个左值
  {
    ptr_ = other.ptr_;
    if (ptr_) {
      shared_count_ =
        other.shared_count_;
      other.ptr_ = nullptr;
    }
  }
  ```

* std::move 转**换成右值引用的目的是触发移动构造函数的调用**。

* “值类别”（value category）和“值类型”（value type）是两个看似相似、却毫不相干的术语。**前者指的是上面这些左值、右值相关**的概念，**后者则是与引用类型（reference type）**相对而言，表明一个变量是代表实际数值，还是引用另外一个数值。在 C++ 里，**所有的原生类型、枚举、结构、联合、类都代表值类型**，**只有引用（&）和指针（*）才是引用类型**。

* 临时对象的生命周期非常短，通常只在声明该对象的那条语句上，执行完即销毁

* C++ 的规则是：一个临时对象会在包含这个临时对象的**完整表达式估值完成后、按生成顺序的逆序被销毁**

* 比如vector<int>v本身是**栈对象**，但**里面allocator分配的空间是在堆中**。如果不用移动，由于容器内实际资源位置可能位于堆上，复制对象时必须使用深拷贝，开销较大。



```cpp
std::string foo() {
    std::string s = some_value();
    return s; // 返回值采用的是右值操作，返回的是一个右值，使用的移动的方式，而不是拷贝 （在 C++11 中）
}

std::string t = foo(); // foo() 返回值是纯 右值，所以是一次移动操作
```





#### exit(0)  & return -1

* exit(0) : 退出进程， return -1 表示的是结束当前的函数，不能结束整个进程的执行



#### noexpect 作用

* noexcept告诉编译器，函数不会抛异常，**有利于编译器对程序做更多的优化**。**异常是在运行时检测的**。为了实现运行时检测，编译器创建额外的代码，然而这会妨碍程序优化。 
* 若运行时，noexecpt函数向外抛出了异常。程序会直接终止，调用std::terminate()函数，该函数内部会调用std::abort()终止程序。 
* 以下情形鼓励用noexcept： 移动构造函数（move constructor） 移动赋值函数（move assignment） 析构函数（destructor）
* 







#### 多线程编程

```cpp
void run(int delay, std::function<void()> task) {
    thread([delay, task] () { // 并发基本代码
        std::this_thread::sleep_for(std::chrono::milliseconds(delay));
        task();
    }).detach(); // detach 表示的是当前的进行交给系统托管了，和主线程进程是没有关系了。
    // 所以主线程结束了，子线程还是可以继续执行，因为主线程和子线程已经分离了。
    // 如果 不 detach 那么会报错，因为子线程可能还没结束，但是主线程已经结束了，子线程和主线程是相关联的，同时线程和进程没有分离,抛出异常
}

/*
1. join() 函数？
join() 函数有两个作用， 1. 等待子线程执行完毕，主线程才结束执行。 2. 清理子线程相关的存储器， std::thread 不再和子线程相关联，释放子线程的资源
因为 join  是阻塞式的函数，所以子线程没有结束，主线程 main 是不会结束的。
join() 函数也可以回收子线程的资源

2. detach 函数
detach() 函数，让线程在后台运行，意味着线程之间不能直接产生交互，后台线程归属和控制都是由 C++ 运行库处理
detach() 函数需要考虑的是：确保子线程的参数必须是对象的复制，因为可能主线退出导致临时对象的实现，子线程对象相继实现.
主线程退出，子线程还在执行，子线程拥有使用主线程资源，会出错.
主线程 mian 可以比子线程提前退出，detach() 函数不是阻塞式的函数，所以子线程可以在后台接着运行，运行完，将子线程的资源释放。
*/
```











#### 拷贝构造函数，拷贝赋值运算符

* 拷贝构造是在初始化的时候发生的拷贝

* 在初始化之后，我们需要进行赋值操作，执行拷贝赋值运算符

* ```cpp
	class A {
	public:
	  	A(const A& a) {
	      	num = a.num;
	      	// 需要开辟新的空间
	      	p = new int[500];
	      	memcpy(p, a.p, 500 * sizeof(int));
	    }
	  	A& operator = (const A& a) {
	      	num = a.num;
	      	p = new int[500];
	      	memcpy(p, a.p, 500 * sizeof(int));
	    }
	private:
	  	int num;
	  	int* p;
	}
	
	int main() {
	  	A a; // 默认构造函数
	  	A b = a; // 拷贝构造函数，因为是初始化的时候发生拷贝
	  	A d(a); // 拷贝构造函数，因为初始化发生拷贝
	  	A c;
	  	c = a; // 拷贝赋值运算符，初始化之后，执行的拷贝操作
	  	return 0;
	}
	```

* 







#### 指针

* 因为 指针大小和 CPU 可以处理数据位数相关，CPU 是32 位，表示 CPU 一次可以处理 32位的数据，CPU 是64位表示的是 CPU 一次可以处理 64位的数据
* 指针存放的是内存地址，而内存地址长度是固定的 （32位或者 64位），所以要存放固定长度的地址，小了不够用，大了浪费
* 32 位系统，使用 4B  的指针，可以表示所有的这些地址范围，32位系统 4GB 大小，是32位 4B * 每一位占用 1个字节就是 1Byte,所以总的大小是 4GB
* 64位系统，这个位数指的是CPU 里面的通用[寄存器](https://so.csdn.net/so/search?q=寄存器&spm=1001.2101.3001.7020)的数据宽度为64位，也就是说一个地址占二进制位数是64
* 地址跟系统有关，但是基本数据类型占的大小是由C语言本身决定。

* 







#### move 语义

* std::move

* 可以进行所有权的转移

* 可以将左值转换成右值

* 左值一般在内存中，右值一般在内存或CPU寄存器中

* ```cpp
  int a = 3 // 左值
  a + 5 // 这个是右值，
  ```

* ```cpp
	template <typename T>
	typename remove_reference<T>::type&& move(T&& t)
	// move_reference 是一个类，使用这个操作可以将引用去除，例如原来是 int && 现在变成 int
	{
	    return static_case<typename remove_reference<T>::type&&>(t);
	}
	```

* ```cpp
	int && move(int&& && t){ // 引用折叠，编译器内部是能够识别的，但是没有提供相应的接口
	  	// int&& && 等价于 int &&
	    return static_case<int&&>(t);
	}
	
	//或
	int && move(int& && t){
	  	// int& && 等价于 int &
	    return static_case<int&&>(t);
	}
	// 所以 move 类似做了一个强制类型转换,转成右值
	```

* ```cpp
	class A {
	public:
	    A() {
	        cout << "constructor" << endl;
	    }
	    ~A() {
	        cout << "deconstructor" << endl;
	    }
	    A(int a) {
	        cout << "constructor this A" << endl;
	    }
	    A(const A& a) {  // 拷贝构造函数
	        cout << "copy constructor" << endl;
	        ptr = new int();
	        memcpy(ptr, a.ptr, sizeof(int));
	    }
	    A& operator = (const A& a) {
	        cout << "operator = :" << endl;
	        return *this;
	    }
	    A(A && a) {  // 表达式必须是可以修改的
	        // 移动构造函数
	        cout << "move constructor" << endl;
	        ptr = a.ptr;  // 所有权转移，同时通过移动构造函数创建 A 对象，减少了对堆空间的频繁操作，从而提升了程序的执行效率
	        a.ptr = nullptr;  // 原 A 对象的指针地址已经指向 NULL 了，因此不能通过其访问之前的堆空间了。实现了所有权的转移
	    }
	private:
	    int* ptr;
	};
	
	// 万能引用
	template<typename T>
	void f(T&& param) {
	    cout << "the value is : " << param << endl;
	}
	
	int main() {
	    /*
	    移动构造函数
	    vector<A>vec;
	    // A a;
	    vec.push_back(5);
	    cout << "-------------------" << endl;
	    vector<A>vec1;
	    // A b;
	    vec1.emplace_back(5);
	    cout << "*******************" << endl;
	    constructor this A
	    move constructor
	    deconstructor
	    -------------------
	    constructor this A
	    *******************
	    deconstructor
	    deconstructor
	    */
	
	    // 万能引用,可以接受一个左值，也可以接受一个右值
	    int a = 123;
	    f(a); // 可以传入一个左值
	    f(10086); // 可以传入一个右值
	    return 0;
	}
	```

* ![img](https://img2018.cnblogs.com/blog/603001/201912/603001-20191204201935249-1417409621.png)

* 而移动构造函数与拷贝构造不同，它并不是重新分配一块新的空间，将要拷贝的对象复制过来，而是"偷"了过来，将自己的指针指向别人的资源，然后将别人的指针修改为`nullptr`，这一步很重要，如果不将别人的指针修改为空，那么临时对象析构的时候就会释放掉这个资源，"偷"也白偷了。

* move 对那些拥有资源的对象有作用，对拥有内存，文件句柄等资源的成员对象有效，但是基本类型 如 ：int 还是会执行拷贝的，因为没有对应的移动拷贝函数，所以 move 对含有资源的对象更有意义.

* ```cpp
	class A {
	public:
	    A() {
	        cout << "constructor A!" << endl;
	    }
	    A(int a) : num(a) {}
	    ~A() {
	        cout << "deconstructor A!" << endl;
	    }
	    A(const A& a) {
	        cout << "copy constructor A!" << endl;
	    }
	    A& operator = (const A& a) {
	        cout << "assignment constructor A!" << endl;
	        return *this;
	    }
	    A(A&& a) { // 这个不能是 const 因为需要对其进行修改: a.ptr = nullptr
	        cout << "move constructor A!" << endl;
	    }
	    A& operator = (const A&& a) {
	        cout << "move assignment A!" << endl;
	        return *this;
	    }
	    // int getNum() {
	    //     return num;
	    // }
	private:
	    int num;
	};
	
	void Swap(A& a, A& b) noexcept {
	    A temp(std::move(a));
	    a = std::move(b);
	    b = std::move(temp);
	}
	
	int main() {
	    A a(2);
	    A b(3);
	    Swap(a, b);
	    cout << "---------" << endl;
	    swap(a, b);
	    /*
	    move constructor A!
	    move assignment A!
	    move assignment A!
	    deconstructor A! // 这个是调用移动构造函数，所以最终需要进行析构函数释放,析构 temp 这个对象
	    ---------
	    move constructor A!
	    move assignment A!
	    move assignment A!
	    deconstructor A! // 这个是调用移动构造函数，所以最终需要进行析构函数释放,析构 temp 这个对象
	    deconstructor A! // 析构初始的 a
	    deconstructor A! // 析构初始的 b
	    */
	    return 0;
	}
	```

* 







#### 完美转发

* std::forward<T> (args)  T 是右值的话，这个 std::forward<T>(args)  相当于是 std::move(args)
* 



#### 万能引用

* T && 是万能引用，可以是左值，也可以是右值，是万能的引用

* **T&& 或者 auto && 形式**，表示的是，传入的参数可以是左值，也可以是右值，所以这个 **T 是一个模板**，是可以推导的， auto 也是可以推导的，模板顾名思义就是可以进行类型推导的。

* 模板 + 类型别名可能出现引用折叠， 

* > & &&, && &, 会被引用折叠成左值
  >
  > && && 会被引用折叠成右值



#### Emplace_back() & push_back() 区别

* Vector 中的 push_back 对应 emplace_back, map / set 的 insert 对应 emplace
* 底层实现机制不同，push_back() 向底部传输一个数据，但是首先是需要创建一个元素(调用构造函数），然后对这个元素进行拷贝（调用拷贝构造函数）或者移动（调用移动构造函数）到容器中，如果是拷贝的话，事后会自动销毁创建的元素。
* Emplace_back() 直接在容器尾部创建这个元素，省去了拷贝和移动元素的过程。
* push_back() 的底层实现比 emplace_back() 更加复杂， 所以我们需要尽可能的使用 emplace_back()，因为这个效率更高。
* emplace_back() 函数的功能是可以直接将参数传给对象的构造函数，在容器中构造出一个对象，从而实现 0 拷贝。(底层机制是调用placement new即布局new运算符来实现的)

```cpp
class testdemo {
public:
    testdemo(int num) : num(num) {
        cout << "构造函数" << endl;
    }
    ~testdemo() {
        cout << "析构函数" << endl;
    }
    testdemo(const testdemo& other) : num(other.num) {
        cout << "拷贝构造函数" << endl;
    }
    testdemo(testdemo&& other) : num(other.num) {
        cout << "移动构造函数" << endl;
    }
private:
    int num;
};
int main() {
    vector<testdemo>vec;
    cout << "push_back operation : " << endl;
    vec.push_back(5);

    cout << "emplace_back operation : " << endl;
    vector<testdemo>vec1;
    vec1.emplace_back(5);
    cout << "-------------" << endl;
    return 0;
}

output:

push_back operation : // 会调用 n 次移动构造或者拷贝构造（根据传入参数是左值或者右值判断）
构造函数
移动构造函数
析构函数 // 析构的是调用构造函数构早的数据
emplace_back operation :  // 不会调用移动或者拷贝构造函数
构造函数
-------------
析构函数
析构函数
```

* **push_back() 慢的原因**：
* 创建临时Student对象时,需要申请内存空间, **申请内存空间一向是耗时很严重的操作**;
* 拷贝构造函数的复制操作也是需要CPU时间的;
* **emplace_back() 快的原因**：
* 原理就是**emplace_back函数是直接在team中已有的空间上, 调用了Student类的构造函数**, **节省了临时对象的内存空间申请以及拷贝构造函数的复制**操作.
* 





#### void func()  & func(void)区别

```cpp
void func(){} 表示该函数在调用时可以传递任意多的参数（但是无法调用） 这个是在 C 语言中是这样，但是在 C++ 发现编译过不去
void func(void){} 表示该函数在调用时不能传递任何实参
void func() 这个函数没有返回值。
func(void) 这个函数在没有申明返回值的情况时，默认是返回 int。
```



#### vector & array

* **array 底层数据是存储在栈中，速度更快一点**， array<int, 10>arr; 定义方式
* vector 底层是数据是存储在堆中，但是vector 这个对象是存储在栈中的，速度稍微慢点。



#### const & constexpr

* **const 并不能代表着这个常量，他是对常量进行一个修饰作用**，这个修饰告诉编译器这个变量只能被初始化，不能被修改，而这个变量的**值可以在运行时指定，也可以在编译时指定**
* 声明是 const 类型的函数 bool solve() const {} 表示的是**这个函数不能修改任何成员变量**，同时类外调用 const 对象，这个成员函数必须声明成 const
* eg: void getPoint(const Point& a) {} **a 只能调用 Point 类中的常量成员函**数.
* constexpr: 可以用来修饰变量、函数、构造函数，一旦以上任意一个被其修饰，那么等于告诉编译器：请大胆的将我看**成编译时就可以得出常量值的表达式去优化**



#### 必须执行初始化列表的构造函数

* **类A即为带有初始化列表（红框内）的构造函数**。但是，这个类没有默认构造函数

* ```cpp
  class A {
  public:
      A() : x(8) {}  // 类 A 存在默认构造函数，这个就是 OK 的
      A(int x) : x(x) {}
  private:
      int x;
  };
  
  class B : public A {
  public:
      B() : val(0) {}  // 不需要加 A(2) 之类的也可以，因为 class A 存在默认构造函数
      B(int val) : val(val) {} // 如果类A没有默认构造函数，那么这里会报错
  private:
      int val;
      A a;
  };
  
  int main() {
      B b;
      return 0;
  }
  ```

* 

* ![c++ 必须带有初始化列表的构造函数_构造函数](https://s2.51cto.com/images/202110/f79dfa3055e9b5f1a0a5216a427db1d28e2a91.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=)

* 当类B含有成员a为A类型时，**在类B的所有构造函数中，必须使用初始化列表对a初始化**。在构造函数体内赋值是不行的。（下图会报错）

* ![c++ 必须带有初始化列表的构造函数_初始化列表_02](https://s2.51cto.com/images/202110/836e5ad796f798a37b5088440dd1f96146d04b.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=)

* **基类，无默认构造函数**（注意基类没有才会出错，基类有的话就不会报出错信息）

* **当类B继承类A时，在B中也需要使用构造函数初始化列表对A进行初始化**，如下图红框。

* ![c++ 必须带有初始化列表的构造函数_构造函数_04](https://s2.51cto.com/images/202110/a557afe8740fe95bfec189499ff9a4a96ac19f.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=)

* **const 成员或引用类型的成员。**

* 



#### 静态链接和动态链接的区别

* 静态链接：将其链接文件包含在 exe 可执行文件中，是可执行文件的一部分,是被**编译**到 EXE 文件中，lib 文件就是静态链接，在编译的时候将其链接到 exe 文件中

* 动态链接：在运行的时候才动态的加载链接进去，在程序启动的时候，加载你的 dll 文件，dll 就是一个动态链接库，是被**链接**到 EXE 文件中.

* 如果在编译的时候静态链接了，那么我们就不需要进行动态链接的操作了。

* 动态链接是一个独立存在的单元，单独的模块在运行的时候加载，可执行文件知道需要这个 dll 文件，如果这个文件不存在，就会报错.

* 使用静态链接，可以将所有需要的文件都加载到 exe 文件中，不需要额外的外部文件的依赖，所以，这个 exe 文件可以在任意的地方执行，换个文件夹也是可以执行的，但是如果采用的是动态链接，那么需要在当前 exe 文件夹中添加这个 dll 文件，我们的 exe 所在文件夹必须随时随地拥有依赖的动态链接库文件

* 使用时机：

	> * 当库比较小，使用静态库，因为我们需要将内容拷贝到可执行文件中，静态库被打包到应用程序中了，所以运行速度比较快,同时发布的时候，无需提供静态库，移植方便.
	> * 静态库缺点：消耗系统资源，浪费内存，因为有可能一个可执行文件多次加载静态库的情况。浪费内存。更新，部署，发布麻烦，因为我们修改一个静态库中的内容，其他所有的东西都需要重新编译，链接，部署非常的不方便
	> * 动态库的优点：可以实现进程间资源贡献（共享库），更新部署发布方便，因为代码资源是不需要打包到执行文件中，是运行时加载的。
	> * 动态库缺点：加载速度慢， 发布程序需要提供依赖的动态库
	> * 所以库比较大的时候，使用动态库





#### inline 关键字

* inline 函数的目的是：小函数大量的消耗栈内存的问题，引入了 inline 关键词修饰，避免重复的调用小函数，对栈空间进行频繁的开辟带来的内存浪费，我们在调用的地方直接将小函数展开。

* inline 比较适合简单的函数，复杂的 while for 和递归函数都是不行的。

* 不是声明是 inline 函数就执行内联操作，需要看编译器是否执行内联的操作.只是一个建议

* 内联函数的调用需要进行代码量的复制，所以程序的总的代码量就会变得非常的大，消耗更多的内存空间，同时当程序的执行时间占据主要的时间消耗，函数调用节省的时间将是可以忽略不计的。

* 以下两种情况不要使用 inline 函数 ： 

  > * 如果函数体内的函数代码比较长，使用内联函数会带来内存的消耗比较高
  > * 当函数内存在循环，执行函数体内的代码远比函数调用带来的消耗大，所以编译器会优化不进行展开





#### Virtual 函数

* C++中虚函数表位于**只读数据段（.rodata）**，也就是C++内存模型中的**常量区**；而**虚函数则位于代码段（.text）**，也就是C++内存模型中的代码区。

* 析构函数和构造函数中不存在多态： 因为在析构函数和构造函数中调用虚函数不是多态，因为编译的时候就确定调用的是哪个函数，不需要运行时确定，所以不是多态

* 不要在构造函数和析构函数中使用虚函数，因为基类在构造函数和析构函数调用虚函数的时候不可以下降到派生类，因为当构造函数中存在虚函数，去派生类中找相应的函数，但是当执行这个虚函数的时候，我们的派生类的成员变量还没有执行初始化操作，因为派生类的构造函数还没有执行，因为基类的构造函数是优先派生类执行的，所以**要求使用对象内部未初始化的成分是非常危险的**。析构函数同理：因为派生类的析构函数优先执行，所以执行完了派生类的成员变量都释放了，再会基类析构函数调用虚函数，访问派生类的函数，因为成员变量都释放了。所以对未初始化的成员访问是非常危险的。

* 虚函数指针指向的是一个由函数指针组成的数组称为**虚函数表**，**每一个带有虚函数的 class 都包含一个对应的虚函数表**。当一个对象调用某一个虚函数的时候，实际被调用的函数是取决于对象的 虚函数指针所指向的虚函数表。**编译器会在虚函数表中寻找合适的函数指针执行相应的函数**

* **在 C++ 中对未初始化成分进行使用是非常危险的事情**

* ```cpp
	class Base {
	
	public:
	
	int a;
	
	virtual void f() { cout << "Base::f" << endl; }
	
	virtual void g() { cout << "Base::g" << endl; }
	
	virtual void h() { cout << "Base::h" << endl; }
	};
	
	int main() {
	  	Base a;
	  	print(sizeof(a)) ;// 打印出来是 16， 因为虚函数指针大小在 64 位是 8 字节，同时使用了内存对其的技术，所以 int  为了和指针对其所以大小就是 8 字节
	}
	```

* 同一个类的不同对象是公用一个虚函数表的。

* ```cpp
	#include<iostream>
	using namespace std;
	
	
	class A     
	{     
	};    
	
	class B     
	{  
	    char ch;     
	    virtual void func0()  {  }   
	};   
	
	class C    
	{  
	    char ch1;  
	    char ch2;  
	    virtual void func()  {  }    
	    virtual void func1()  {  }   
	};  
	
	class D: public A, public C  
	{     
	    int d;     
	    virtual void func()  {  }   
	    virtual void func1()  {  }  
	};     
	class E: public B, public C  
	{     
	    int e;     
	    virtual void func0()  {  }   
	    virtual void func1()  {  }  
	};  
	
	int main(void)  
	{  
	    cout<<"A="<<sizeof(A)<<endl;    //result=1  
	    cout<<"B="<<sizeof(B)<<endl;    //result=16      
	    cout<<"C="<<sizeof(C)<<endl;    //result=16  
	    cout<<"D="<<sizeof(D)<<endl;    //result=16  
	    cout<<"E="<<sizeof(E)<<endl;    //result=32 
	  	// 因为继承了 B 和 C，所以 B中的char 和 C 中的 char 也是需要继承的，所以大小是 8 字节 + 8字节，同时 E 是继承多个（B和C)，所以会产生两个虚函数指针，这时总的虚函数指针 8 + 8 字节，所以总的大小是：8 + 8 + 2 * 8
	    return 0;  
	}  
	```

* 





#### 构造函数不能是虚函数：

> * 虚函数的执行依赖于虚函数表，但是虚函数表是在构造函数执行结束之后，初始化一个虚函数表，让其指向正确的虚函数表，而在构造函数期间，**虚函数表还没有被初始化**，那么就没法调用构造函数。
> * 存储空间考虑：因为构造函数是虚的，所以需要使用虚函数表来调用，但是这个时候对象还没有进行实例化的操作，就是内存空间还没有，**因为指向虚函数表的指针存储在对象的内存空间中的**，那么此时就没法找到虚函数表，自然就没法调用虚函数。
> * 使用角度看没有实际的意义，因为构造函数就是初始化一个实例，没必要使用虚函数，
> * 从实现角度：虚函数表在构造函数执行之后才建立。
> * 实际含义角度：在调用构造函数时还不能确定对象的真实类型（因为子类会调父类的构造函数）；而且构造函数的作用是提供初始化，在对象生命期只执行一次，不是对象的动态行为，也没有必要成为虚函数。



#### char* & char[] 区别

* ```cpp
  1   char ch1[] = "ABCDEFGH";
  2   char * ch2 = "abcdefgh";
  ```

* ```cpp
  .globl ch1
      .data // data 数据区域
      .type   ch1, @object
      .size   ch1, 9
  ch1:
      .string "ABCDEFGH"
  .globl ch2
      .section    .rodata // char* 不能修改在只读数据区
  .LC0:
      .string "abcdefgh"
      .data
      .align 8
      .type   ch2, @object
      .size   ch2, 8
  ch2:
      .quad   .LC0
  ```

* > 1. ch1的内容存储在**函数栈**中，可以被修改，函数一旦返回空间就被释放。
  > 2. ch2的内容保存在**只读数据段**中，不可被修改，其空间不会被释放。

* > 1. ch1强调的是**数组**，ch2强调是**指针**，在各自作用域范围内有效，可操作。
  >
  > 2. 作为**数组可以操作的是数组值，那么数组内容是可以改变的**。
  >
  > 3. 作为指针可以操作的是指针值，那么指针内容是可以改变的，即指向另一个地址，但不能改变指针指向的内容。换句话说就是可以修改指针的值，但不能修改指针值的值。(const * )

```cpp
int main() {
    char ch1[] = "ABCDEFG";
    char* ch2 = "abcdefg";
    ch2[5] = 'h'; // 破坏常量,因为不能修改内部的内容
    ch2 = "hello"; // 这个就可以，因为指针重新指向别的对象，所以不会出错
    printf("%s\n", ch2);
    return 0;
}
```











#### this 指针

* this 指针是 c++ 的一个关键字，类似的关键字有`friend` 等等.
* this  是一个指针常量，那么就是这个指针不能指向别的对象
* **this 指向当前对象，可以通过 this 访问当前对象的所有成员**，比如一个 class 内部的所有函数，所有私有变量都可以使用 this 来指向，`eg : this -> a`
* this 指针可以通过解引用，获取当前的这个对象， this 是一个指针，代表打印出 this 我们将打印出的是一个地址
* this 只能在成员函数内部使用，在其他地方是没有意义的也是非法的
* 只有当对象被创建之后，this 才有意义，因此不能在 static 成员函数中使用
* **总结** ：this 实际上是成员函数的形参，在调用成员函数的时候将对象的地址作为实参传递给 this ， this 是隐式的，不会出现在代码中，只有在编译阶段由编译器默默的将其添加到参数列表中。只有在对象调用一个成员函数的时候，才会出现 this 指针，才会给 this 赋值





#### 引用作为返回值

* 通过引用作为返回值和通过指针作为函数的返回值，作用其实差不多
* 当函数作为引用进行返回的时候，返回的是一个指向返回值的隐式指针
* 可以放在赋值语句的左边 ：

```c++
// 原本 num[2] = 10010;
int& solve(vector<int>num) { // 作为引用进行返回的操作,可以对其返回值进行修改，
    return num[2];
}
solve(num) = 10086;
cout << num[2] << endl;
// 输出的 num[2] = 10086
```



#### 引用和指针的区别

* 引用： 指针常量：`*const` 指向的地址不可以改变，但是地址里面的内容可以改变
* 所以引用只能被初始化一次，之后不能改变这个引用所指向的地址，但是指向地址里面的内容是可以改变的
* 普通的指针指向的内容可以变
* 引用是一个别名，所以大小就是对应对象的类型大小，指针在32位操作系统一直是4字节
* 指针本身就是一个对象，引用不是一个对象，但是引用必须绑定在某个对象上。
* 引用必须初始化，指针不需要，所以传参传入一个指针可能是一个空指针，这是一个不安全的操作



#### 虚析构函数

* 带多态性质的基类应该声明成一个 virtual 析构函数，如果 class 带有任何虚函数，他就应该拥有一个 virtual 虚构函数

* classes 的设计目的如果不是为了给 base class 使用，或者不是为了具有多态的特性，那么就不应该使用 virtual 虚函数.不要无端的设置成虚析构函数.

* 因为基类的指针去销毁一个 派生类对象 可能造成**基类的析构函数先执行，派生类的析构函数没法执行销毁操作**，会出现局部销毁的情况，造成资源的浪费

* 不要无端的设计成虚析构函数：虚函数指针会带来内存占用的问题，会增加内存的消耗，相比于未使用虚函数。因为会多包含一个虚函数指针，这个会带来额外的 4 个字节的内存占用 (32位系统中) ---- 即对象的体积会增加

* 如果在派生类中申请了空间，并在其析构函数中对其空间进行了释放，如果不是虚析构函数，当删除基类指针指向的派生类对象的时候，不会触发动态绑定，因而只会调用基类的析构函数，而不会调用派生类的析构函数

* 系统默认将析构函数设置成非虚的，因为**系统不会将该类默认是基类**，为了减少内存开销，所以设置成非基类，但是如果一个类是基类，我们程序员需要**显示的将其设置成虚函数**。

* ```cpp
  #include <iostream>
  
  using namespace std;
  
  class Father {
  public:
      virtual ~Father() {
          cout << "class Father destroyed" << endl;
      }
  };
  
  class Son : public Father {
  public:
      ~Son() {
          cout << "class Son destroyed" << endl;
      }
  };
  
  int main() {
      Father* p = new Son;  // 指针明明指向的是派生类对象，需要先删除派生类的资源，再删除基类的资源
      delete p;
  
      return 0;
  }
  
  /*
  运行结果：
  class Son destroyed
  class Father destroyed
  */
  
  ```

* 





#### c++ static 关键字

* 静态成员的资源属于类，但是独立于类存在的，静态资源的是类初始化的时候加载的，而非静态资源是类实例化对象的时候加载的，**类的初始化是早于类的实例化**
* 静态成员不能调用非静态成员，因为初始化是要早于实例化的,因为静态方法没有 this 指针，没法访问非静态成员变量。
* 非静态成员可以调用非静态成员，因为静态成员是属于类本身，在类对象产生之前就已经存在了。
* **静态成员变量**在使用前必须进行初始化 `int MyClass::m_nNumber = 0`, 否则会在 linker 时出错
* 局部静态变量只会进行一次初始化操作
* 静态成员函数不需要进行初始化操作，成员函数也没法进行初始化.

> 小林 coding 总结
>
> * 静态成员变量本质是全局变量，哪怕一个对象都不存在，类的静态成员变量也是存在的， 
> * 非静态成员函数可以调用静态成员变量的。
>* 静态成员函数本质上是全局函数
> * 设置静态成员这个机制的目的是将某些紧密相关的全局变量和函数写在类里面，看上去想一个整体，易于维护和理解
> * 在静态成员函数中，不能访问非静态成员变量，也不能调用非静态成员函数
> * 静态成员必须在定义类的文件中对静态成员变量进行初始化，否则会编译失误。
> * 当声明 static 成员变量或者成员函数的时候，只能被当前的 c++ 文件访问到，不能被整个文件夹的别的 c++ 文件访问到
> * 

* 为什么是静态局部变量可以保证线程安全性？
* 因为 c++ 11 之后 static 是线程安全的

* 原因是Magic Static特性。**如果变量在初始化时，并发线程同时进入到static声明语句，并发线程会阻塞等待初始化结束。这样可以保证在获取静态局部变量的时候一定是初始化过的，所以具有线程安全性，同时也避免了new对象时指令重排序造成对象初始化不完全的现象**。并且相比较与使用智能指针以及mutex来保证线程安全和内存安全来说，这样做能够提升效率



#### C++ 构造函数未定义错误

* ```cpp
  class C {
  public:
      // C(int b) : x(b), t(x){} // 构造函数顺序不对可能出现 misleading
      C() = default;  // t x 都是未定义的,是一个随机的数值，因为没有声明初始值
      void print() { std::cout << "t : " << t << " x : " << x << std::endl; }
  private:
      int t, x;
  };
  C c;
  c.print();
  // t : -1414962992 x : 32766
  ```

* 编译器帮忙生成的构造函数和析构函数 **函数体都是空的**，啥也不做，所以有堆上数据一定要自己写析构函数释放内存。





#### 浅拷贝和深拷贝及其实现

* 含有**指针类型的成员或者动态分配内存**的成员，我们需要提供自定义的拷贝构造函数
* 在提供的拷贝构造函数的同时，我们还应该考虑实现自定义的赋值运算符
* 对于指针和动态分配的空间，在拷贝中应该重新分配空间。
* 拷贝构造函数和拷贝赋值函数的区别：
* 拷贝构造函数必须使用引用的方式传递参数，这是因为以值的方式传递，可能出现无限的循环递归下去 eg: solve(p) 一直执行这个 solve(p)递归执行，最终爆栈
* 拷贝构造函数会产生新的对象实例，如果产生了新的实例，那么调用的是拷贝构造，如果没有，那就是对已有的对象赋值，调用的是赋值运算符





#### 为什么 vector 扩容1.5倍或者2倍

> 可以拆分成两个问题：
>
> * 为什么是成倍增长，而不是增长一个指定大小容量
> * 为什么增长的倍数选择的是 1.5 或者是 2 ，而不是3 或者是 4倍

第一个问题：

* 为什么成倍增长，而不是分配指定的大小

1. 采用成倍增长，需要执行的向 vector 中 push_back 的次数会更少：n 个元素，选择的分配倍数是 m ，所以总共需要 一下次数的内存分配：
   $$
   m^k = n \Rightarrow k = \log_{m}{n} 
   $$
   所以总共需要执行的 push_back 次数是：
   $$
   \sum_{i= 1}^{\log_{m}{n} } m^i \approx \frac{mn}{m - 1} = n
   $$

   2. 采用指定大小的增长，总共需要的内存分配次数是：

   $$
   m * k = n\Rightarrow k = \frac{n}{m} 
   $$

   总共需要执行的 push_back 次数是：
   $$
   \sum_{i = 1}^{\frac{n}{m} }mi = O(n^2) 
   $$
   所以使用成倍增长，需要执行的 push_back 次数更少。

* 为什么增长的倍数选择的是 1.5 或者是 2 ，而不是3 或者是 4倍

因为倍数首先不能太大，也不能小于1，采用2倍的原因是，2倍正好是分配大小大于之前所有分配的总和，是边界，这样我们可以重新使用之前分配的缓存。

* 2 倍扩容问题在于没法利用已经释放的空间

* 原因是：假设第 n 次扩容分配了 2^n 的空间，但是之前分配的空间总和是：

* $$
	1 + 2 + 2^2 + ... + 2^{n - 1} = 2^n - 1
	$$

* 正好放不下 2^n 的空间，这样导致的结果就是操作系统需要不断的分配新的内存页，并且数组的首地址也在不断的增大，这样会造成缓存不命中.

* 假设扩容的倍率是 x , 首次分配的空间是1， 第二次扩容分配的空间是 ：O(x ^ 2) .如果希望第二次扩容正好能用上之前申请的空间，则有： 
	$$
	1 + x = x^2
	$$

* x = 1.618,当然第二次扩容需要将第一次扩容的数据拷贝出来，这样至少需要等到第三次扩容才能重复使用空间：

* $$
	1 + x = x^3
	->
	1 + x + x ^ 2 + ... + x ^{n - 2} = x ^ n
	$$

* 求出极限 x = 1.618,所以通常使用拓展的倍数都是 1.5 比如：visual studio 和 Java 的 ArrayList采用的都是拓展 1.5 倍内存空间

























#### c++ 基础知识

* .h 文件不会进行编译， 只会对 cpp 文件进行编译
* 不能在.h 文件进行全局变量的**定义**操作，因为可能出现重复定义的链接错误
* 定义： `int a = 8`  ,声明 ： `extern int a `  
* 声明需要满足两个条件 ： 1. 声明必须使用的是 extern 关键字， 2. 不能给变量赋初值



#### 程序执行的基本过程

* 预处理：加载头文件，宏定义之类的，生成 .i 文件

* 它们都以符号“#”开头

* “预处理”的目的是**文字替换**，用到的就是我们熟悉的各种预处理指令，比如 #include、#define、#if 等，实现“预处理编程”。

* ```cpp
	// gcc -E test.cc -o test.i 预处理过程 c++: g++ -E test.cpp -o test.i
	// 生成的文件存在 test.i 文件中
	// source code:
	#include <stdio.h>
	
	#define f(x) 10 + (x << 1)
	
	int main() {
	    // this is the test code
	    int num = f(5);
	    printf("%d\n", num);
	    printf("hello\n");
	
	    return 0;
	}
	
	// gcc -E test.cc 之后的代码
	// 头文件展开操作，没赋值进来，加载进来，宏定义也是直接展开，注释也没了
	# 5 "test.cc"
	int main() {
	
	    int num = 10 + (5 << 1);
	    printf("%d\n", num);
	    printf("hello\n");
	
	    return 0;
	}
	```

* 

* 编译：检查语法并对代码进行优化，将文本文件翻译成 .s 文件。得到汇编语言程序，头文件不参与编译过程。

* ```cpp
	// g++ -S test.cpp -o test.s 生成的是汇编文件 .s
	// source code
	#include <cstdio>
	
	#define f(x) 10 + (x << 1)
	
	int main() {
	    // this is the test code
	    int num = f(5);
	    printf("%d\n", num);
	    printf("hello\n");
	
	    return 0;
	}
	
	// 生成的汇编代码：
		.file	"test.cpp"
		.text
		.section	.rodata
	.LC0:
		.string	"%d\n"
	.LC1:
		.string	"hello"
		.text
		.globl	main
		.type	main, @function
	main:
	.LFB0:
		.cfi_startproc
		endbr64
		pushq	%rbp
		.cfi_def_cfa_offset 16
		.cfi_offset 6, -16
		movq	%rsp, %rbp
		.cfi_def_cfa_register 6
		subq	$16, %rsp
		movl	$20, -4(%rbp)
		movl	-4(%rbp), %eax
		movl	%eax, %esi
		leaq	.LC0(%rip), %rdi
		movl	$0, %eax
		call	printf@PLT
		leaq	.LC1(%rip), %rdi
		call	puts@PLT
		movl	$0, %eax
		leave
		.cfi_def_cfa 7, 8
		ret
		.cfi_endproc
	.LFE0:
		.size	main, .-main
		.ident	"GCC: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0"
		.section	.note.GNU-stack,"",@progbits
		.section	.note.gnu.property,"a"
		.align 8
		.long	 1f - 0f
		.long	 4f - 1f
		.long	 5
	0:
		.string	 "GNU"
	1:
		.align 8
		.long	 0xc0000002
		.long	 3f - 2f
	2:
		.long	 0x3
	3:
		.align 8
	4:
	
	```

* 

* 汇编：汇编过程将之前编译得到的 .s 文件转换成二进制文件，并将结果保存在目标文件 obj 文件。

* ```cpp
	// g++ -c test.cpp -o test.o 生成的是二进制文件，还是不能执行，需要链接之后才可以执行
	// 这一步是汇编过程，但是 -c 不进行链接操作，只是编译
	```

* 

* 链接：将不同的 obj 文件链接成同一个可执行的文件。生成 exe 文件。

* 运行：执行可执行的文件

* 编译阶段之后，有了可执行文件，C++ 程序就可以跑起来了，进入运行阶段。这个时候，“**静态的程序”被载入内存**（这个时候程序才会被加载到内存中），由 CPU 逐条语句执行，就形成了“动态的进程”。

* ```cpp
	// GCC 编译选项
	/*
	1. -g 在编译的时候生成调试信息，方便后面的 GDB debug
	2. -D 在程序编译的时候定义一个宏 eg -DDEBUG  这样文件中 #ifdef DEBUG ... 执行接下来的语句就可以了
	3. -w 不生成任何警告信息
	4. -c 编译，汇编，不进行链接操作
	5. -E 预处理，不进行编译（查看预处理过后的源代码）
	6. -S 编译指定源文件，不进行汇编操作
	7. -Wall 生成所有的警告，可能有些变量未使用，也会警告
	8. -On 表示进行等级 1- 3 的优化， -O0未优化， -O3 最高级优化
	9. -l 表示编译的时候使用到的库，动态或者静态库
	10. -L 表示编译的时候，搜索的库的路径
	11. -I 包含需要加载的库的路径
	*/
	// 对静态库进行链接编译的操作流程
	/*
	*** 注意无论是动态库还是静态库都需要包含 库文件 以及 头文件 .h 文件， .h 表示的是包含哪些函数，哪些接口，库文件是具体的实现函数. ***
	├── app
	├── include
	│   └── head.h
	├── lib
	│   └── libcalc.a
	├── main.c
	└── src
	    ├── add.c
	    ├── div.c
	    ├── mult.c
	    └── sub.c
	1. gcc -c a.c b.c 生成一系列 .o 文件
	2. 生成静态库：ar rcs libcalc.a a.o b.o   
	3. gcc main.c -o app -I ./include/ -l calc -L ./lib
	4. 最终生成的是 app 文件，这里使用了 链接库 libcalc
	*/
	
	// 制作动态库
	/*
	├── app
	├── include
	│   └── head.h
	├── lib
	│   └── libcalc.so
	├── main.c
	└── src
	    ├── add.c
	    ├── div.c
	    ├── mult.c
	    └── sub.c
	1. gcc 获得 .o 文件，得到和位置无关的代码
	gcc -c -fpic a.c b.c  // fpic 表示的是与位置无关
	2. gcc 获得动态库
	gcc -shared a.o b.o -o libcalc.so
	3. 运行:
	gcc main.c -o app -I ./include/ -l calc -L ./lib
	4. 加载动态链接库：
	// 临时配置:
	export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/zhoubing/Desktop/linux_c/1.4/library/lib
	// 永久配置
	1. 用户级别
	vim ~/.zshrc
	插入下面内容
	export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/zhoubing/Desktop/linux_c/1.4/library/lib
	2. 系统级别
	sudo vim /etc/profile
	插入下面的内容
	export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/zhoubing/Desktop/linux_c/1.4/library/lib
	
	ldd app 将动态链接库的依赖打印出来
	
	*/
	
	```

* ![img](https://static001.geekbang.org/resource/image/9c/df/9cb2036ae3dbda30a00d58bdd4834ddf.jpg?wh=2276*1523)

* 我们应该在**“编码”“预处理”“编译”**这前面三个阶段多下功夫，**消灭 Bug，优化代码**，尽量不要让 Bug 在“运行”阶段才暴露出来，也就是所谓的“把问题扼杀在萌芽期”。



#### C++  语言的五种范式

* C++ 是一种多范式的编程语言。具体来说，现代 C++（11/14 以后）支持**“面向过程”“面向对象”“泛型”“模板元”“函数式”**这五种主要的编程范式。
* <img src="https://static001.geekbang.org/resource/image/6e/87/6ef13308109b2d1795e43c5206c32687.jpg?wh=8000*4500" alt="img" style="zoom:50%;" />

* **面向过程**： 它的核心思想是“命令”，通常就是**顺序执行**的语句、子程序（函数），把任务分解成若干个步骤去执行，最终达成目标。
* **面向对象**： 它的核心思想是**“抽象”和“封装”**，倡导的是把任务分解成一些高内聚低耦合的对象，这些对象互相通信协作来完成任务。它强调对象之间的关系和接口，而不是完成任务的具体步骤。
* 面向对象范式包括 class、public、private、virtual、this 等类相关的关键字，还有构造函数、析构函数、友元函数等概念。
* **泛型编程**： 是自 STL（标准模板库）纳入到 C++ 标准以后才逐渐流行起来的新范式，核心思想是“一切皆为类型”，或者说是“参数化类型”“类型擦除”，**使用模板而不是继承的方式来复用代码**，所以运行效率更高，代码也更简洁。（泛型的基础就是 template 关键字）
* **模板元编程**： 它的核心思想是“类型运算”，操作的数据是编译时可见的“类型”，所以也比较特殊，**代码只能由编译器执行，而不能被运行时的 CPU 执行**。type_traits、enable_if 等（更多是以库为主）
* **函数式**: 所谓的“函数式”并不是 C++ 里写成函数的子程序，而是数学意义上、无副作用的函数，核心思想是“一切皆可调用”，**通过一系列连续或者嵌套的函数调用实现对数据的处理。**(C++11 引入了 Lambda 表达式)



#### Makefile & GDB

* Makefile

	> ```cpp
	> /*
	> 目标... : 依赖 ...
	>   命令（shell命令）
	> eg:
	> app : add.c mult.c div.c sub.c
	> 		gcc add.c mult.c div.c sub.c -o app
	> 原则：
	> 1. 如果当前依赖没有，makefile 向下查找是否存在符合条件的依赖
	> 2. 检测更新，如果出现更新的内容，我们需要重新生成文件，否则不需要重新生成可执行文件
	> */
	> ```
	>
	> * 基本语法：
	>
	> 	> * 变量名= 变量值
	> 	> * 预定义变量：
	> 	> * AR ： 归档维护程序名称，默认值为 ar 
	> 	> * CC :  C 编译器的名称， 默认值是 cc
	> 	> * CXX : C++ 编译器的名称，默认值是 g++
	> 	> * $@: 目标的完整名称
	> 	> * $< : 第一个依赖文件的名称
	> 	> * $^ : 所有的依赖文件
	> 	> * 获取变量的值：
	> 	> * $(变量名)
	>
	> * 模式匹配：
	>
	> * %.o : %.c  % 表示的是通配符，匹配一个字符串，上面表示的是两个 % 匹配的是同一个字符串 

* 







#### 链接的过程

* ![img](https://img-blog.csdn.net/20180722165120296?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX0g5NTI3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
* 所有的**全局变量和静态变量都是数据**，其余的**局部变量等都是指令**
* 0-128MB 是在用户空间，但是是系统预留的空间，用户没法访问。
* .text 段，该段存放的是 程序段， .data 存放的是数据段，这个数据段是已经初始化的并且初始化不为 0 ，.bss 存放的是数据段，但是这个数据段是未初始化或者初始化为 0
* 链接基本过程：
* 合并段，将**各个文件不同的段部分进行合并操作**，将每个 .o 文件的 .text 部分合并到一起，将每一个 .o 文件的 .data 和 .bss 部分合并到一起.
* 调整段的偏移，每一个进程都有自己的虚拟地址空间，都是从 0 开始的，将各个文件的各个段加载进来之后，段的大小会发生改变，相对于 0  的偏移地址也会发生改变，这个时候需要调整段偏移。
* ![img](https://img-blog.csdn.net/20180722165120954?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX0g5NTI3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
* 完成符号的重定位：连接器需要给每一个符号加上一个偏移量，因为之前进行了段的偏移调整，所以符号的绝对位置发生了改变，但是段内的各个符号的相对位置并没有发生改变



#### new & malloc 的区别

* malloc 函数会在堆中分配一段内存空间，程序需要一个 freelist 的东西，去跟踪哪些内存块是空闲的。

* 属性区别

> * new/delete 是一个关键字，需要编译器的支持
> * malloc/free 是库函数，若要使用需要引入相应的库函数

* 使用区别

> * malloc: 每一次在申请内存的时候，需要输入申请的大小
> * new：无需显示的填入申请的内存大小，new 会根据 new 的类型分配内存,同时在对某个类进行调用的时候，我们还需要调用构造函数，malloc 只是单纯的分配内存空间。
> * int *p = (int * ) malloc(4)
> * int *p = new int()

* 内存位置区别

> * new的内部使用的是 malloc 实现的，所以两个分配都是在内存的堆中
>
> * new /delete 符号是可以被重载的，就是我们可以重新实现 new 的代码。 malloc 是 C 的库函数，无法对其进行重载。（因为 new 是一个 operator 可以类似 + - * / 这种运算符一样对其进行重载的操作)

* 返回类型的区别

> * new : 在进行内存分配成功的时候，返回的是对象类型的指针，类型严格与对象匹配，无需进行类型转换，所以 new 是符合类型安全的操作符。
> * malloc 内存分配成功返回的是 void* (不支持隐式类型转换，必须进行显示的类型转换）,需要进行强制类型转换转成我们需要的类型，所以 C++ 中使用 new 比使用 malloc 安全可靠

* 分配失败情况的区别

> * malloc : 分配失败返回的是 NULL， 我们可以通过判断返回值判断是否分配成功
> * new： 分配失败，会抛出 bad_alloc 异常，它不会返回 NULL， 分配失败，如果不捕获异常，那么我们就会出现异常退出的情况。

* 定义对象系统调度过程区别

> * new 需要三个步骤： 
>
>   > * 调用 operator new 函数 分配一块足够的内存空间，来存储特定类型的对象
>   > * 编译器传入相应的构造函数来构造对象，同时为其传入初值（需要调用构造函数）
>   > * 对象构造好之后，返回的是一个指向该对象的指针
>
> * delete 过程：
>
>   > * 调用析构函数释放对象
>   > * 同时调用 operator delete 函数释放内存空间



* 扩张内存大小的区别

> * new : 没有内存扩张机制
> * malloc 存在内存扩张机制： 当发现内存不够的时候，使用 realloc 函数进行内存大小的扩张,relloc 函数首先判断当前的地址内存是否足够，够返回的是原来的地址指针，否则 realloc 函数会在另外有足够大小的内存空间申请一块内存，同时将当前的内存空间的内容拷贝到新的内存空间，最后返回新的地址指针。（需要询问操作系统进行内存的扩张分配，这个消耗是比较大的）



* malloc 分配内存单元物理内存和虚拟内存的变化？

	> * malloc 分配内存，如果内存不够调用 sbrk 和 mmap , 但是这个时候分配的内存只是虚拟地址空间，而且只是虚拟内存的页号，代表当前的地址空间进程可以使用，实际还没有分配到实际的物理页面
	> * 内核维护一张页表，进程可以使用页表项进行虚拟地址到物理地址的转化。
	> * 当进程访问这个分配的内存空间的时候，如果还没有分配其物理地址会引发缺页中断，内核这个时候会为其分配实际的物理页面。让这个新分配的物理页面和这个虚拟地址对应起来。

	

* 一般用法是new一个数组的话一般是delete [] 其他的直接delete即可。

* 但是其实**对于内置数据类型，其实是delete[] 和delete都可以的**。
*  **基本类型的对象没有析构函数**，所以回收基本类型组成的数组空间用 delete 和 delete[] 都是应该可以的
* 但是对于**类对象数组**，只能用 delete[]。对于 new 的单个对象，只能用 delete 不能用 delete[] 回收空间













#### 指针函数 & 函数指针的区别

* 前面的两个字是修饰词，后面两个字才是真正的东西



* 指针函数：

  > * 带指针的函数，本质是一个函数，函数的返回类型是一个指针类型.

* 函数指针：

  > * 指向函数的首地址的指针变量，本质是一个指针变量。
  >
  > * 函数指针是一个指针，但是这个指针可以指向的是一个具体的函数
  >
  > * ```c++
  >   int max(int a, int b) {
  >   return a > b ? a : b;
  >   } 
  >   /* 求最小值，返回值是int类型，返回两个整数中较小的一个*/
  >   int min(int a, int b) {
  >   return a < b ? a : b;
  >   }
  >   int (*f)(int, int);
  >   f = min;
  >   int a = f(1, 2) // 1
  >   f = max;
  >   int a = f(1, 2) // 2
  >   ```

  

#### 数组指针 & 指针数组

##### 数组指针

* char (*p)[10]  : 是一个指针, 指向的是一个数组，数组名称就是**数组的首地址**
* sizeof(p) == 8



##### 数组

* char a[10] : a 表示的是数组**首元素的地址**
* p = &a;  // 不能 p = a



##### 指针数组

* int *p[10] : 是一个数组
* sizeof(p) == 8 * 10
* 



```cpp
int main() {
    const char *p[10] = {"hello", "zb", "world"};  // 指针类型的数组
    // c++ 中 "hello" 是 const char* 类型
    char (*q)[10]; // 数组类型的指针, 指向的是数组的首地址
    char qq[10] = "hello";
    q = &qq;  // 必须这样才可以，因为需要指向数组的地址
    std::cout << *q << std::endl;
    std::cout << "The addr of q is : " << q << " the addr of qq is : " << qq << std::endl;
    std::cout << sizeof(p) << " " << sizeof(q) << std::endl;
    std::cout << p[0] << std::endl;
    return 0;
}

/* 
hello
The addr of q is : 0x7ffd25560a8e the addr of qq is : hello
80 8
hello
*/
```









#### 野指针

*  delete  p 之后，只是释放了指针中存放的地址的内存空间，但是指针也需要一个 4 字节的空间存放，这个空间并没有释放，所以  p 中存放的地址还是原来的地址，但是这个地址中的内容现在是删除了，就导致野指针。
* 第一次 delete 之后指针 p 指向的那块区域变成不可访问的区域了，再次删除就是不合法的。
* 野指针作为一个指针，甚至都没有被初始化，也就是说虽然它的类型是一个指针，但它根本没有值。它跟NULL指针还有差别，NULL是指向了0地址，而野指针是没有地址。也就相当于，int a=0;和int a;的区别。
* 空悬指针：简单地说，空悬指针是对象的指针的生存周期比对象更长所导致的，也就是说，对象销毁、删除了，不存在了，指针仍然存在，这时这个指针就成了空悬指针。函数返回指针，但是这个指针变量在函数结束的时候就已经没了，那么变成悬空指针



#### Memcpy & strcpy 区别

* **复制的内容不同**。strcpy只能复制字符串，而memcpy可以复制任意内容，例如字符数组、整型、结构体、类等。
* **复制的方法不同**。strcpy不需要指定长度，它遇到被复制字符的串结束符"\0"才结束，如果空间不够，就会引起内存溢出。memcpy则是根据其第3个参数决定复制的长度。
* **用途不同**。通常在复制字符串时用strcpy，而需要复制其他类型数据时则一般用memcpy，由于字符串是以“\0”结尾的，所以对于在数据中包含“\0”的数据只能用memcpy。







#### 为什么拷贝构造必须使用的是引用类型

* 有些重载运算符需要返回引用，其中一个原因是需要返回其自身。返回自己而不是返回自己的一个拷贝。

* 可能造成无限递归的情况：

* 看第四个输出： **copy constructor       **： CExample ccc = aaa;

  构造ccc，实质上是ccc.CExample(aaa); 我们假如拷贝构造函数参数不是引用类型的话， 那么将使得 ccc.CExample(aaa)变成aaa传值给ccc.CExample(CExample ex)，即CExample ex = aaa，因为 ex 没有被初始化， 所以 CExample ex = aaa 继续调用拷贝构造函数，接下来的是构造ex，也就是 ex.CExample(aaa)，必然又会有aaa传给CExample(CExample ex), 即 CExample ex = aaa;那么又会触发拷贝构造函数，就这下永远的递归下去。





#### 函数的返回值存放在哪里

* 一个思路：是存放在主函数开辟的栈空间，因为主函数得要能访问到。
* 但是现代计算机硬件快速发展，CPU 的通用寄存器字长在不断的增长，函数的返回值直接存放在 CPU 的通用寄存器中，而不一定是要在计算机的内存空间
* **具体类型的返回值的存放位置如下所示：**
  - char（8bit）：寄存器a1
  - short（16bit）：寄存器ax
  - int（32bit）：寄存器eax
  - double（64bit）：协处理器堆栈
  - 指针、引用：寄存器eax
  - 类的对象且体积超过64bit：主调函数会在函数栈上创建临时对象存放





#### 智能指针

* unique_ptr:

  > * 不能复制这个指针，可以使用 std::move() 进行所有权的转移
  > * 原因是：当复制之后两个指针指向同一块地址，因为是 unique_ptr 所以当一个指针释放（自动释放），另外一个指针指向的是一个已经删除的内存.
  > * 使用 make_unique<T>() 创建一个 unique_ptr 因为这种方式是异常安全的。
  > * 当离开 unique_ptr 的这个作用域，那么这个 unique_ptr 会自动的释放。自动销毁
  > * unique_ptr 不能进行拷贝的原因:底层的源码对拷贝构造和拷贝赋值调用了 delete 函数

* shared_ptr

> * shared_ptr 调用的时候会分配创建一个控制块，用来存储我们的引用计数变量的。
> * 共享指针因为引用计数变量的存在，所以会有一个内存的消耗 



* weak_ptr

> * 当我们将一个 shared_ptr 拷贝给这个 weak_ptr 我们不会做引用计数的增加



```cpp
#include<bits/stdc++.h>

using namespace std;

template <class T>

class SharedPointer {
public:
    // 构造函数
    SharedPointer(T *p = NULL) {
        this -> m_ptr = p;
        this -> m_count = new size_t;
        if (p == NULL) {
            *this -> m_count = 0;
        } else {
            *this -> m_count = 1;
        }
    }

    // 析构函数
    ~SharedPointer() {
        if (this -> m_ptr != nullptr) {
            if (*this -> m_count == 0) {
                delete this -> m_ptr;
                delete this -> m_count;
                this -> m_ptr = nullptr;
                this -> m_count = nullptr;
            }
        }
    }

    // 拷贝构造函数
    SharedPointer(const SharedPointer& s) {
        if (this != &s) {
            this -> m_ptr = s.m_ptr;
            this -> m_count = s.m_count;
            (*this -> m_count)++;
        }
    }

    // 拷贝赋值函数,重载赋值运算符
    SharedPointer& operator = (const SharedPointer& s) {
        if (this -> m_ptr == s.m_ptr) {
            return *this; // 返回的是对象本身，不是返回的指针
        }
        if (this -> m_ptr) {
            if (*(--this -> m_count) == 0) { // 因为只有自己本身，那么只需要将其覆盖也是可以的，但是这样写会更好一点
                delete this -> m_ptr;
                delete this -> m_count;
                this -> m_ptr = NULL;
                this -> m_count = NULL;
            }
        }
        this -> m_ptr = s.m_ptr;
        this -> m_count = s.m_count;
        (*this -> m_count)++;
        return *this;
    }

    // 重载 -> 操作符
    T& operator -> () {
        if (this -> m_ptr) {
            return this -> m_ptr;
        }
    }

    // 重载 * 操作符
    T& operator * () {
        if (this -> m_ptr) {
            return *(this -> m_ptr);
        }
    }

    // 实现 use_count() 函数
    size_t use_count() const {
        return *(this -> m_count);
    }

private:
    T* m_ptr; // 共享指针是普通的指针实现的。
    size_t* m_count; // 实现引用计数的部分,为了保证指针的引用计数
};

// 两个测试：std 提供的 sharedpointer 以及自己写的 sharedpointer 两个都需要测试

void Std_Shared_Ptr() {
  	// 推荐使用以下方式创建智能指针
  	/*
  	shared_ptr<int>ptr1 = make_shared<int>();
  	shared_ptr<int>pt2r(ptr1);
  	unique_ptr<int>ptr1 = make_unique<int>();
  	*/
    shared_ptr<int>ptr1(new int);
    // shared_ptr<int>ptr2(ptr1);
    shared_ptr<int>ptr3;
    ptr3 = ptr1;
    cout << "这个是 std::shared_ptr 的测试结果：" << endl;
    cout << "ptr1 use count: " << ptr1.use_count() << endl;
    // cout << "ptr2 use count: " << ptr2.use_count() << endl;
    cout << "ptr3 use count: " << ptr3.use_count() << endl;

    *ptr3 = 100;
    cout << "通过对 ptr3 赋值 获取 ptr1 的值: " << *ptr1 << endl;
}

// 自己写的共享指针

void My_Shared_Ptr() {
    SharedPointer<int>ptr1(new int);
    // SharedPointer<int>ptr2(ptr1);
    SharedPointer<int>ptr3;
    ptr3 = ptr1;
    cout << "这个是 My_Shared_ptr 的测试结果：" << endl;
    cout << "ptr1 use count: " << ptr1.use_count() << endl;
    // cout << "ptr2 use count: " << ptr2.use_count() << endl;
    cout << "ptr3 use count: " << ptr3.use_count() << endl;

    *ptr3 = 200;
    cout << "通过对 ptr3 赋值 获取 ptr1 的值: " << *ptr1 << endl;
}

```





####  堆和栈的区别

* 速度：栈上分配数据，只是**一条 CPU 语句**，所以非常的快，但是堆上分配数据速度更慢：因为我们需要去 freelist 中找是否存在合适的位置，记录已经被拿走的内存，多少已经被分配了，用完需要调用 delete 所以 **CPU 指令数更多**。
* 原则：通常在栈中分配内存，有的时候需要去堆中分配内存，目的是：希望这个分配的对象的生命周期比函数的作用域更长。

* 管理方式不同

> * 栈：编译器自动管理。
> * 堆：需要程序员手动控制，容易产生内存泄露

* 空间大小不同

> * 栈：linux 操作系统中栈大小是 8192K。
> * 堆：在32 位操作系统中，大小是：4GB。

* 能否产生碎片

> * 栈：栈是先进后出，所以不存在碎片问题，因为永远不可能出现一个内存块从栈中间弹出。
> * 堆：频繁的 new/delete 势必会造成内存空间的不连续，从而造成大量的碎片

* 生长方向不同

> * 栈：从上向下生长分配的，向内存减小 的方向分配的
> * 堆：从下向上分配的，向内存增加的方向分配。

* 分配方式不同

> * 栈：静态分配和动态分配，静态分配是由编译器完成的，比如局部变量的分配，动态分配是由 alloc 函数分配的，但是栈动态分配是由编译器释放的，不需要手动释放。
> * 堆： 堆只能动态分配，需要程序员手动释放

* 分配效率不同

> * 栈：机器系统提供的数据结构，计算机会在底层对栈提供支持，分配专门的寄存器存放栈的地址，压栈出栈都是有专门的执行指令，所以栈的效率比较高。
> * 堆： 由 c/c++ 的库提供的，机制比较复杂，库函数按照一定的算法在堆中寻找可用的足够大小的空间





#### 实现锁和条件变量







#### 实现计时器 Timer

```cpp
// TimeWheel 时间轮算法
#include <functional>
#include <list>
#include <thread>
#include <mutex>
#include <string>
#include <cstring>
#include <iostream>
#include <ctime>

using namespace std;

typedef struct TimePos_ {
    // 毫米，秒，分钟
    int ms_pos;
    int s_pos;
    int min_pos;
} TimePos;

typedef struct EventInfo_ {
    // 超时时间（为初始化的倍数）
    int interval;
    // 回调函数
    std::function<void()> call_back;
    // 时间
    TimePos time_pos;
    // 定时器 ID
    int timer_id;
} EventInfo;

class TimeWheel {
public:
    TimeWheel();
    ~TimeWheel();
public:
    /*
    step 以毫秒为单位，表示定时器的最小时间粒度
    max_timer 表示定时器所能接受的分钟时间间隔，表示一轮时间的总分钟数
    */
    int InitTimerWheel(int step, int max_min);
    // 设置时间为 TimerOut 的事件时，根据 TimeOut 算出发生此事件时刻的指针位置 {TriggerMin, TriggerS, TriggerMS}
    template<typename Func, typename... Args>
    int AddTimer(int interval, Func&& func, Args&& ... args);
    int AddTimer1(int interval, std::function<void(void)>& call_back);
    int DeleteTimer(int timer_id);

private:
    int DoLoop();
    int GenerateTimerID();
    int InsertTimer(int diff_ms, EventInfo& einfo);
    int GetNextTrigerPos(int interval, TimePos& time_pos);
    int GetMS(TimePos time_pos);
    int DealTimeWheeling(std::list<EventInfo> leinfo);

private:
    // 此处不是链表，是链表数组的头指针，时间轮数组
    std::list<EventInfo> *_pCallbackList = nullptr;
    std::mutex _mutex;

    // 时间
    TimePos _time_pos;

    // 计数刻度
    int _lowCount = 0;
    int _midCount = 0;
    int _highCount = 0;

    // 步长
    int _step_ms = 0;  // 就是 不是 1 ms 一个步长，可能是多个毫秒 一个 步长.
    // 定时器数量
    int _timer_count = 0;

};

// 增加计数器
int TimeWheel::AddTimer1(int interval, std::function<void(void)>& call_back) {
    if (interval < _step_ms || interval % _step_ms != 0 || interval >= _step_ms * _lowCount * _midCount * _highCount) {
        std::cout << "Time interval is invalid" << std::endl;
        return -1;
    }
    std::lock_guard<std::mutex> lock(_mutex);
    // 初始化时间特性
    EventInfo einfo = {0};
    einfo.interval = interval;
    einfo.time_pos.min_pos = _time_pos.min_pos;
    einfo.time_pos.s_pos = _time_pos.s_pos;
    einfo.time_pos.ms_pos = _time_pos.ms_pos;
    einfo.call_back = call_back;

    einfo.timer_id = GenerateTimerID();
    InsertTimer(interval, einfo);
    _timer_count++;
    return einfo.timer_id;
}

// 增加计时器， interval 必须是 _step_ms 的倍数
template<typename Func, typename... Args>
int TimeWheel::AddTimer(int interval, Func&& func, Args&& ... args) {  // 返回的是增加的 计时器的 id
    // 判断不符合条件的情况
    if (interval < _step_ms || interval % _step_ms != 0 || interval >= _step_ms * _lowCount * _midCount * _highCount) {
        cout << "Time interval is invalid" << endl;
        return -1;
    }

    // 加一个锁，避免出现并发错误
    std::lock_guard<std::mutex> lock(_mutex);

    // 初始化时间特性
    EventInfo einfo = {0};
    einfo.interval = interval;
    einfo.time_pos.ms_pos = _time_pos.ms_pos;
    einfo.time_pos.s_pos = _time_pos.s_pos;
    einfo.time_pos.min_pos = _time_pos.min_pos;
    einfo.call_back = std::bind(std::forward<Func>(func), std::forward<Args>(args)...);
    // 使用了完美转发的特性，保留原来的传入引用类型，不发生改变。
    // 随机生成 id
    einfo.timer_id = GenerateTimerID();

    // 插入定时器，定时器个数加一，返回定时器的 id
    InsertTimer(einfo.interval, einfo);
    _timer_count++;

    return einfo.timer_id;
}

TimeWheel::TimeWheel() {
    memset(&_time_pos, 0, sizeof(_time_pos));  // memset 第一个参数需要出入一个指针变量
}

TimeWheel::~TimeWheel() {

}

// 初始化定时器
int TimeWheel::InitTimerWheel(int step_ms, int max_min) {
    if (1000 % step_ms != 0) {
        std::cout << "the step is not suitable, the step should be divided by 1000!" << std::endl;
        return -1;
    }

    // 毫秒，分钟，秒
    _step_ms = step_ms;
    _lowCount = 1000 / step_ms;
    _midCount = 60;
    _highCount = max_min;

    // 事件初始化
    // 初始化链表的数组，数组里的元素为链表，时间轮是数组
    _pCallbackList = new std::list<EventInfo>[_lowCount + _midCount + _highCount];  // 新建的是一个数组

    std::thread th([&] {
        this -> DoLoop();  // 开启一个线程执行这个任务
    });
    th.detach();
    return 0;
}
// 删除定时器
int TimeWheel::DeleteTimer(int timer_id) {
    // 最大的事件数量
    std::unique_lock<std::mutex> lock(_mutex);
    int nCount = _lowCount + _midCount + _highCount;
    int i;
    for (i = 0; i < nCount; i++) {
        std::list<EventInfo>& listfo = _pCallbackList[i];
        // 因为一个时刻可能需要执行很多个事件。
        for (auto item = listfo.begin(); item != listfo.end(); item++) {
            if (timer_id == item -> timer_id) {
                listfo.erase(item);
                return 0;
            }
        }
    }
    if (i == nCount) {
        std::cout << "timer id not found" << std::endl;
        return -1;
    }
    return 0;
}

int TimeWheel::DoLoop() {
    static int cnt = 0;
    while (1) {
        std::this_thread::sleep_for(std::chrono::milliseconds(_step_ms));
        std::unique_lock<std::mutex> lock(_mutex);
        TimePos pos = {0};
        TimePos last_pos = _time_pos;
        GetNextTrigerPos(_step_ms, pos);
        _time_pos = pos; // 更新我们的 时间表，方便下一次更新操作，注意这个 _time_pos 是在不断变化的
        // std::cout << pos.min_pos << " " << pos.s_pos << " " << pos.ms_pos << std::endl;

        // std::cout << last_pos.min_pos << " " << last_pos.s_pos << " " << last_pos.ms_pos << std::endl;
        // std::cout << pos.min_pos << " " << pos.s_pos << " " << pos.ms_pos << std::endl;
        // std::cout << _time_pos.min_pos << " " << _time_pos.s_pos << " " << _time_pos.ms_pos << std::endl;

        if (pos.min_pos != last_pos.min_pos) {
            std::list<EventInfo>& leinfo = _pCallbackList[_time_pos.min_pos + _lowCount + _midCount];
            DealTimeWheeling(leinfo);
            leinfo.clear();
        } else if (pos.s_pos != last_pos.s_pos) {
            std::list<EventInfo>& leinfo = _pCallbackList[_time_pos.s_pos + _lowCount];
            DealTimeWheeling(leinfo);
            leinfo.clear();
        } else if (pos.ms_pos != last_pos.ms_pos) {
            std::list<EventInfo>& leinfo = _pCallbackList[_time_pos.ms_pos];
            DealTimeWheeling(leinfo);
            leinfo.clear();
        } else {
            cout << "The timer is error" << endl;
            return -1;
        }

        lock.unlock();
    }
    return 0;
}

int TimeWheel::GenerateTimerID() {
    // 随机生成 ID
    srand((int)time(NULL));
    int x = rand() % 0x3f3f3f3f;
    int cur_time = static_cast<int>(time(nullptr));
    return x | cur_time | _timer_count;  // 对时间取或操作，最终生成一个随机数
}

int TimeWheel::InsertTimer(int diff_ms, EventInfo& einfo) {
    TimePos time_pos = {0};
    GetNextTrigerPos(diff_ms, time_pos);

    // 数组中的每一个元素都是一个 list 链表
    if (time_pos.min_pos != _time_pos.min_pos) {
        _pCallbackList[_lowCount + _midCount + time_pos.min_pos].push_back(einfo);
    } else if (time_pos.s_pos != _time_pos.s_pos) {
        _pCallbackList[_lowCount + time_pos.s_pos].push_back(einfo);
    } else {
        _pCallbackList[time_pos.ms_pos].push_back(einfo);
    }

    return 0;
}

int TimeWheel::GetNextTrigerPos(int interval, TimePos& time_pos) {
    int cur_ms = GetMS(_time_pos);  // 注意当前时间是 _time_pos 这个值,然后在当前的时间基础上加上一个间隔时间
    int future_ms = interval + cur_ms;
    // 更新 time_pos 这个值，将其最终的结果保存在这个 time_pos 中
    time_pos.min_pos = (future_ms / 60 / 1000) % _highCount;
    time_pos.s_pos = (future_ms % (60 * 1000)) / 1000; // 最终得到的是 秒
    time_pos.ms_pos = (future_ms % 1000) / _step_ms; // 最终得到的是间隔，每一个间隔是 step_ms 不是 1ms 一个间隔
    return 0;
}

int TimeWheel::GetMS(TimePos time_pos) {
    return _step_ms * time_pos.ms_pos + time_pos.s_pos * 1000 + time_pos.min_pos * 60 * 1000;
}

int TimeWheel::DealTimeWheeling(std::list<EventInfo> leinfo) {
    for (auto item = leinfo.begin(); item != leinfo.end(); item++) {
        int last_ms = GetMS(item -> time_pos);
        int cur_ms = GetMS(_time_pos);
        int diff_ms = cur_ms - last_ms;
        // int diff_ms = (cur_ms - last_ms + (_highCount + 1) * 60 * 1000) % ((_highCount + 1) * 60 * 1000);
        if (diff_ms == item -> interval) {
            // 如果时间到了，那么我们执行这个计时器
            item -> call_back();

            item -> time_pos = _time_pos;
            // 下面这一句是干什么用的？？？下面这句是再加入一个事务，就是将当前的任务执行加入下一个任务中,接着执行
            InsertTimer(item -> interval, *item);
        } else {
            InsertTimer(item -> interval - diff_ms, *item); // 现在间隔就是 item -> interval - diff_ms ，将这个间隔加入到 定时器中
        }
    }
    return 0;
}

int g_count = 0;

void func100 (string s) {
    g_count++;
    std::cout << s << " " << g_count << std::endl;
}

void func200 () {
    std::cout << "func 200" << std::endl;
}

void func300 () {
    std::cout << "func 300" << std::endl;
}

int main() {
    // test case //
    TimeWheel tm_wheel;
    tm_wheel.InitTimerWheel(100, 5);
    int timer1 = tm_wheel.AddTimer(1000, func100, "你好计时器");
    std::cout << "timer1 id : " << timer1 << std::endl;
    std::function<void(void)> f200 = std::bind(func200);
    int timer2 = tm_wheel.AddTimer1(2000, f200);
    std::cout << "timer2 id : " << timer2 << endl;
    getchar();
    return 0;
}
```



```cpp
// 小顶堆实现计时器
// #include <bits/stdc++.h>
#include <functional>
#include <chrono>
#include <iostream>
#include <vector>
#include <unordered_map>
#include <memory>
#include <sys/time.h>
#include <thread>

typedef std::chrono::milliseconds _ms;
typedef std::chrono::high_resolution_clock _clock;
typedef _clock::time_point _timeStamp;
typedef std::function<void()> CallBack;
typedef unsigned long long ULL;
class TimeManager;

class TimeNode {
public:
    friend class TimeManger;
    enum class TimerType{ONCE = 0, CIRCLE = 1};
    TimeNode (TimeManager& manager);
    ~TimeNode();

    // 启动一个定时器
    template<typename Func>
    void start(Func func, int ms, TimerType type);
    // 终止一个定时器
    void stop();
    // 执行
    void on_timer(ULL now);

public:
    // friend class TimeManger;
    TimeManager& manager_;
    // 调用函数，包括仿函数
    CallBack m_timerfunc;
    TimerType timetype_;
    // 间隔
    int m_nInterval;
    // 过期
    ULL m_nExpires;
    int m_nHeapIndex; // 快速定位当前的定时器是在 heap_ 数组中的下标，方便快速定位快速删除
};

class TimeManager {
public:
    // friend class TimeNode;
    // 获取当前毫秒数
    // 定义一个静态函数，不属于某一个对象，属于整个类
    static ULL get_current_millisecs();
    // 探测执行
    void detect_timers();
private:
    friend class TimeNode;
    // 添加一个定时器
    void add_timer(TimeNode* timer);
    // 删除一个定时器
    void del_timer(TimeNode* timer);
    // 定时上浮
    void down2top(int index);
    // 定时下浮
    void top2down(int index);
    // 交换两个 timer 的索引
    void swap_heap(int idx1, int idx2);
private:
    // 定义一些变量
    // friend class TimeNode;
    struct HeapEntry {
        ULL time;
        TimeNode* timer;
    };
    std::vector<HeapEntry> heap_;
};





TimeNode::TimeNode(TimeManager& manager) : manager_(manager), m_nHeapIndex(-1) {}

TimeNode::~TimeNode() {
    stop();
}

template<typename Func>
inline void TimeNode::start(Func func, int ms, TimerType type) {  // 定义成 内联函数
    m_timerfunc = func;
    timetype_ = type;
    m_nInterval = ms;
    m_nExpires = ms + TimeManager::get_current_millisecs();
    manager_.add_timer(this);  // 添加这个计时器
}

void TimeNode::stop() {
    if (m_nHeapIndex != -1) {
        manager_.del_timer(this);
        m_nHeapIndex = -1;
    }
}

// 执行相应的函数
void TimeNode::on_timer(ULL now) {
    if (timetype_ == TimerType::CIRCLE) {
        // 需要循环执行
        m_nExpires = m_nInterval + now;
        manager_.add_timer(this);  // 其实自始至终，我们只有一个 manager_ 做相应的管理操作s
    } else {
        m_nHeapIndex = -1;
    }
    m_timerfunc(); // 执行函数
}

// 检查哪些函数可以执行了.
void TimeManager::detect_timers() {
    ULL now = get_current_millisecs();
    while (!heap_.empty() && heap_[0].time < now) {
        // 注意需要先删除，然后再需要的话再删除，否则可能出错
        TimeNode* node = heap_[0].timer;
        del_timer(node);
        node -> on_timer(now);
    }
}

void TimeManager::add_timer(TimeNode* timer) {
    // 插入到数组的最后一个位置，上浮
    timer -> m_nHeapIndex = heap_.size(); // 先插入的下标什么
    HeapEntry entry = {timer -> m_nExpires, timer};
    heap_.push_back(entry);
    down2top(heap_.size() - 1);
}

void TimeManager::del_timer(TimeNode* timer) {
    // 堆中插入元素，然后上浮，类似堆的插入操作
    // 注意这个元素插入之后可能是上浮也可能是下沉
    int idx = timer -> m_nHeapIndex;
    if (!heap_.empty() && idx < heap_.size()) {
        if (idx == heap_.size() - 1) {
            // 就是最后一个可以直接删除
            heap_.pop_back();
        } else {
            // 是中间的元素，这个时候我们需要将其和最后一个元素交换，然后 pop_back() 最后一个元素
            swap_heap(idx, heap_.size() - 1);
            heap_.pop_back();
            // 当然需要对 idx 这个数据进行调整操作,因为当前 idx 对应的是 heap_ 的最后一个元素了
            int parent_idx = (idx - 1) / 2;
            if (idx >= 0 && heap_[idx].time < heap_[parent_idx].time) {
                down2top(idx);
            } else {
                top2down(idx);
            }
        }
    }
}

// 构建小顶堆的上浮和下沉操作
void TimeManager::down2top(int index) {
    // 从下至上，和父节点比较，直到所有的比较完成结束
    int parent_idx = (index - 1) / 2;
    int tar_idx = index;
    if (index >= 0 && heap_[tar_idx].time < heap_[parent_idx].time) {
        tar_idx = parent_idx;
    }
    if (tar_idx != index) {
        swap_heap(index, parent_idx);  // 注意不是简单的交换就行了，因为我们还需要交换 index
        down2top(parent_idx);  // 递归的方式求解
    }
}
// 小顶堆
void TimeManager::top2down(int index) {
    int n = heap_.size();
    int tar_idx = index;
    int left_idx = 2 * index + 1;
    int right_idx = 2 * index + 2;
    if (left_idx < n && heap_[left_idx].time < heap_[tar_idx].time) tar_idx = left_idx;
    if (right_idx < n && heap_[right_idx].time < heap_[tar_idx].time) tar_idx = right_idx;
    if (tar_idx != index) {
        swap_heap(tar_idx, index);
        top2down(tar_idx);
    }
}

void TimeManager::swap_heap(int idx1, int idx2) {
    HeapEntry temp = heap_[idx1];
    heap_[idx1] = heap_[idx2];
    heap_[idx2] = temp;
    // 注意上面只是简单的交换，但是在 heap 中的下标并没有交换，这个也是需要交换的
    heap_[idx1].timer->m_nHeapIndex = idx1;
    heap_[idx2].timer->m_nHeapIndex = idx2;
}

ULL TimeManager::get_current_millisecs() {
    timeval tv;
    ::gettimeofday(&tv, 0);
    ULL ret = tv.tv_sec;  // 转换成秒, tv_sec 是秒， tv_usec 是微秒
    return ret * 1000 + tv.tv_usec / 1000; // 将其转换成毫秒，最终传出去
}

void func1 (int n) {
    std::cout << "func1: " << n << std::endl;
}

void func2 () {
    std::cout << "func2!" << std::endl;
}

void func3 (int n) {
    std::cout << "hello zhoubing, this is : " << n << std::endl;
}

int main() {
    // 这样只需要开一个线程就可以了，不需要开多个线程了,因为之前的一定是提前执行的
    CallBack f1 = std::bind(func1, 100);
    CallBack f2 = std::bind(func2);
    CallBack f3 = std::bind(func3, 10086);
    TimeManager manager;
    TimeNode timer1(manager); // 使用 一个线程管理所有的定时;
    TimeNode timer2(manager);
    TimeNode timer3(manager);
    timer1.start(f1, 1000, TimeNode::TimerType::CIRCLE);
    timer2.start(f2, 1500, TimeNode::TimerType::ONCE);
    timer3.start(f3, 500, TimeNode::TimerType::ONCE);

    while (1) {
        std::this_thread::sleep_for(_ms(100));  // 每隔 100 ms 就去检测一下，避免过于经常的进行检查操作。
        manager.detect_timers();
    }
    getchar();

    return 0;
}
```



#### 实现 线程池 ThreadPool

```cpp
#include <iostream>
#include <cstdio>
#include <thread>
#include <memory>
#include <functional>
#include <queue>
#include <mutex>
#include <condition_variable>

/*
1. 什么是线程池？
线程池，就是存放线程的容器，一个存放大量线程的池子，需要线程的时候去池子中取，用完之后将这个线程放回池子中
线程池是对线程的高校管理，从而提高 CPU 的并发性。当出现一个任务的时候，线程池分配线程在不阻塞主线程的情况下，完成这项任务
2. 为什么需要线程池？
使用线程池，最大的好处是：服务器可以避免因为重复的建立和销毁线程带来的开销，从而提高服务端对客户端的响应速度.
3. 线程池类似生产者消费者模型，因为之前说了从线程池中取线程，用完放回线程池，所以从线程池中取线程类似消费者，
放回线程池类似生产者。
4. 线程池的初始化？
线程池并不是每提交一次任务初始化一次，而是进行一次初始化，保持一个非活跃状态直至一些任务完成，这样可以减小系统开销
5. 线程池的工作过程？
使用一个 vector 来存储线程。主要的思路是在没有任务的时候保持线程处于非活跃状态，有任务的时候进行处理操作。
6. 线程池如何执行销毁操作？
当需要执行销毁操作的时候，唤醒所有的线程，并且逐个 join
*/

typedef std::function<void()> CallBack;
typedef std::chrono::milliseconds _ms;

class MyThreadPool {
public:
    ~MyThreadPool(void);
    // 获取线程池指针
    static MyThreadPool* getinstance();
    // 添加任务，成功返回 True ，失败返回 False
    bool start(CallBack func);
private:
    // 定义一些变量
    int max_thread;
    int max_task;
    // 线程池数组
    std::vector<std::thread>threads;
    // 任务队列,就是可以调用的函数队列
    std::queue<CallBack> m_tasks;
    // 锁
    std::mutex mutex_;
    // 条件变量
    std::condition_variable has_task;

    bool running_flag;

private:
    // 定义一些函数
    MyThreadPool(void);  // 采用的是单例模式线程安全
    bool InitThread();
    void DestroyPool();
    // 工作线程
    void WorkFunc();
    static MyThreadPool* m_pool;
    static std::mutex* single_mutex;  // 这个是给 getinstance 使用的
};

// 注意：静态变量在实例化之前进行初始化操作。
MyThreadPool* MyThreadPool::m_pool = NULL;
std::mutex* MyThreadPool::single_mutex = new std::mutex();

MyThreadPool::MyThreadPool(void) : max_thread(3), max_task(30), running_flag(true) {} // 一般都是采用初始化列表的方式，进行默认的构造函数

MyThreadPool::~MyThreadPool(void) {
    DestroyPool();
}

MyThreadPool* MyThreadPool::getinstance() {
    if (m_pool == NULL) {
        // 上锁
        std::lock_guard<std::mutex>lock(*single_mutex);
        if (m_pool == NULL) {
            m_pool = new MyThreadPool();
        }
        // 在这里即将释放锁， 因为 要结束 if 作用域了
    }
    return m_pool;
}

bool MyThreadPool::start(CallBack func) {
    // 如果是第一次，需要初始化线程池
    {
        if (threads.size() == 0) { // 双重 if 判断保证线程安全
            std::unique_lock<std::mutex> lock(mutex_);
            if (threads.size() == 0) {
                // 初始化线程
                if (!InitThread()) {
                    // std::cout << "ok" << std::endl;
                    return false;
                }
            }
        }
    }
    // 判断工作队列是否满了，没有满，我们就加入一个任务
    {
        std::unique_lock<std::mutex> lock(mutex_);
        if (m_tasks.size() < max_task) {
            m_tasks.push(func);
        } else {
            // std::cout << m_tasks.size() << std::endl;
            return false;
        }
    }
    has_task.notify_one();
    return true;
}

bool MyThreadPool::InitThread() {
    for (int i = 0; i < max_thread; i++) {
        std::cout << "the id is : " << i << std::endl;
        threads.push_back(std::thread(&MyThreadPool::WorkFunc, this));
    }
    return true;
}

void MyThreadPool::DestroyPool() {
    {
        std::unique_lock<std::mutex> lock(mutex_);
        running_flag = false;
    }
    has_task.notify_all();
    for (int i = 0; i < threads.size(); i++) {
        threads[i].join();
    }
    threads.clear();
}

void MyThreadPool::WorkFunc() {  // 每一个线程都是在执行一个 while 死循环的操作，一直的执行
    // 执行的函数
    {  // 这个有点类似作用域的作用
        while (running_flag || !m_tasks.empty()) {
            CallBack cal_func;
            // 获取任务，使用条件变量
            {
                std::unique_lock<std::mutex> lock(mutex_);
                while (m_tasks.empty()) {
                    // std::cout << "target" << std::endl;
                    has_task.wait(lock);  // 注意当在这里出现阻塞的时候，会释放锁，让别的线程执行
                }
                cal_func = m_tasks.front();
                m_tasks.pop();
            }
            // 执行任务
            cal_func();
        }
    }
}

static int ticket = 0;  // 需要修改的，所以不需要定义成 const

void showTicket(std::mutex* mtx) {
    std::lock_guard<std::mutex> lock(*mtx);
    std::cout << "Thread id is : " << std::this_thread::get_id() << " show ticket is : " << ticket++ << std::endl;
}

int main() {
    // std::cout << "ok" << std::endl;
    std::mutex* mtx = new std::mutex();
    int sum = 0;
    std::shared_ptr<MyThreadPool> pool(MyThreadPool::getinstance());
    for (int i = 0; i < 100; i++) {
        if (pool -> start(std::bind(showTicket, mtx)) == false) {
            sum++;
        }
    }
    std::cout << "Not used tasks are : " << sum << std::endl;
    delete mtx;
    // getchar();
    return 0;
}
```









#### 实现 vector



```cpp
#include <bits/stdc++.h>

using namespace std;

const int Space = 10;

template<typename T>
class MyVector{
public:
    MyVector() : m_size(0), m_capacity(Space) {
        m_ptr = new T[m_capacity];
    }
    explicit MyVector(int size) : m_size(size), m_capacity(size + Space) {
        m_ptr = new T[m_capacity];
    }
    ~MyVector() {
        if (m_ptr) {
            delete [] m_ptr;
            // m_ptr = nullptr;
        }
    }
    MyVector(const MyVector& rhs) {
        m_ptr = nullptr;
        cout << "拷贝构造" << endl;
        operator = (rhs);
    }
    MyVector& operator = (const MyVector& rhs) {
        cout << "拷贝赋值" << endl;
        if (this != &rhs) {
            m_size = rhs.m_size;
            m_capacity = rhs.m_capacity;
            delete [] m_ptr;
            m_ptr = new T[m_capacity];
            for (int i = 0; i < m_size; i++) {
                m_ptr[i] = rhs.m_ptr[i];
            }
        }
        return *this;
    }

    MyVector(MyVector&& rhs) {
        m_ptr = nullptr;
        cout << "移动拷贝构造" << endl;
        m_size = rhs.m_size;
        m_capacity = rhs.m_capacity;
        m_ptr = rhs.m_ptr;
        rhs.m_ptr = nullptr;
        // return *this;
    }

    MyVector& operator = (MyVector&& rhs) {
        cout << "移动赋值构造" << endl;
        m_size = rhs.m_size;
        m_capacity = rhs.m_capacity;
        m_ptr = rhs.m_ptr;
        rhs.m_ptr = nullptr;
        return *this;
    }

    T& operator [] (int idx) {
        return m_ptr[idx];
    }
    T& operator [] (int idx) const {
        return m_ptr[idx];
    }

    void push_back(const T& x) {
        if (m_size == m_capacity) {
            reAllocator(m_size * 2 + 1);
        }
        m_ptr[m_size++] = x;
    }
    void pop_back() {
        m_size--;
    }

    T& back() {
        return m_ptr[m_size - 1];
    }
    int size() {
        return m_size;
    }
    int capacity() {
        return m_capacity;
    }
    bool isempty() {
        return size() == 0;
    }

    // 实现一下迭代器
    typedef T* iterator;
    typedef const T* const_iterator;

    iterator begin() {
        return &m_ptr[0];
    }
    const_iterator begin() const {
        return &m_ptr[0];
    }
    iterator end() {
        return &m_ptr[m_size];
    }
    const_iterator end() const {
        return &m_ptr[m_size];
    }

private:
    int m_size;
    int m_capacity;
    T* m_ptr;

    void resize(int newSize) {
        if (newSize > m_capacity) {
            reAllocator(newSize * 2);
        }
        m_size = newSize;
    }

    void reAllocator(int newCapacity) {
        if (newCapacity < m_size) return;
        T* oldPtr = m_ptr;
        m_ptr = new T[newCapacity];
        for (int i = 0; i < m_size; i++) {
            m_ptr[i] = oldPtr[i];
        }
        m_capacity = newCapacity;
        delete []oldPtr;
    }
};



int main() {
    MyVector<int>vec;
    vec.push_back(10);
    vec.push_back(100);
    vec.push_back(1000);
    cout << vec.back() << " " << vec.size() << " " << vec.capacity() << " " << vec.isempty() << endl;
    vec.pop_back();
    vec.push_back(200);
    vec.push_back(10000);
    cout << vec.back() << endl;
    vec.back() = 10086;
    cout << vec.back() << endl;

    for (auto it = vec.begin(); it != vec.end(); it++) {
        cout << *it << " ";
    }
    cout << endl;
    cout << vec[3] << endl;
    MyVector<int> test;
    test = vec;
    for (int i = 0; i < test.size(); i++) {
        cout << test[i] << " ";
    }
    cout << endl;
    MyVector<int> test1;
    test1 = std::move(test);
    // test = vec;
    for (int i = 0; i < test1.size(); i++) {
        cout << test1[i] << " ";
    }
    cout << endl;
    return 0;
}
```









#### 实现 array

```cpp
#include<bits/stdc++.h>

using namespace std;

// 模板会在编译的时候就进行处理操作，所以可以将 nums[S],因为数组传进去的大小是需要提前知道的。
template<class T, size_t S>
class MyArray{
public:
    constexpr size_t size() const;
    T& operator[] (size_t idx);
    const T& operator[] (size_t idx) const;
    T* Data();
    const T* Data() const;
private:
    int nums[S];
};

template<class T, size_t S>
constexpr size_t MyArray<T, S>::size() const {
    return S;
}

template<class T, size_t S>
T& MyArray<T, S>::operator[] (size_t idx) {
    assert(idx < S);
    return nums[idx];
}

template<class T, size_t S>
const T& MyArray<T, S>::operator[] (size_t idx) const {
    // static_assert(idx <= S, "out of the array size!");
    assert(idx < S);
    return nums[idx];
}

template<class T, size_t S>
T* MyArray<T, S>::Data() {
    return nums;
}

template<class T, size_t S>
const T* MyArray<T, S>::Data() const {
    return nums;
}


void Test() {
    MyArray<int, 10> data;
    memset(data.Data(), 0, data.size() * sizeof(int));
    data[0] = 10;
    data[1] = 20;
    data[9] = 31;
    for (size_t i = 0; i < data.size(); i++) {
        cout << data[i] << endl;
    }
    cout << "copyarray: " << endl;
    const auto& arraycopy = data; // 注意这个返回的是一个引用，并没有对对象进行拷贝，只不过使用的是别名。
    // 但是这个别名是一个 const 的引用类型.
    cout << arraycopy.size() << endl;
    cout << arraycopy[9] << endl;
    cout << arraycopy[10] << endl;
    data[10] = 40;
}

int main() {
    Test();
    return 0;
}
```







